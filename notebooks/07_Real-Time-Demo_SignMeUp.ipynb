{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this notebook, real-time predictions can be made on signs you perform in front of your running webcam. \n",
    "\n",
    "### <span style=\"color:green\">Quick User Guide: \n",
    "\n",
    "<span style=\"color:green\">1. Load your trained TF Model.</span>\n",
    "\n",
    "<span style=\"color:green\">2. Copy + paste used Configurations from the pre-processing notebook</span>\n",
    "\n",
    "<span style=\"color:green\">Optional: 3. Copy + paste used Pre-processing Layer from the pre-processing notebook</span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Menu\n",
    "![image](../images/Real-Time-Demo-Menu.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import Dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-macos in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (2.12.0)\n",
      "Requirement already satisfied: opencv-python in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (4.7.0.72)\n",
      "Requirement already satisfied: mediapipe-silicon in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (0.9.1)\n",
      "Requirement already satisfied: sklearn in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (0.0.post1)\n",
      "Requirement already satisfied: matplotlib in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (3.4.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (23.3.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (3.8.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (0.4.8)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (16.0.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (1.23.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (58.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (1.53.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (2.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (2.12.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from mediapipe-silicon) (22.2.0)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from mediapipe-silicon) (4.7.0.72)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow-macos) (0.40.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from jax>=0.3.15->tensorflow-macos) (0.0.4)\n",
      "Requirement already satisfied: scipy>=1.7 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from jax>=0.3.15->tensorflow-macos) (1.10.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (2.17.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (0.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-macos) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-macos) (6.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-macos) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-macos) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-macos) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow-macos opencv-python mediapipe-silicon sklearn matplotlib\n",
    "#!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib # original code line from tutorial (he had a windows system)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os # easier file path handling\n",
    "\n",
    "# for camera feed\n",
    "import cv2 # opencv\n",
    "from matplotlib import pyplot as plt # imshow for easy visualization\n",
    "import time # to insert \"sleep\" in between frames\n",
    "import mediapipe as mp # for accessing and reading from webcam\n",
    "\n",
    "# for model re-building\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking\n",
    "\n",
    "# for loading .json dictionary\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Load Files</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Load Saved Model</span>\n",
    "\n",
    "#### <span style=\"color:green\">Here, you can load your trained TensorFlow model, e.g. LSTM.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = tf.keras.models.load_model('../models/LSTM/LSTM_model_20signsz_8.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sign_to_prediction_index_map.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label map dictionary \n",
    "#LABEL_MAP = json.load(open(\"../data/asl-signs/sign_to_prediction_index_map.json\", \"r\")) \n",
    "LABEL_MAP = {'brown': 0,  'call(phone)': 1,  'cow': 2,  'cry': 3,  'dad': 4,  'fireman': 5,  'frog': 6,  'gum': 7,  'ice cream': 8,  'my/mine': 9,  'nose': 10,  'owl': 11,  'please': 12,  'radio': 13,  'quiet': 14,  'shirt': 15,  'tomorrow': 16,  'uncle': 17,  'water': 18,  'who': 19}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Mechanics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTDOWN = 0\n",
    "#SELECTED_SIGNS = ['hello', 'mom', 'dad', 'hungry', 'thirsty']\n",
    "#SELECTED_SIGNS = ['apple', 'bath', 'owl', 'clown', 'better']\n",
    "SELECTED_SIGNS = list(LABEL_MAP.keys()) \n",
    "\n",
    "LEVEL = [['frog', 'cow', 'quiet', 'nose', 'dad'], ['brown', 'fireman', 'call(phone)', 'please', 'my/mine']]\n",
    "#LEVEL = [['frog', 'cow', 'quiet', 'nose', 'dad']] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Preprocessing Setup </span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">These configuration must be the same ones, which you used to preprocess the data to train the TensorFlow model, which you loaded above! </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of used landmarks: 43\n",
      "(22, 129)\n"
     ]
    }
   ],
   "source": [
    "#Define length of sequences for padding or cutting; 22 is the median length of all sequences\n",
    "LENGTH = 22\n",
    "\n",
    "#final data will be flattened, if false data will be 3 dimensional\n",
    "FLATTEN = False\n",
    "\n",
    "#Define padding mode \n",
    "#1 = padding at start&end; 2 = padding at end)\n",
    "PADDING = 2\n",
    "CONSTANT_VALUE = 0 \n",
    "\n",
    "#define if z coordinate will be dropped\n",
    "DROP_Z = False\n",
    "DEFINE_HAND = True\n",
    "\n",
    "#Defining Landmarks\n",
    "#index ranges for each landmark type\n",
    "#dont change these landmarks\n",
    "FACE = list(range(0, 468))\n",
    "LEFT_HAND = list(range(468, 489))\n",
    "POSE = list(range(489, 522))\n",
    "POSE_UPPER = list(range(489, 510))\n",
    "RIGHT_HAND = list(range(522, 543))\n",
    "LIPS = [61, 185, 40, 39, 37,  0, 267, 269, 270, 409,\n",
    "                 291,146, 91,181, 84, 17, 314, 405, 321, 375, \n",
    "                 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, \n",
    "                 95, 88, 178, 87, 14,317, 402, 318, 324, 308]\n",
    "lipsUpperOuter= [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291]\n",
    "lipsLowerOuter= [146, 91, 181, 84, 17, 314, 405, 321, 375, 291]\n",
    "lipsUpperInner= [78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308]\n",
    "lipsLowerInner= [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308]\n",
    "#defining landmarks that will be merged\n",
    "averaging_sets = []\n",
    "\n",
    "#generating list with all landmarks selected for preprocessing\n",
    "#change landmarks you want to use here:\n",
    "point_landmarks_right = RIGHT_HAND + lipsUpperInner + lipsLowerInner\n",
    "point_landmarks_left = LEFT_HAND + lipsUpperInner + lipsLowerInner\n",
    "\n",
    "#calculating sum of total landmarks used\n",
    "LANDMARKS = len(point_landmarks_right) + len(averaging_sets)\n",
    "print(f'Total count of used landmarks: {LANDMARKS}')\n",
    "\n",
    "#defining input shape for model\n",
    "if DROP_Z:\n",
    "    INPUT_SHAPE = (LENGTH,LANDMARKS*2)\n",
    "else:\n",
    "    INPUT_SHAPE = (LENGTH,LANDMARKS*3)\n",
    "print(INPUT_SHAPE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors used\n",
    "color_turquoise = (177, 195, 0)\n",
    "color_coral = (81, 99, 241)\n",
    "color_bg = (242, 248, 255)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objects and Functions for MP Holistic Keypoints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize MP Holistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing utilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mediapipe_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to detect MP Holistic landmarks from an image, e.g. frames of your camera feed\n",
    "def mediapipe_detection(image, model): \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # color conversion BGR to RGB\n",
    "    image.flags.writeable = False                   # image no longer writeable\n",
    "    results = model.process(image)                  # make prediction\n",
    "    image.flags.writeable = True                    # image is writeable again\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # color conversion back to original\n",
    "    return image, results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### draw_styled_landmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to draw landmarks points and connecting lines on top of an image, e.g. on top of your camera feed\n",
    "def draw_styled_landmarks(image, results): \n",
    "    # draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                              mp_drawing.DrawingSpec(color=color_turquoise, thickness=1, circle_radius=1), \n",
    "                              mp_drawing.DrawingSpec(color=color_turquoise, thickness=1, circle_radius=1))\n",
    "    # draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                              mp_drawing.DrawingSpec(color=color_bg, thickness=2, circle_radius=4), \n",
    "                              mp_drawing.DrawingSpec(color=color_bg, thickness=2, circle_radius=2)) \n",
    "    # draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                              mp_drawing.DrawingSpec(color=color_coral, thickness=2, circle_radius=4), \n",
    "                              mp_drawing.DrawingSpec(color=color_coral, thickness=2, circle_radius=2)) \n",
    "    # draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                              mp_drawing.DrawingSpec(color=color_coral, thickness=2, circle_radius=4), \n",
    "                              mp_drawing.DrawingSpec(color=color_coral, thickness=2, circle_radius=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### extract_keypoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract coordinates (+visibility) of all landmarks --> keypoints\n",
    "# and concatenates everything into a flattened list \n",
    "def extract_keypoints(results): \n",
    "    face = np.array([[r.x, r.y, r.z] for r in results.face_landmarks.landmark]) if results.face_landmarks else np.zeros([468, 3])\n",
    "    left_hand = np.array([[r.x, r.y, r.z] for r in results.left_hand_landmarks.landmark]) if results.left_hand_landmarks else np.zeros([21, 3])\n",
    "    pose = np.array([[r.x, r.y, r.z] for r in results.pose_landmarks.landmark]) if results.pose_landmarks else np.zeros([33, 3]) # x, y, z and extra value visibility\n",
    "    right_hand = np.array([[r.x, r.y, r.z] for r in results.right_hand_landmarks.landmark]) if results.right_hand_landmarks else np.zeros([21, 3])\n",
    "    return np.concatenate([face, left_hand, pose, right_hand]) # original code\n",
    "    # a flattened list with list of all pose, face, left_hand, right_hand landmark x, y, z, (+visibility) coordinates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### prob_viz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to visualize predicted word probabilities with a dynamic real-time bar chart\n",
    "def prob_viz(pred, image, SELECTED_SIGNS): \n",
    "    output_frame = image.copy() \n",
    "    bar_left = 10\n",
    "    bar_text_space = 5\n",
    "    text_left = bar_left + bar_text_space\n",
    "    bar_top = 140\n",
    "    bar_height = 20\n",
    "    bar_bottom = bar_top + bar_height\n",
    "    alpha = 0.3  # Transparency factor\n",
    "\n",
    "    overlay = output_frame.copy()\n",
    "    cv2.rectangle(output_frame, (0, 120), (230, 720), color_bg, -1)\n",
    "    # Following line overlays transparent rectangle over the image\n",
    "    output_frame = cv2.addWeighted(overlay, alpha, output_frame, 1 - alpha, 0)\n",
    "    \n",
    "    for num, prob in enumerate(pred): \n",
    "        cv2.rectangle(output_frame, \n",
    "                      pt1=(bar_left, bar_top+num*27), \n",
    "                      pt2=(bar_left+int(prob*100), bar_bottom+num*27), \n",
    "                      color=color_turquoise, thickness=-1)\n",
    "\n",
    "        cv2.putText(img=output_frame, \n",
    "                    text=SELECTED_SIGNS[num], \n",
    "                    org=(text_left, bar_bottom-3+num*27), \n",
    "                    fontFace=cv2.FONT_ITALIC, fontScale=0.75, \n",
    "                    color=(50, 50, 50), \n",
    "                    thickness=1, lineType=cv2.LINE_AA)\n",
    "    return output_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper Function - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_nan_mean(x, axis=0):\n",
    "    #calculates the mean of a TensorFlow tensor x along a specified axis while ignoring any NaN values in the tensor.\n",
    "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis)\n",
    "\n",
    "def right_hand_percentage(x):\n",
    "    #calculates percentage of right hand usage\n",
    "    right = tf.gather(x, RIGHT_HAND, axis=1)\n",
    "    left = tf.gather(x, LEFT_HAND, axis=1)\n",
    "    right_count = tf.reduce_sum(tf.where(tf.math.is_nan(right), tf.zeros_like(right), tf.ones_like(right)))\n",
    "    left_count = tf.reduce_sum(tf.where(tf.math.is_nan(left), tf.zeros_like(left), tf.ones_like(left)))\n",
    "    return right_count / (left_count+right_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating preprocessing layer that will be added to final model\n",
    "class FeatureGen(tf.keras.layers.Layer):\n",
    "    #defines custom tensorflow layer \n",
    "    def __init__(self):\n",
    "        #initializes layer\n",
    "        super(FeatureGen, self).__init__()\n",
    "    \n",
    "    def call(self, x_in, MIRROR=False):\n",
    "        #drop z coordinates if required\n",
    "        if DROP_Z:\n",
    "            x_in = x_in[:, :, 0:2]\n",
    "        if MIRROR:\n",
    "            #flipping x coordinates\n",
    "            x_in = np.array(x_in)\n",
    "            x_in[:, :, 0] = (x_in[:, :, 0]-1)*(-1)\n",
    "            x_in = tf.convert_to_tensor(x_in)\n",
    "\n",
    "        #generates list with mean values for landmarks that will be merged\n",
    "        x_list = [tf.expand_dims(tf_nan_mean(x_in[:, av_set[0]:av_set[0]+av_set[1], :], axis=1), axis=1) for av_set in averaging_sets]\n",
    "        \n",
    "        #extracts specific columns from input x_in defined by landmarks\n",
    "        if DEFINE_HAND:\n",
    "            handedness = right_hand_percentage(x_in)\n",
    "            if handedness > 0.5:\n",
    "                x_list.append(tf.gather(x_in, point_landmarks_right, axis=1))\n",
    "            else: \n",
    "                x_list.append(tf.gather(x_in, point_landmarks_left, axis=1))\n",
    "        else:\n",
    "            x_list.append(tf.gather(x_in, point_landmarks, axis=1))\n",
    "        #concatenates the two tensors from above along axis 1/columns\n",
    "        x = tf.concat(x_list, 1)\n",
    "\n",
    "        #padding to desired length of sequence (defined by LENGTH)\n",
    "        #get current number of rows\n",
    "        x_padded = x\n",
    "        current_rows = tf.shape(x_padded)[0]\n",
    "        #if current number of rows is greater than desired number of rows, truncate excess rows\n",
    "        if current_rows > LENGTH:\n",
    "            x_padded = x_padded[:LENGTH, :, :]\n",
    "\n",
    "        #if current number of rows is less than desired number of rows, add padding\n",
    "        elif current_rows < LENGTH:\n",
    "            #calculate amount of padding needed\n",
    "            pad_rows = LENGTH - current_rows\n",
    "\n",
    "            if PADDING ==4: #copy first/last frame\n",
    "                if pad_rows %2 == 0: #if pad_rows is even\n",
    "                    padding_front = tf.repeat(x_padded[0:1, :], pad_rows//2, axis=0)\n",
    "                    padding_back = tf.repeat(x_padded[-1:, :], pad_rows//2, axis=0)\n",
    "                else: #if pad_rows is odd\n",
    "                    padding_front = tf.repeat(x_padded[0:1, :], (pad_rows//2)+1, axis=0)\n",
    "                    padding_back = tf.repeat(x_padded[-1:, :], pad_rows//2, axis=0)\n",
    "                x_padded = tf.concat([padding_front, x_padded, padding_back], axis=0)\n",
    "            elif PADDING == 5: #copy last frame\n",
    "                padding_back = tf.repeat(x_padded[-1:, :], pad_rows, axis=0)\n",
    "                x_padded = tf.concat([x_padded, padding_back], axis=0)\n",
    "            else:\n",
    "                if PADDING ==1: #padding at start and end\n",
    "                    if pad_rows %2 == 0: #if pad_rows is even\n",
    "                        paddings = [[pad_rows//2, pad_rows//2], [0, 0], [0, 0]]\n",
    "                    else: #if pad_rows is odd\n",
    "                        paddings = [[pad_rows//2+1, pad_rows//2], [0, 0], [0, 0]]\n",
    "                elif PADDING ==2: #padding only at the end of sequence\n",
    "                    paddings = [[0, pad_rows], [0, 0], [0, 0]]\n",
    "                x_padded = tf.pad(x_padded, paddings, mode='CONSTANT', constant_values=CONSTANT_VALUE)\n",
    "\n",
    "        x = x_padded\n",
    "        current_rows = tf.shape(x)[0]\n",
    "\n",
    "        #interpolate single missing values\n",
    "        x = pd.DataFrame(np.array(x).flatten()).interpolate(method='linear', limit=2, limit_direction='both')\n",
    "        #fill missing values with zeros\n",
    "        x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
    "        \n",
    "        #reshape data to 2D or 3D array\n",
    "        if FLATTEN:\n",
    "            x = tf.reshape(x, (1, current_rows*INPUT_SHAPE[1]))\n",
    "        else:\n",
    "            x = tf.reshape(x, (1, current_rows, INPUT_SHAPE[1]))\n",
    "\n",
    "        return x\n",
    "\n",
    "#define converter using generated layer\n",
    "feature_converter = FeatureGen()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Time Prediction / Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Press \"Q\" to interrupt the camera feed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial setup\n",
    "sequence = [] # to collect all 22 frames for prediction\n",
    "predictions = []\n",
    "threshold = 0.5 # confidence metrics (only render prediction results, if confidence is above threshold)\n",
    "sentence = []\n",
    "sign_predicted = '...'\n",
    "stage = 0 # stage of the game level\n",
    "\n",
    "cap = cv2.VideoCapture(0) # grabbing webcam\n",
    "MODE = -1 # initial mode for menu\n",
    "L_RENDER = 0 # initial state without render\n",
    "PREDICT = 0\n",
    "\n",
    "# set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic: \n",
    "    while cap.isOpened(): # loop through all frames \n",
    "        key = cv2.waitKey(10) & 0xFF\n",
    "        # read feed\n",
    "        ret, frame = cap.read()\n",
    "        # lower quality for zoom transmission\n",
    "        #frame = cv2.resize(frame, (0,0), fx = 0.75, fy = 0.75)\n",
    "        # make detections \n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        #draw_landmarks(image, results)\n",
    "        if L_RENDER == 1: \n",
    "            draw_styled_landmarks(image, results)\n",
    "\n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results) # extract keypoints x, y, z for face, left_hand, pose, right_hand from mediapipe holistic predictions, keypoints.shape e.g. (543, 3)\n",
    "\n",
    "        # Start menu\n",
    "        if MODE == -1: \n",
    "\n",
    "            # transparent box\n",
    "            overlay = image.copy()\n",
    "            cv2.rectangle(image, (0, 0), (1280, 100), color_coral, -1)\n",
    "            cv2.rectangle(image, (0, 100), (1280, 620), color_turquoise, -1)\n",
    "            cv2.rectangle(image, (0, 620), (1280, 720), color_coral, -1)\n",
    "            alpha = 0.5  # Transparency factor\n",
    "\n",
    "            # Following line overlays transparent rectangle over the image\n",
    "            image = cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0)\n",
    "\n",
    "            text_indent = 300\n",
    "            cv2.putText(image, 'SignMeUp!', (text_indent+50, 175+50), cv2.FONT_HERSHEY_SIMPLEX, 4, color_bg, 6, cv2.LINE_AA)\n",
    "            cv2.putText(image, '(Demo)', (text_indent+250, 230+50), cv2.FONT_ITALIC, 1, color_bg, 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'Press \"0\" for basic mode', (text_indent+50, 300+35), cv2.FONT_ITALIC, 1, color_coral, 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'Press \"1\" for advanced mode', (text_indent+50, 340+35), cv2.FONT_ITALIC, 1, color_coral, 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'Press \"g\" for playing the tutorial game', (text_indent+50, 380+35), cv2.FONT_ITALIC, 1, color_coral, 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'Press \"r\" to reset', (text_indent+50, 420+35), cv2.FONT_ITALIC, 1, color_coral, 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'Press \"m\" to return to menu', (text_indent+50, 460+35), cv2.FONT_ITALIC, 1, color_coral, 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'Press \"l\" to turn on/off landmarks', (text_indent+50, 500+35), cv2.FONT_ITALIC, 1, color_coral, 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, '(C) 2023 SIGNMEUP', (text_indent+175, 550+50), cv2.FONT_HERSHEY_PLAIN, 2, color_bg, 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "            cv2.imshow(\"SignMeUp Real-Time Demo\", image)\n",
    "            \n",
    "            #change modes\n",
    "            if key == ord('1'): \n",
    "                MODE = 1\n",
    "            elif key == ord('0'): \n",
    "                MODE = 0\n",
    "            elif key == ord('g'): \n",
    "                MODE = 2\n",
    "            elif key == ord('q'): \n",
    "                break\n",
    "\n",
    "        # Live mode\n",
    "        elif MODE == 0 or MODE == 1 or MODE == 2:\n",
    "            if MODE == 1:\n",
    "                image = prob_viz(pred, image, SELECTED_SIGNS)\n",
    "            if (results.left_hand_landmarks or results.right_hand_landmarks) and PREDICT: #check if hand landmarks are present: \n",
    "                sequence.append(keypoints) # keep appending keypoints (frames) to a sequence, np.array(sequence).shape e.g. (22, 543, 3)\n",
    "                # pre-processing\n",
    "                model_input = feature_converter(np.array(sequence))\n",
    "\n",
    "                # prediction\n",
    "                pred = model.predict(model_input)[0] # model.fit() expects something in shape (num_sequences, 30, 1662), e.g. (1, 30, 1662) for a single sequence\n",
    "                selected_labels = [LABEL_MAP[x] for x in SELECTED_SIGNS]\n",
    "                pred = pred[selected_labels]\n",
    "                sign_predicted = '...'\n",
    "            \n",
    "            else: #no hands visible\n",
    "                if len(sequence) > 2 : #run predictions if sequence was created\n",
    "\n",
    "                    # pre-processing\n",
    "                    model_input = feature_converter(np.array(sequence))\n",
    "\n",
    "                    # prediction\n",
    "                    pred = model.predict(model_input)[0] # model.fit() expects something in shape (num_sequences, 30, 1662), e.g. (1, 30, 1662) for a single sequence\n",
    "                    selected_labels = [LABEL_MAP[x] for x in SELECTED_SIGNS]\n",
    "                    pred = pred[selected_labels]\n",
    "\n",
    "                    # if the confidence of the most confident prediction is above threshold\n",
    "                    if pred[np.argmax(pred)] > threshold: \n",
    "                        if MODE == 2: #game mode\n",
    "                            level_labels = [LABEL_MAP.get(x) for x in LEVEL[stage]] \n",
    "                            # filter predictions for only signs used in level\n",
    "                            pred_level = []\n",
    "                            for i in range(len(pred)): \n",
    "                                if i in level_labels: \n",
    "                                    pred_level.append(pred[i])\n",
    "                                else: \n",
    "                                    pred_level.append(0)\n",
    "                            # find sign with max probability\n",
    "                            sign_predicted = SELECTED_SIGNS[np.argmax(pred_level)]\n",
    "                            sentence.append(sign_predicted)\n",
    "                        else:\n",
    "                            sentence = []\n",
    "                            sign_predicted = SELECTED_SIGNS[np.argmax(pred)]\n",
    "\n",
    "                    else: #if threshold is not reached\n",
    "                        sign_predicted = '...'\n",
    "                    sequence = [] #resetting sequence for next iteration\n",
    "                else:\n",
    "                    pred = 20 *[0]\n",
    "            \n",
    "            # rendering of predicted sign/word at the top of the feed inside a textbox (whole width: 1280)\n",
    "            overlay = image.copy()\n",
    "            cv2.rectangle(image, (0, 0), (230, 120), color_turquoise, -1)\n",
    "            alpha = 0.5  # Transparency factor\n",
    "            # Following line overlays transparent rectangle over the image\n",
    "            image = cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0)\n",
    "\n",
    "            if PREDICT: \n",
    "                cv2.putText(image, 'You signed:', (10, 40), cv2.FONT_ITALIC, 0.75, color_bg, 1, cv2.LINE_AA)\n",
    "                cv2.putText(image, '\"'+sign_predicted+'\"', (10, 90), cv2.FONT_ITALIC, 1, color_bg, 2, cv2.LINE_AA)\n",
    "            else: \n",
    "                cv2.putText(image, '(press p to start)', (10, 40), cv2.FONT_ITALIC, 0.75, color_bg, 1, cv2.LINE_AA)\n",
    "            \n",
    "            # formatting of level boxes and texts\n",
    "            larea_start = 240\n",
    "            larea_end = 1280\n",
    "            lbox_counts = 5\n",
    "            lbox_spacing = 5\n",
    "            lbox_step = (larea_end - larea_start)/lbox_counts\n",
    "            lbox_width = lbox_step - lbox_spacing\n",
    "            lbox_x_start = [larea_start + n*lbox_step  for n in range(lbox_counts)]\n",
    "            lbox_x_end = [lbox_x_start[n] + lbox_width for n in range(lbox_counts)]\n",
    "\n",
    "            if MODE == 2: # Game mode\n",
    "                cv2.putText(image, f'Level: {stage}', (1125, 700), cv2.FONT_ITALIC, 1, color_coral, 2, cv2.LINE_AA)\n",
    "                cv2.imshow(\"SignMeUp Real-Time Demo\", image)\n",
    "                for i in range(len(LEVEL[0])): \n",
    "                    # box showing the sign to perform in this \"level\" \n",
    "                    overlay = image.copy() # for transparency effect\n",
    "                    alpha = 0.5  # Transparency factor\n",
    "\n",
    "                    if LEVEL[stage][i] in sentence: #if sign is in sentence, it will be colored\n",
    "                        cv2.rectangle(overlay, (int(lbox_x_start[i]), 0), (int(lbox_x_end[i]), 60), color_turquoise, -1)\n",
    "                        image = cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0) # overlays transparent rectangle over the image\n",
    "                        cv2.putText(image, LEVEL[stage][i], (int(lbox_x_start[i])+10, 40), cv2.FONT_ITALIC, 1, color_bg, 1, cv2.LINE_AA)\n",
    "                    else: \n",
    "                        cv2.rectangle(overlay, (int(lbox_x_start[i]), 0), (int(lbox_x_end[i]), 60), color_coral, -1)\n",
    "                        image = cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0) # overlays transparent rectangle over the image\n",
    "                        cv2.putText(image, LEVEL[stage][i], (int(lbox_x_start[i])+10, 40), cv2.FONT_ITALIC, 1, color_bg, 2, cv2.LINE_AA)\n",
    "                    \n",
    "                    \n",
    "                if all(item in sentence for item in LEVEL[stage]): #checking if level is cleared\n",
    "                    if stage == len(LEVEL)-1: #end of tutorial\n",
    "                        cv2.putText(image, 'CONGRATULATIONS!!!', (10, 400), cv2.FONT_ITALIC, 4, color_coral, 10, cv2.LINE_AA)\n",
    "                        cv2.putText(image, 'You finished the tutorial!', (250, 500), cv2.FONT_ITALIC, 2, color_coral, 5, cv2.LINE_AA)\n",
    "                        cv2.putText(image, 'Press \"m\" to return to menu!', (350, 600), cv2.FONT_ITALIC, 1, color_turquoise, 2, cv2.LINE_AA)\n",
    "                        cv2.imshow(\"SignMeUp Real-Time Demo\", image)\n",
    "                    else: #iterating to next level\n",
    "                        stage += 1\n",
    "                        sentence = []\n",
    "                        cv2.putText(image, 'Level cleared!!!', (175, 400), cv2.FONT_ITALIC, 4, color_coral, 10, cv2.LINE_AA)\n",
    "                        cv2.putText(image, 'Press any key to continue to the next level.', (300, 500), cv2.FONT_ITALIC, 1, color_turquoise, 2, cv2.LINE_AA)\n",
    "                        cv2.imshow(\"SignMeUp Real-Time Demo\", image)\n",
    "                        cv2.waitKey(0)\n",
    "                            \n",
    "        # show current frame to screen\n",
    "        cv2.imshow(\"SignMeUp Real-Time Demo\", image)\n",
    "\n",
    "        #change modes\n",
    "        if key == ord('1'): \n",
    "            MODE = 1\n",
    "        elif key == ord('0'): \n",
    "            MODE = 0\n",
    "        elif key == ord('l'): \n",
    "            L_RENDER = not L_RENDER \n",
    "        elif key == ord('g'): \n",
    "            MODE = 2\n",
    "        elif key == ord('r'): \n",
    "            stage = 0\n",
    "            sentence = []\n",
    "            sign_predicted = \"...\"\n",
    "        elif key == ord('m'): \n",
    "            MODE = -1\n",
    "        elif key == ord('p'): \n",
    "            PREDICT = not PREDICT\n",
    "        elif key == ord('q'): \n",
    "            break\n",
    "\n",
    "        \n",
    "# release camera and close feed window \n",
    "cap.release()\n",
    "cv2.destroyAllWindows() \n",
    "cv2.waitKey(1) # some workaround to fix the bug, that window doesn't close\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
