{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook is adapted and modified from the following Youtube tutorial: https://www.youtube.com/watch?v=doDUihpj6ro "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this notebook, real-time predictions can be made on signs you perform in front of your running webcam. \n",
    "\n",
    "### <span style=\"color:green\">Quick User Guide: \n",
    "\n",
    "<span style=\"color:green\">1. Load your trained TF Model.</span>\n",
    "\n",
    "<span style=\"color:green\">2. Copy + paste used Configurations from Franziska's pre-processing notebook</span>\n",
    "\n",
    "<span style=\"color:green\">Optional: 3. Copy + paste used Pre-processing Layer from Franziska's pre-processing notebook</span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import Dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow-macos opencv-python mediapipe-silicon sklearn matplotlib\n",
    "#!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib # original code line from tutorial (he had a windows system)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os # easier file path handling\n",
    "\n",
    "# for camera feed\n",
    "import cv2 # opencv\n",
    "from matplotlib import pyplot as plt # imshow for easy visualization\n",
    "import time # to insert \"sleep\" in between frames\n",
    "import mediapipe as mp # for accessing and reading from webcam\n",
    "\n",
    "# for model re-building\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# for loading .json dictionary\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Load Files</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Load Saved Model</span>\n",
    "\n",
    "### <span style=\"color:green\">Here, you can load your trained TensorFlow model, e.g. LSTM.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = tf.keras.models.load_model('../models/LSTM_model_2_1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sign_to_prediction_index_map.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label map dictionary \n",
    "label_map = json.load(open(\"../data/asl-signs/sign_to_prediction_index_map.json\", \"r\")) \n",
    "\n",
    "# actions to detect\n",
    "actions = np.array(list(label_map.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Franziskas's Configuration :) </span>\n",
    "\n",
    "### <span style=\"color:green\">For now, just copy + paste the \"Configuration\" cell from Franziska's notebook \"TF_Load-PreprocessData.ipynb\" into the following cell :D </span>\n",
    "\n",
    "### <span style=\"color:green\">These configuration must be the same ones, which you used to preprocess the data to train the TensorFlow model, which you loaded above! </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit dataset for quick test\n",
    "QUICK_TEST = False\n",
    "QUICK_LIMIT = 1000\n",
    "\n",
    "#Define length of sequences for padding or cutting; 22 is the median length of all sequences\n",
    "LENGTH = 22\n",
    "\n",
    "#define min or max length of sequences; sequences too long/too short will be dropped\n",
    "#max value of 92 was defined by calculating the interquartile range\n",
    "MIN_LENGTH = 10\n",
    "MAX_LENGTH = 92\n",
    "\n",
    "#final data will be flattened, if false data will be 3 dimensional\n",
    "FLATTEN = False\n",
    "\n",
    "#define initialization of numpy array \n",
    "ARRAY = False #(True=Zeros, False=empty values)\n",
    "\n",
    "#Define padding mode \n",
    "#1 = padding at start&end; 2 = padding at end; 3 = no padding, 4 = copy first/lastframe, 5 = copy last frame)\n",
    "#Note: Mode 3 will give you an error due to different lengths, working on that\n",
    "PADDING = 2\n",
    "CONSTANT_VALUE = 0 #only required for mode 1 and 2; enter tf.constant(float('nan')) for NaN\n",
    "\n",
    "#define if z coordinate will be dropped\n",
    "DROP_Z = True\n",
    "\n",
    "#define if csv file should be filtered\n",
    "CSV_FILTER  = True\n",
    "#define how many participants for test set\n",
    "TEST_COUNT = 5 #5 participants account for ca 23% of dataset\n",
    "#generate test or train dataset (True = Train dataset; False = Test dataset)\n",
    "TRAIN = True #only works if CSV_FILTER is activated\n",
    "#TRAIN = False\n",
    "\n",
    "#define filenames for x and y:\n",
    "feature_data = 'X_train' #x data\n",
    "feature_labels = 'y_train' #y data\n",
    "\n",
    "#use for test dataset\n",
    "#feature_data = 'X_test' #x data\n",
    "#feature_labels = 'y_test' #y data\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "#Defining Landmarks\n",
    "#index ranges for each landmark type\n",
    "#dont change these landmarks\n",
    "FACE = list(range(0, 468))\n",
    "LEFT_HAND = list(range(468, 489))\n",
    "POSE = list(range(489, 522))\n",
    "POSE_UPPER = list(range(489, 510))\n",
    "RIGHT_HAND = list(range(522, 543))\n",
    "LIPS = [61, 185, 40, 39, 37,  0, 267, 269, 270, 409,\n",
    "                 291,146, 91,181, 84, 17, 314, 405, 321, 375, \n",
    "                 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, \n",
    "                 95, 88, 178, 87, 14,317, 402, 318, 324, 308]\n",
    "#defining landmarks that will be merged\n",
    "averaging_sets = [FACE]\n",
    "\n",
    "#generating list with all landmarks selected for preprocessing\n",
    "#change landmarks you want to use here:\n",
    "point_landmarks = LEFT_HAND + POSE_UPPER + RIGHT_HAND + LIPS\n",
    "\n",
    "\n",
    "#calculating sum of total landmarks used\n",
    "LANDMARKS = len(point_landmarks) + len(averaging_sets)\n",
    "print(f'Total count of used landmarks: {LANDMARKS}')\n",
    "\n",
    "#defining input shape for model\n",
    "if DROP_Z:\n",
    "    INPUT_SHAPE = (LENGTH,LANDMARKS*2)\n",
    "else:\n",
    "    INPUT_SHAPE = (LENGTH,LANDMARKS*3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Objects and Functions for MP Holistic Keypoints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MP Holistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing utilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions (later they all go into a python module for multiple use)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mediapipe_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to detect MP Holistic landmarks from an image, e.g. frames of your camera feed\n",
    "def mediapipe_detection(image, model): \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # color conversion BGR to RGB\n",
    "    image.flags.writeable = False                   # image no longer writeable\n",
    "    results = model.process(image)                  # make prediction\n",
    "    image.flags.writeable = True                    # image is writeable again\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # color conversion back to original\n",
    "    return image, results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### draw_styled_landmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to draw landmarks points and connecting lines on top of an image, e.g. on top of your camera feed\n",
    "def draw_styled_landmarks(image, results): \n",
    "    # draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                              mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                              mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "    # draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                              mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                              mp_drawing.DrawingSpec(color=(80,256,121), thickness=2, circle_radius=2)) \n",
    "    # draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                              mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                              mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)) \n",
    "    # draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                              mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                              mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract_keypoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract coordinates (+visibility) of all landmarks --> keypoints\n",
    "# and concatenates everything into a flattened list \n",
    "def extract_keypoints(results): \n",
    "    face = np.array([[r.x, r.y, r.z] for r in results.face_landmarks.landmark]) if results.face_landmarks else np.zeros([468, 3])\n",
    "    left_hand = np.array([[r.x, r.y, r.z] for r in results.left_hand_landmarks.landmark]) if results.left_hand_landmarks else np.zeros([21, 3])\n",
    "    pose = np.array([[r.x, r.y, r.z] for r in results.pose_landmarks.landmark]) if results.pose_landmarks else np.zeros([33, 3]) # x, y, z and extra value visibility\n",
    "    right_hand = np.array([[r.x, r.y, r.z] for r in results.right_hand_landmarks.landmark]) if results.right_hand_landmarks else np.zeros([21, 3])\n",
    "    return np.concatenate([face, pose, left_hand, right_hand]) # original code\n",
    "    # a flattened list with list of all pose, face, left_hand, right_hand landmark x, y, z, (+visibility) coordinates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prob_viz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to visualize predicted word probabilities with a dynamic real-time bar chart\n",
    "def prob_viz(pred, actions, input_frame, color): \n",
    "    output_frame = input_frame.copy() \n",
    "    for num, prob in enumerate(pred): \n",
    "        cv2.rectangle(output_frame, (0,60+num*20), (int(prob*100), 45+num*20), color, -1)\n",
    "        # cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "        cv2.putText(output_frame, actions[num], (0, 45+num*20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1, cv2.LINE_AA)\n",
    "        # cv2.putText(image, 'OpenCV', org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "    return output_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Pre-processing Layer</span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_nan_mean(x, axis=0):\n",
    "    #calculates the mean of a TensorFlow tensor x along a specified axis while ignoring any NaN values in the tensor.\n",
    "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">In the following cell copy + paste the pre-processing layer from Franziska's notebook (,if you added / changed something)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating preprocessing layer that will be added to final model\n",
    "class FeatureGen(tf.keras.layers.Layer):\n",
    "    #defines custom tensorflow layer \n",
    "    def __init__(self):\n",
    "        #initializes layer\n",
    "        super(FeatureGen, self).__init__()\n",
    "    \n",
    "    def call(self, x_in):\n",
    "        #drop z coordinates if required\n",
    "        if DROP_Z:\n",
    "            x_in = x_in[:, :, 0:2]\n",
    "        \n",
    "        #generates list with mean values for landmarks that will be merged\n",
    "        x_list = [tf.expand_dims(tf_nan_mean(x_in[:, av_set[0]:av_set[0]+av_set[1], :], axis=1), axis=1) for av_set in averaging_sets]\n",
    "        #extracts specific columns from input x_in defined by landmarks\n",
    "        x_list.append(tf.gather(x_in, point_landmarks, axis=1))\n",
    "        #concatenates the two tensors from above along axis 1/columns\n",
    "        x = tf.concat(x_list, 1)\n",
    "\n",
    "        #padding to desired length of sequence (defined by LENGTH)\n",
    "        #get current number of rows\n",
    "        x_padded = x\n",
    "        current_rows = tf.shape(x_padded)[0]\n",
    "        #if current number of rows is greater than desired number of rows, truncate excess rows\n",
    "        if current_rows > LENGTH:\n",
    "            x_padded = x_padded[:LENGTH, :, :]\n",
    "\n",
    "        #if current number of rows is less than desired number of rows, add padding\n",
    "        elif current_rows < LENGTH:\n",
    "            #calculate amount of padding needed\n",
    "            pad_rows = LENGTH - current_rows\n",
    "\n",
    "            if PADDING ==4: #copy first/last frame\n",
    "                if pad_rows %2 == 0: #if pad_rows is even\n",
    "                    padding_front = tf.repeat(x_padded[0:1, :], pad_rows//2, axis=0)\n",
    "                    padding_back = tf.repeat(x_padded[-1:, :], pad_rows//2, axis=0)\n",
    "                else: #if pad_rows is odd\n",
    "                    padding_front = tf.repeat(x_padded[0:1, :], (pad_rows//2)+1, axis=0)\n",
    "                    padding_back = tf.repeat(x_padded[-1:, :], pad_rows//2, axis=0)\n",
    "                x_padded = tf.concat([padding_front, x_padded, padding_back], axis=0)\n",
    "            elif PADDING == 5: #copy last frame\n",
    "                padding_back = tf.repeat(x_padded[-1:, :], pad_rows, axis=0)\n",
    "                x_padded = tf.concat([x_padded, padding_back], axis=0)\n",
    "            else:\n",
    "                if PADDING ==1: #padding at start and end\n",
    "                    if pad_rows %2 == 0: #if pad_rows is even\n",
    "                        paddings = [[pad_rows//2, pad_rows//2], [0, 0], [0, 0]]\n",
    "                    else: #if pad_rows is odd\n",
    "                        paddings = [[pad_rows//2+1, pad_rows//2], [0, 0], [0, 0]]\n",
    "                elif PADDING ==2: #padding only at the end of sequence\n",
    "                    paddings = [[0, pad_rows], [0, 0], [0, 0]]\n",
    "                elif PADDING ==3: #no padding\n",
    "                    paddings = [[0, 0], [0, 0], [0, 0]]\n",
    "                x_padded = tf.pad(x_padded, paddings, mode='CONSTANT', constant_values=CONSTANT_VALUE)\n",
    "\n",
    "        x = x_padded\n",
    "        current_rows = tf.shape(x)[0]\n",
    "\n",
    "        #interpolate single missing values\n",
    "        x = pd.DataFrame(np.array(x).flatten()).interpolate(method='linear', limit=2, limit_direction='both')\n",
    "        #fill missing values with zeros\n",
    "        x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
    "        \n",
    "        #reshape data to 2D or 3D array\n",
    "        if FLATTEN:\n",
    "            x = tf.reshape(x, (1, current_rows*INPUT_SHAPE[1]))\n",
    "        else:\n",
    "            x = tf.reshape(x, (1, current_rows, INPUT_SHAPE[1]))\n",
    "\n",
    "        return x\n",
    "\n",
    "#define converter using generated layer\n",
    "feature_converter = FeatureGen()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Time Prediction / Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Press \"Q\" to interrupt the camera feed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables \n",
    "sequence = [] # to collect all 22 frames for prediction\n",
    "sentence = [] # history of all predictions (predicted words)\n",
    "predictions = []\n",
    "threshold = 0.4 # confidence metrics (only render prediction results, if confidence is above threshold)\n",
    "\n",
    "cap = cv2.VideoCapture(0) # grabbing webcam\n",
    "# set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic: \n",
    "    while cap.isOpened(): # loop through all frames \n",
    "\n",
    "        # read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # make detections \n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        #print(results)\n",
    "\n",
    "        #draw_landmarks(image, results)\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results) # extract keypoints x, y, z for face, left_hand, pose, right_hand from mediapipe holistic predictions, keypoints.shape e.g. (543, 3)\n",
    "        sequence.append(keypoints) # keep appending keypoints (frames) to a sequence, np.array(sequence).shape e.g. (22, 543, 3)\n",
    "        sequence = sequence[-LENGTH:] # takes last LENGTH frames of the sequence\n",
    "\n",
    "        # \n",
    "        if len(sequence) == LENGTH: \n",
    "            # pre-processing\n",
    "            model_input = feature_converter(np.array(sequence))\n",
    "            print(f'OMG! Frenzy Franzi is converting your mediapipe input! See how the shape is changing from {np.array(sequence).shape} to {model_input.shape}! SO AWESOME!!!')\n",
    "            \n",
    "            # prediction\n",
    "            pred = model.predict(model_input)[0] # model.fit() expects something in shape (num_sequences, 30, 1662), e.g. (1, 30, 1662) for a single sequence\n",
    "            predictions.append(np.argmax(pred))\n",
    "            #print(actions[np.argmax(res)])\n",
    "\n",
    "            # 3. Visualization logic\n",
    "            # makes sure the last 15 frames had the same prediction (more stable transition from one sign to another) \n",
    "            if np.unique(predictions[-15:])[0]==np.argmax(pred): \n",
    "                # if the confidence of the most confident prediction is above threshold\n",
    "                if pred[np.argmax(pred)] > threshold: \n",
    "                    # if there is already a last prediction\n",
    "                    if len(sentence) > 0: \n",
    "                        # only append the predicted word, if it differs from the last prediction (prevent double actions)\n",
    "                        if actions[np.argmax(pred)] != sentence[-1]: \n",
    "                            sentence.append(actions[np.argmax(pred)])\n",
    "                    # just append if there is no last prediction (first prediction)\n",
    "                    else: \n",
    "                        sentence.append(actions[np.argmax(pred)])\n",
    "\n",
    "            # limit the history to the last 5 predictions\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # viz probabilities\n",
    "            color = (150, 150, 150) # color for bars\n",
    "            print(f'Example prediction for \"{actions[0]}\": {pred[0]}')\n",
    "            image = prob_viz(pred, actions, image, color)\n",
    "\n",
    "        # some rendering\n",
    "        #cv2.rectangle(image, (0, 0), (1280, 40), (200, 200, 200), -1)\n",
    "        #cv2.putText(image, ' '.join(sentence), (323,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (52, 75, 102), 2, cv2.LINE_AA)\n",
    "        # cv2.putText(image, 'OpenCV', org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "        # show to screen\n",
    "        cv2.imshow(\"OpenCV Feed\", image)\n",
    "\n",
    "        # break gracefully \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'): \n",
    "            break \n",
    "        \n",
    "# release camera and close feed window \n",
    "cap.release()\n",
    "cv2.destroyAllWindows() \n",
    "cv2.waitKey(1) # some workaround to fix the bug, that window doesn't close\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Save the recordings and predictions for later use? \n",
    "\n",
    "Add a feature, so that the user can type into a textbox to correct wrong prediction + real-time training / update of the model? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
