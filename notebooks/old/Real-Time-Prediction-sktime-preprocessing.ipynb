{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook is adapted and modified from the following Youtube tutorial: https://www.youtube.com/watch?v=doDUihpj6ro "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this notebook, real-time predictions can be made on signs you perform in front of your running webcam. \n",
    "\n",
    "### <span style=\"color:green\">Quick User Guide: \n",
    "\n",
    "<span style=\"color:green\">1. Load your trained TF Model.</span>\n",
    "\n",
    "<span style=\"color:green\">2. Copy + paste used Configurations from Franziska's pre-processing notebook</span>\n",
    "\n",
    "<span style=\"color:green\">Optional: 3. Copy + paste used Pre-processing Layer from Franziska's pre-processing notebook</span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import Dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-macos in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (2.12.0)\n",
      "Requirement already satisfied: opencv-python in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (4.7.0.72)\n",
      "Requirement already satisfied: mediapipe-silicon in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (0.9.1)\n",
      "Requirement already satisfied: sklearn in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (0.0.post4)\n",
      "Requirement already satisfied: matplotlib in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (3.7.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (23.3.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (3.8.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (0.4.8)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (16.0.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (58.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (1.54.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (2.12.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (2.12.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from mediapipe-silicon) (23.1.0)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from mediapipe-silicon) (4.7.0.72)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow-macos) (0.40.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from jax>=0.3.15->tensorflow-macos) (0.1.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from jax>=0.3.15->tensorflow-macos) (1.10.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (0.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-macos) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-macos) (6.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-macos) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-macos) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow-macos opencv-python mediapipe-silicon sklearn matplotlib\n",
    "#!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib # original code line from tutorial (he had a windows system)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os # easier file path handling\n",
    "\n",
    "# for camera feed\n",
    "import cv2 # opencv\n",
    "from matplotlib import pyplot as plt # imshow for easy visualization\n",
    "import time # to insert \"sleep\" in between frames\n",
    "import mediapipe as mp # for accessing and reading from webcam\n",
    "\n",
    "# for model re-building\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import sktime\n",
    "from sktime.classification.kernel_based import RocketClassifier\n",
    "\n",
    "# for loading .json dictionary\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Load Files</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Load Saved Model</span>\n",
    "\n",
    "### <span style=\"color:green\">Here, you can load your trained TensorFlow model, e.g. LSTM.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pkl model RR_F_02.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RidgeClassifierCV from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelBinarizer from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model_name = 'RR_F_02.pkl'\n",
    "model_ext = model_name.split('.')[-1]\n",
    "\n",
    "if model_ext == 'pkl': \n",
    "    model = pickle.load(open('../models/'+model_name, 'rb'))\n",
    "    print('Loaded ' + model_ext + ' model ' + model_name)\n",
    "elif model_ext == 'h5': \n",
    "    model = tf.keras.models.load_model('../models/'+model_name)\n",
    "    print('Loaded ' + model_ext + ' model ' + model_name)\n",
    "    model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sign_to_prediction_index_map.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label map dictionary \n",
    "label_map = json.load(open(\"../data/asl-signs/sign_to_prediction_index_map.json\", \"r\")) \n",
    "\n",
    "# actions to detect\n",
    "actions = np.array(list(label_map.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Franziskas's Configuration :) </span>\n",
    "\n",
    "### <span style=\"color:green\">For now, just copy + paste the \"Configuration\" cell from Franziska's notebook \"TF_Load-PreprocessData.ipynb\" into the following cell :D </span>\n",
    "\n",
    "### <span style=\"color:green\">These configuration must be the same ones, which you used to preprocess the data to train the TensorFlow model, which you loaded above! </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of used landmarks: 42\n",
      "(22, 84)\n"
     ]
    }
   ],
   "source": [
    "#limit dataset for quick test\n",
    "QUICK_TEST = True\n",
    "QUICK_LIMIT = 500\n",
    "\n",
    "#Define length of sequences for padding or cutting; 22 is the median length of all sequences\n",
    "LENGTH = 22\n",
    "\n",
    "#define min or max length of sequences; sequences too long/too short will be dropped\n",
    "#max value of 92 was defined by calculating the interquartile range\n",
    "MIN_LENGTH = 10\n",
    "MAX_LENGTH = 92\n",
    "\n",
    "#final data will be flattened, if false data will be 3 dimensional\n",
    "FLATTEN = False\n",
    "\n",
    "#define initialization of numpy array \n",
    "ARRAY = False #(True=Zeros, False=empty values)\n",
    "\n",
    "#Define padding mode \n",
    "#1 = padding at start&end; 2 = padding at end; 3 = no padding, 4 = copy first/lastframe, 5 = copy last frame)\n",
    "#Note: Mode 3 will give you an error due to different lengths, working on that\n",
    "PADDING = 2\n",
    "CONSTANT_VALUE = 0 #only required for mode 1 and 2; enter tf.constant(float('nan')) for NaN\n",
    "\n",
    "#define if z coordinate will be dropped\n",
    "DROP_Z = True\n",
    "\n",
    "#mirror, flips x coordinate for data augmentation\n",
    "MIRROR = True\n",
    "\n",
    "#define if csv file should be filtered\n",
    "CSV_FILTER  = True\n",
    "#define how many participants for test set\n",
    "TEST_COUNT = 5 #5 participants account for ca 23% of dataset\n",
    "#generate test or train dataset (True = Train dataset; False = Test dataset)\n",
    "#TRAIN = True #only works if CSV_FILTER is activated\n",
    "TRAIN = False\n",
    "\n",
    "#define filenames for x and y:\n",
    "#feature_data = 'X_train' #x data\n",
    "#feature_labels = 'y_train' #y data\n",
    "\n",
    "#use for test dataset\n",
    "feature_data = 'X_test' #x data\n",
    "feature_labels = 'y_test' #y data\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "#Defining Landmarks\n",
    "#index ranges for each landmark type\n",
    "#dont change these landmarks\n",
    "FACE = list(range(0, 468))\n",
    "LEFT_HAND = list(range(468, 489))\n",
    "POSE = list(range(489, 522))\n",
    "POSE_UPPER = list(range(489, 510))\n",
    "RIGHT_HAND = list(range(522, 543))\n",
    "LIPS = [61, 185, 40, 39, 37,  0, 267, 269, 270, 409,\n",
    "                 291,146, 91,181, 84, 17, 314, 405, 321, 375, \n",
    "                 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, \n",
    "                 95, 88, 178, 87, 14,317, 402, 318, 324, 308]\n",
    "#defining landmarks that will be merged\n",
    "averaging_sets = []\n",
    "\n",
    "#generating list with all landmarks selected for preprocessing\n",
    "#change landmarks you want to use here:\n",
    "point_landmarks = LEFT_HAND + RIGHT_HAND\n",
    "\n",
    "#calculating sum of total landmarks used\n",
    "LANDMARKS = len(point_landmarks) + len(averaging_sets)\n",
    "print(f'Total count of used landmarks: {LANDMARKS}')\n",
    "\n",
    "#defining input shape for model\n",
    "if DROP_Z:\n",
    "    INPUT_SHAPE = (LENGTH,LANDMARKS*2)\n",
    "else:\n",
    "    INPUT_SHAPE = (LENGTH,LANDMARKS*3)\n",
    "print(INPUT_SHAPE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Objects and Functions for MP Holistic Keypoints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MP Holistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing utilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions (later they all go into a python module for multiple use)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mediapipe_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to detect MP Holistic landmarks from an image, e.g. frames of your camera feed\n",
    "def mediapipe_detection(image, model): \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # color conversion BGR to RGB\n",
    "    image.flags.writeable = False                   # image no longer writeable\n",
    "    results = model.process(image)                  # make prediction\n",
    "    image.flags.writeable = True                    # image is writeable again\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # color conversion back to original\n",
    "    return image, results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### draw_styled_landmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to draw landmarks points and connecting lines on top of an image, e.g. on top of your camera feed\n",
    "def draw_styled_landmarks(image, results): \n",
    "    # draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                              mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                              mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "    # draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                              mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                              mp_drawing.DrawingSpec(color=(80,256,121), thickness=2, circle_radius=2)) \n",
    "    # draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                              mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                              mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)) \n",
    "    # draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                              mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                              mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract_keypoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract coordinates (+visibility) of all landmarks --> keypoints\n",
    "# and concatenates everything into a flattened list \n",
    "def extract_keypoints(results): \n",
    "    face = np.array([[r.x, r.y, r.z] for r in results.face_landmarks.landmark]) if results.face_landmarks else np.zeros([468, 3])\n",
    "    left_hand = np.array([[r.x, r.y, r.z] for r in results.left_hand_landmarks.landmark]) if results.left_hand_landmarks else np.zeros([21, 3])\n",
    "    pose = np.array([[r.x, r.y, r.z] for r in results.pose_landmarks.landmark]) if results.pose_landmarks else np.zeros([33, 3]) # x, y, z and extra value visibility\n",
    "    right_hand = np.array([[r.x, r.y, r.z] for r in results.right_hand_landmarks.landmark]) if results.right_hand_landmarks else np.zeros([21, 3])\n",
    "    return np.concatenate([face, pose, left_hand, right_hand]) # original code\n",
    "    # a flattened list with list of all pose, face, left_hand, right_hand landmark x, y, z, (+visibility) coordinates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prob_viz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to visualize predicted word probabilities with a dynamic real-time bar chart\n",
    "def prob_viz(pred, actions, input_frame, color): \n",
    "    output_frame = input_frame.copy() \n",
    "    for num, prob in enumerate(pred): \n",
    "        cv2.rectangle(output_frame, (0,60+num*20), (int(prob*100), 45+num*20), color, -1)\n",
    "        # cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "        cv2.putText(output_frame, actions[num], (0, 45+num*20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1, cv2.LINE_AA)\n",
    "        # cv2.putText(image, 'OpenCV', org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "    return output_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Pre-processing Layer</span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_nan_mean(x, axis=0):\n",
    "    #calculates the mean of a TensorFlow tensor x along a specified axis while ignoring any NaN values in the tensor.\n",
    "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">In the following cell copy + paste the pre-processing layer from Franziska's notebook (,if you added / changed something)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating preprocessing layer that will be added to final model\n",
    "class FeatureGen(tf.keras.layers.Layer):\n",
    "    #defines custom tensorflow layer \n",
    "    def __init__(self):\n",
    "        #initializes layer\n",
    "        super(FeatureGen, self).__init__()\n",
    "    \n",
    "    def call(self, x_in, MIRROR=False):\n",
    "        #drop z coordinates if required\n",
    "        if DROP_Z:\n",
    "            x_in = x_in[:, :, 0:2]\n",
    "        if MIRROR:\n",
    "            #flipping x coordinates\n",
    "            x_in = np.array(x_in)\n",
    "            x_in[:, :, 0] = (x_in[:, :, 0]-1)*(-1)\n",
    "            x_in = tf.convert_to_tensor(x_in)\n",
    "\n",
    "        #generates list with mean values for landmarks that will be merged\n",
    "        x_list = [tf.expand_dims(tf_nan_mean(x_in[:, av_set[0]:av_set[0]+av_set[1], :], axis=1), axis=1) for av_set in averaging_sets]\n",
    "        #extracts specific columns from input x_in defined by landmarks\n",
    "        x_list.append(tf.gather(x_in, point_landmarks, axis=1))\n",
    "\n",
    "        #concatenates the two tensors from above along axis 1/columns\n",
    "        x = tf.concat(x_list, 1)\n",
    "\n",
    "        #padding to desired length of sequence (defined by LENGTH)\n",
    "        #get current number of rows\n",
    "        x_padded = x\n",
    "        current_rows = tf.shape(x_padded)[0]\n",
    "        #if current number of rows is greater than desired number of rows, truncate excess rows\n",
    "        if current_rows > LENGTH:\n",
    "            x_padded = x_padded[:LENGTH, :, :]\n",
    "\n",
    "        #if current number of rows is less than desired number of rows, add padding\n",
    "        elif current_rows < LENGTH:\n",
    "            #calculate amount of padding needed\n",
    "            pad_rows = LENGTH - current_rows\n",
    "\n",
    "            if PADDING ==4: #copy first/last frame\n",
    "                if pad_rows %2 == 0: #if pad_rows is even\n",
    "                    padding_front = tf.repeat(x_padded[0:1, :], pad_rows//2, axis=0)\n",
    "                    padding_back = tf.repeat(x_padded[-1:, :], pad_rows//2, axis=0)\n",
    "                else: #if pad_rows is odd\n",
    "                    padding_front = tf.repeat(x_padded[0:1, :], (pad_rows//2)+1, axis=0)\n",
    "                    padding_back = tf.repeat(x_padded[-1:, :], pad_rows//2, axis=0)\n",
    "                x_padded = tf.concat([padding_front, x_padded, padding_back], axis=0)\n",
    "            elif PADDING == 5: #copy last frame\n",
    "                padding_back = tf.repeat(x_padded[-1:, :], pad_rows, axis=0)\n",
    "                x_padded = tf.concat([x_padded, padding_back], axis=0)\n",
    "            else:\n",
    "                if PADDING ==1: #padding at start and end\n",
    "                    if pad_rows %2 == 0: #if pad_rows is even\n",
    "                        paddings = [[pad_rows//2, pad_rows//2], [0, 0], [0, 0]]\n",
    "                    else: #if pad_rows is odd\n",
    "                        paddings = [[pad_rows//2+1, pad_rows//2], [0, 0], [0, 0]]\n",
    "                elif PADDING ==2: #padding only at the end of sequence\n",
    "                    paddings = [[0, pad_rows], [0, 0], [0, 0]]\n",
    "                elif PADDING ==3: #no padding\n",
    "                    paddings = [[0, 0], [0, 0], [0, 0]]\n",
    "                x_padded = tf.pad(x_padded, paddings, mode='CONSTANT', constant_values=CONSTANT_VALUE)\n",
    "\n",
    "        x = x_padded\n",
    "        current_rows = tf.shape(x)[0]\n",
    "\n",
    "        #interpolate single missing values\n",
    "        x = pd.DataFrame(np.array(x).flatten()).interpolate(method='linear', limit=2, limit_direction='both')\n",
    "        #fill missing values with zeros\n",
    "        x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
    "        \n",
    "        #reshape data to 2D or 3D array\n",
    "        if FLATTEN:\n",
    "            x = tf.reshape(x, (1, current_rows*INPUT_SHAPE[1]))\n",
    "        else:\n",
    "            x = tf.reshape(x, (1, current_rows, INPUT_SHAPE[1]))\n",
    "\n",
    "        return x\n",
    "\n",
    "#define converter using generated layer\n",
    "feature_converter = FeatureGen()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Time Prediction / Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Press \"Q\" to interrupt the camera feed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 22, 84), dtype=float32, numpy=\n",
       "array([[[0.48827308, 0.54781103, 0.5138066 , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.4919893 , 0.5501779 , 0.51671714, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.49286288, 0.5509448 , 0.51795065, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.50966674, 0.5402619 , 0.5294343 , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.50846183, 0.5371434 , 0.52831703, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.50498414, 0.537418  , 0.5265826 , ..., 0.        ,\n",
       "         0.        , 0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "X is not of a supported input data type.X must be in a supported mtype format for Panel, found <class 'tensorflow.python.framework.ops.EagerTensor'>Use datatypes.check_is_mtype to check conformance with specifications.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m model_input \u001b[39m=\u001b[39m feature_converter(np\u001b[39m.\u001b[39marray(sequence))\n\u001b[1;32m     31\u001b[0m \u001b[39m#print(f'OMG! Frenzy Franzi is converting your mediapipe input! See how the shape is changing from {np.array(sequence).shape} to {model_input.shape}! SO AWESOME!!!')\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[39m# prediction\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(model_input)\u001b[39m.\u001b[39mflatten() \u001b[39m# model.fit() expects something in shape (num_sequences, 30, 1662), e.g. (1, 30, 1662) for a single sequence\u001b[39;00m\n\u001b[1;32m     35\u001b[0m predictions\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39margmax(pred))\n\u001b[1;32m     36\u001b[0m \u001b[39m#print(actions[np.argmax(res)])\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[39m# 3. Visualization logic\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m# makes sure the last 15 frames had the same prediction (more stable transition from one sign to another) \u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages/sktime/classification/base.py:233\u001b[0m, in \u001b[0;36mBaseClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_is_fitted()\n\u001b[1;32m    232\u001b[0m \u001b[39m# boilerplate input checks for predict-like methods\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_convert_X_for_predict(X)\n\u001b[1;32m    235\u001b[0m \u001b[39m# handle the single-class-label case\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_dictionary) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages/sktime/classification/base.py:609\u001b[0m, in \u001b[0;36mBaseClassifier._check_convert_X_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Input checks, capability checks, repeated in all predict/score methods.\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \n\u001b[1;32m    594\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[39mValueError if the capabilities in self._tags do not handle the data.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    608\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_convert(X)\n\u001b[0;32m--> 609\u001b[0m X_metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_classifier_input(\n\u001b[1;32m    610\u001b[0m     X, return_metadata\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMETADATA_REQ_IN_CHECKS\n\u001b[1;32m    611\u001b[0m )\n\u001b[1;32m    612\u001b[0m missing \u001b[39m=\u001b[39m X_metadata[\u001b[39m\"\u001b[39m\u001b[39mhas_nans\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    613\u001b[0m multivariate \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m X_metadata[\u001b[39m\"\u001b[39m\u001b[39mis_univariate\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages/sktime/classification/base.py:721\u001b[0m, in \u001b[0;36mBaseClassifier._check_classifier_input\u001b[0;34m(self, X, y, enforce_min_instances, return_metadata)\u001b[0m\n\u001b[1;32m    717\u001b[0m X_valid, _, X_metadata \u001b[39m=\u001b[39m check_is_scitype(\n\u001b[1;32m    718\u001b[0m     X, scitype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPanel\u001b[39m\u001b[39m\"\u001b[39m, return_metadata\u001b[39m=\u001b[39mreturn_metadata\n\u001b[1;32m    719\u001b[0m )\n\u001b[1;32m    720\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m X_valid:\n\u001b[0;32m--> 721\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    722\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX is not of a supported input data type.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    723\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX must be in a supported mtype format for Panel, found \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(X)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    724\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUse datatypes.check_is_mtype to check conformance \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    725\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwith specifications.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    726\u001b[0m     )\n\u001b[1;32m    727\u001b[0m n_cases \u001b[39m=\u001b[39m X_metadata[\u001b[39m\"\u001b[39m\u001b[39mn_instances\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    728\u001b[0m \u001b[39mif\u001b[39;00m n_cases \u001b[39m<\u001b[39m enforce_min_instances:\n",
      "\u001b[0;31mTypeError\u001b[0m: X is not of a supported input data type.X must be in a supported mtype format for Panel, found <class 'tensorflow.python.framework.ops.EagerTensor'>Use datatypes.check_is_mtype to check conformance with specifications."
     ]
    }
   ],
   "source": [
    "# 1. New detection variables \n",
    "sequence = [] # to collect all 22 frames for prediction\n",
    "sentence = [] # history of all predictions (predicted words)\n",
    "predictions = []\n",
    "threshold = 0.4 # confidence metrics (only render prediction results, if confidence is above threshold)\n",
    "\n",
    "cap = cv2.VideoCapture(0) # grabbing webcam\n",
    "# set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic: \n",
    "    while cap.isOpened(): # loop through all frames \n",
    "\n",
    "        # read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # make detections \n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        #print(results)\n",
    "\n",
    "        #draw_landmarks(image, results)\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results) # extract keypoints x, y, z for face, left_hand, pose, right_hand from mediapipe holistic predictions, keypoints.shape e.g. (543, 3)\n",
    "        sequence.append(keypoints) # keep appending keypoints (frames) to a sequence, np.array(sequence).shape e.g. (22, 543, 3)\n",
    "        sequence = sequence[-LENGTH:] # takes last LENGTH frames of the sequence\n",
    "\n",
    "        # \n",
    "        if len(sequence) == LENGTH: \n",
    "            # pre-processing\n",
    "            model_input = feature_converter(np.array(sequence))\n",
    "            #print(f'OMG! Frenzy Franzi is converting your mediapipe input! See how the shape is changing from {np.array(sequence).shape} to {model_input.shape}! SO AWESOME!!!')\n",
    "            \n",
    "            # prediction\n",
    "            pred = model.predict(model_input).flatten() # model.fit() expects something in shape (num_sequences, 30, 1662), e.g. (1, 30, 1662) for a single sequence\n",
    "            predictions.append(np.argmax(pred))\n",
    "            #print(actions[np.argmax(res)])\n",
    "\n",
    "            # 3. Visualization logic\n",
    "            # makes sure the last 15 frames had the same prediction (more stable transition from one sign to another) \n",
    "            if np.unique(predictions[-30:])[0]==np.argmax(pred): \n",
    "                # if the confidence of the most confident prediction is above threshold\n",
    "                if pred[np.argmax(pred)] > threshold: \n",
    "                    # if there is already a last prediction\n",
    "                    if len(sentence) > 0: \n",
    "                        # only append the predicted word, if it differs from the last prediction (prevent double actions)\n",
    "                        if actions[np.argmax(pred)] != sentence[-1]: \n",
    "                            sentence.append(actions[np.argmax(pred)])\n",
    "                    # just append if there is no last prediction (first prediction)\n",
    "                    else: \n",
    "                        sentence.append(actions[np.argmax(pred)])\n",
    "\n",
    "            # limit the history to the last 5 predictions\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # viz probabilities\n",
    "            color = (150, 150, 150) # color for bars\n",
    "            print(f'Example prediction for \"{actions[0]}\": {pred[0]}')\n",
    "            image = prob_viz(pred, actions, image, color)\n",
    "\n",
    "        # some rendering\n",
    "        #cv2.rectangle(image, (0, 0), (1280, 40), (200, 200, 200), -1)\n",
    "        #cv2.putText(image, ' '.join(sentence), (323,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (52, 75, 102), 2, cv2.LINE_AA)\n",
    "        # cv2.putText(image, 'OpenCV', org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "        # show to screen\n",
    "        cv2.imshow(\"OpenCV Feed\", image)\n",
    "\n",
    "        # break gracefully \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'): \n",
    "            break \n",
    "        \n",
    "# release camera and close feed window \n",
    "cap.release()\n",
    "cv2.destroyAllWindows() \n",
    "cv2.waitKey(1) # some workaround to fix the bug, that window doesn't close\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RocketClassifier(n_jobs=-1, num_kernels=5000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RocketClassifier</label><div class=\"sk-toggleable__content\"><pre>RocketClassifier(n_jobs=-1, num_kernels=5000, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RocketClassifier(n_jobs=-1, num_kernels=5000, random_state=42)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Save the recordings and predictions for later use? \n",
    "\n",
    "Add a feature, so that the user can type into a textbox to correct wrong prediction + real-time training / update of the model? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
