{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow - Loading data - Adjusting the layer to our needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/roberthatch/gislr-feature-data-on-the-shoulders/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tflite in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (2.10.0)\n",
      "Requirement already satisfied: flatbuffers in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tflite) (23.3.3)\n",
      "Requirement already satisfied: numpy in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tflite) (1.23.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tflite-runtime (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tflite-runtime\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tflite_runtime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtflite_runtime\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtflite\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tflite_runtime'"
     ]
    }
   ],
   "source": [
    "%pip install tqdm\n",
    "%pip install tflite-runtime\n",
    "import os\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflite_runtime.interpreter as tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file paths for Kaggle\n",
    "LANDMARK_FILES_DIR = \"/kaggle/input/asl-signs/train_landmark_files\"\n",
    "TRAIN_FILE = \"/kaggle/input/asl-signs/train.csv\"\n",
    "OUTPUT = \"\"\n",
    "label_map = json.load(open(\"/kaggle/input/asl-signs/sign_to_prediction_index_map.json\", \"r\"))\n",
    "\n",
    "#for local notebook, adjust file paths here if required\n",
    "#LANDMARK_FILES_DIR = \"../data/asl-signs/\"\n",
    "#TRAIN_FILE = \"../data/asl-signs/train.csv\"\n",
    "#OUTPUT = \"../data/\" #path to save x and y files\n",
    "#label_map = json.load(open(\"../data/asl-signs/sign_to_prediction_index_map.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of used landmarks: 104\n"
     ]
    }
   ],
   "source": [
    "#limit dataset for quick test\n",
    "QUICK_TEST = False\n",
    "QUICK_LIMIT = 1000\n",
    "\n",
    "#Define length of sequences for padding or cutting; 22 is the median length of all sequences\n",
    "LENGTH = 22\n",
    "\n",
    "#define min or max length of sequences; sequences too long/too short will be dropped\n",
    "#max value of 92 was defined by calculating the interquartile range\n",
    "MIN_LENGTH = 10\n",
    "MAX_LENGTH = 92\n",
    "\n",
    "#final data will be flattened, if false data will be 3 dimensional\n",
    "FLATTEN = False\n",
    "\n",
    "#define initialization of numpy array \n",
    "ARRAY = False #(True=Zeros, False=empty values)\n",
    "\n",
    "#Define padding mode \n",
    "#1 = padding at start&end; 2 = padding at end; 3 = no padding, 4 = copy first/lastframe, 5 = copy last frame)\n",
    "#Note: Mode 3 will give you an error due to different lengths, working on that\n",
    "PADDING = 2\n",
    "CONSTANT_VALUE = 0 #only required for mode 1 and 2; enter tf.constant(float('nan')) for NaN\n",
    "\n",
    "#define if z coordinate will be dropped\n",
    "DROP_Z = True\n",
    "\n",
    "#define if csv file should be filtered\n",
    "CSV_FILTER  = True\n",
    "#define how many participants for test set\n",
    "TEST_COUNT = 5 #5 participants account for ca 23% of dataset\n",
    "#generate test or train dataset (True = Train dataset; False = Test dataset)\n",
    "TRAIN = False #only works if CSV_FILTER is activated\n",
    "#TRAIN = False\n",
    "\n",
    "#define filenames for x and y:\n",
    "#feature_data = 'X_train' #x data\n",
    "#feature_labels = 'y_train' #y data\n",
    "\n",
    "#use for test dataset\n",
    "feature_data = 'X_test' #x data\n",
    "feature_labels = 'y_test' #y data\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "#Defining Landmarks\n",
    "#index ranges for each landmark type\n",
    "#dont change these landmarks\n",
    "FACE = list(range(0, 468))\n",
    "LEFT_HAND = list(range(468, 489))\n",
    "POSE = list(range(489, 522))\n",
    "POSE_UPPER = list(range(489, 510))\n",
    "RIGHT_HAND = list(range(522, 543))\n",
    "LIPS = [61, 185, 40, 39, 37,  0, 267, 269, 270, 409,\n",
    "                 291,146, 91,181, 84, 17, 314, 405, 321, 375, \n",
    "                 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, \n",
    "                 95, 88, 178, 87, 14,317, 402, 318, 324, 308]\n",
    "#defining landmarks that will be merged\n",
    "averaging_sets = [FACE]\n",
    "\n",
    "#generating list with all landmarks selected for preprocessing\n",
    "#change landmarks you want to use here:\n",
    "point_landmarks = LEFT_HAND + POSE_UPPER + RIGHT_HAND + LIPS\n",
    "\n",
    "\n",
    "#calculating sum of total landmarks used\n",
    "LANDMARKS = len(point_landmarks) + len(averaging_sets)\n",
    "print(f'Total count of used landmarks: {LANDMARKS}')\n",
    "\n",
    "#defining input shape for model\n",
    "if DROP_Z:\n",
    "    INPUT_SHAPE = (LENGTH,LANDMARKS*2)\n",
    "else:\n",
    "    INPUT_SHAPE = (LENGTH,LANDMARKS*3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS_PER_FRAME = 543\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    #defines which columns will be read from the file\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    #calculates the number of frames in the data by dividing the length of the data by the number of rows per frame\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    #reshapes the data into a 3D array with shape (n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_nan_mean(x, axis=0):\n",
    "    #calculates the mean of a TensorFlow tensor x along a specified axis while ignoring any NaN values in the tensor.\n",
    "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis)\n",
    "\n",
    "def tf_nan_std(x, axis=0):\n",
    "    #calculates the standard deviation of a tensor x along a specified axis, while ignoring any NaN values in the tensor\n",
    "    d = x - tf_nan_mean(x, axis=axis)\n",
    "    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis))\n",
    "\n",
    "#this function is only required if mean and std will be calculated for specific segments of the data\n",
    "def flatten_means_and_stds(x, axis=0):\n",
    "    #Get means and stds\n",
    "    x_mean = tf_nan_mean(x, axis=0)\n",
    "    x_std  = tf_nan_std(x,  axis=0)\n",
    "    #concats mean and std values for each sequence\n",
    "    x_out = tf.concat([x_mean, x_std], axis=0)\n",
    "    x_out = tf.reshape(x_out, (1, INPUT_SHAPE[1]*2))\n",
    "    #replaces NaN values with zeros\n",
    "    x_out = tf.where(tf.math.is_finite(x_out), x_out, tf.zeros_like(x_out))\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Feature Preprocessing Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating preprocessing layer that will be added to final model\n",
    "class FeatureGen(tf.keras.layers.Layer):\n",
    "    #defines custom tensorflow layer \n",
    "    def __init__(self):\n",
    "        #initializes layer\n",
    "        super(FeatureGen, self).__init__()\n",
    "    \n",
    "    def call(self, x_in):\n",
    "        #drop z coordinates if required\n",
    "        if DROP_Z:\n",
    "            x_in = x_in[:, :, 0:2]\n",
    "        \n",
    "        #generates list with mean values for landmarks that will be merged\n",
    "        x_list = [tf.expand_dims(tf_nan_mean(x_in[:, av_set[0]:av_set[0]+av_set[1], :], axis=1), axis=1) for av_set in averaging_sets]\n",
    "        #extracts specific columns from input x_in defined by landmarks\n",
    "        x_list.append(tf.gather(x_in, point_landmarks, axis=1))\n",
    "        #concatenates the two tensors from above along axis 1/columns\n",
    "        x = tf.concat(x_list, 1)\n",
    "\n",
    "        #padding to desired length of sequence (defined by LENGTH)\n",
    "        #get current number of rows\n",
    "        x_padded = x\n",
    "        current_rows = tf.shape(x_padded)[0]\n",
    "        #if current number of rows is greater than desired number of rows, truncate excess rows\n",
    "        if current_rows > LENGTH:\n",
    "            x_padded = x_padded[:LENGTH, :, :]\n",
    "\n",
    "        #if current number of rows is less than desired number of rows, add padding\n",
    "        elif current_rows < LENGTH:\n",
    "            #calculate amount of padding needed\n",
    "            pad_rows = LENGTH - current_rows\n",
    "\n",
    "            if PADDING ==4: #copy first/last frame\n",
    "                if pad_rows %2 == 0: #if pad_rows is even\n",
    "                    padding_front = tf.repeat(x_padded[0:1, :], pad_rows//2, axis=0)\n",
    "                    padding_back = tf.repeat(x_padded[-1:, :], pad_rows//2, axis=0)\n",
    "                else: #if pad_rows is odd\n",
    "                    padding_front = tf.repeat(x_padded[0:1, :], (pad_rows//2)+1, axis=0)\n",
    "                    padding_back = tf.repeat(x_padded[-1:, :], pad_rows//2, axis=0)\n",
    "                x_padded = tf.concat([padding_front, x_padded, padding_back], axis=0)\n",
    "            elif PADDING == 5: #copy last frame\n",
    "                padding_back = tf.repeat(x_padded[-1:, :], pad_rows, axis=0)\n",
    "                x_padded = tf.concat([x_padded, padding_back], axis=0)\n",
    "            else:\n",
    "                if PADDING ==1: #padding at start and end\n",
    "                    if pad_rows %2 == 0: #if pad_rows is even\n",
    "                        paddings = [[pad_rows//2, pad_rows//2], [0, 0], [0, 0]]\n",
    "                    else: #if pad_rows is odd\n",
    "                        paddings = [[pad_rows//2+1, pad_rows//2], [0, 0], [0, 0]]\n",
    "                elif PADDING ==2: #padding only at the end of sequence\n",
    "                    paddings = [[0, pad_rows], [0, 0], [0, 0]]\n",
    "                elif PADDING ==3: #no padding\n",
    "                    paddings = [[0, 0], [0, 0], [0, 0]]\n",
    "                x_padded = tf.pad(x_padded, paddings, mode='CONSTANT', constant_values=CONSTANT_VALUE)\n",
    "\n",
    "        x = x_padded\n",
    "        current_rows = tf.shape(x)[0]\n",
    "\n",
    "        #interpolate single missing values\n",
    "        x = pd.DataFrame(np.array(x).flatten()).interpolate(method='linear', limit=2, limit_direction='both')\n",
    "        #fill missing values with zeros\n",
    "        x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
    "        \n",
    "        #reshape data to 2D or 3D array\n",
    "        if FLATTEN:\n",
    "            x = tf.reshape(x, (1, current_rows*INPUT_SHAPE[1]))\n",
    "        else:\n",
    "            x = tf.reshape(x, (1, current_rows, INPUT_SHAPE[1]))\n",
    "\n",
    "        return x\n",
    "\n",
    "#define converter using generated layer\n",
    "feature_converter = FeatureGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = tf.keras.models.load_model('/kaggle/input/model/LSTM_model_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLiteModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.prep_inputs = FeatureGen()\n",
    "        self.model = model\n",
    "        \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 543, 3], dtype=tf.float32, name='inputs')])\n",
    "    def call(self, inputs):\n",
    "        x = self.prep_inputs(tf.cast(inputs, dtype=tf.float32))\n",
    "        x = tf.expand_dims(x, axis=0)\n",
    "        outputs = self.model(x)[0, :]\n",
    "        # Return a dictionary with the output tensor\n",
    "        return {'outputs': outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_keras_model = TFLiteModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = \"model.tflite\"\n",
    "with open(model_path, \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip submission.zip $model_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
