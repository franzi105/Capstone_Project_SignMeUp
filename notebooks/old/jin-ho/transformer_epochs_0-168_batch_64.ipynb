{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 200\n",
    "WDIR = '../../data/'\n",
    "OUTDIR = '../../models/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#loading our preprocessed datasets\n",
    "X_train = np.load(WDIR+'X_train.npy')\n",
    "y_train = np.load(WDIR+'y_train.npy')\n",
    "\n",
    "X_test = np.load(WDIR+'X_test.npy')\n",
    "y_test = np.load(WDIR+'y_test.npy')\n",
    "\n",
    "n_classes = len(np.unique(y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 22, 84)]     0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_24 (LayerN  (None, 22, 84)      168         ['input_4[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_12 (Multi  (None, 22, 84)      347220      ['layer_normalization_24[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)           (None, 22, 84)       0           ['multi_head_attention_12[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_24 (TFOpL  (None, 22, 84)      0           ['dropout_27[0][0]',             \n",
      " ambda)                                                           'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_25 (LayerN  (None, 22, 84)      168         ['tf.__operators__.add_24[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)             (None, 22, 4)        340         ['layer_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)           (None, 22, 4)        0           ['conv1d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)             (None, 22, 84)       420         ['dropout_28[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_25 (TFOpL  (None, 22, 84)      0           ['conv1d_25[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_24[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_26 (LayerN  (None, 22, 84)      168         ['tf.__operators__.add_25[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_13 (Multi  (None, 22, 84)      347220      ['layer_normalization_26[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)           (None, 22, 84)       0           ['multi_head_attention_13[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_26 (TFOpL  (None, 22, 84)      0           ['dropout_29[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_25[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_27 (LayerN  (None, 22, 84)      168         ['tf.__operators__.add_26[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_26 (Conv1D)             (None, 22, 4)        340         ['layer_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)           (None, 22, 4)        0           ['conv1d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_27 (Conv1D)             (None, 22, 84)       420         ['dropout_30[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_27 (TFOpL  (None, 22, 84)      0           ['conv1d_27[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_26[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_28 (LayerN  (None, 22, 84)      168         ['tf.__operators__.add_27[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_14 (Multi  (None, 22, 84)      347220      ['layer_normalization_28[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)           (None, 22, 84)       0           ['multi_head_attention_14[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_28 (TFOpL  (None, 22, 84)      0           ['dropout_31[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_27[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_29 (LayerN  (None, 22, 84)      168         ['tf.__operators__.add_28[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_28 (Conv1D)             (None, 22, 4)        340         ['layer_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_32 (Dropout)           (None, 22, 4)        0           ['conv1d_28[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_29 (Conv1D)             (None, 22, 84)       420         ['dropout_32[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_29 (TFOpL  (None, 22, 84)      0           ['conv1d_29[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_28[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_30 (LayerN  (None, 22, 84)      168         ['tf.__operators__.add_29[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_15 (Multi  (None, 22, 84)      347220      ['layer_normalization_30[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_33 (Dropout)           (None, 22, 84)       0           ['multi_head_attention_15[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_30 (TFOpL  (None, 22, 84)      0           ['dropout_33[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_29[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_31 (LayerN  (None, 22, 84)      168         ['tf.__operators__.add_30[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_30 (Conv1D)             (None, 22, 4)        340         ['layer_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)           (None, 22, 4)        0           ['conv1d_30[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_31 (Conv1D)             (None, 22, 84)       420         ['dropout_34[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_31 (TFOpL  (None, 22, 84)      0           ['conv1d_31[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_30[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_3 (Gl  (None, 22)          0           ['tf.__operators__.add_31[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 128)          2944        ['global_average_pooling1d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)           (None, 128)          0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 250)          32250       ['dropout_35[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,428,458\n",
      "Trainable params: 1,428,458\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "392/392 [==============================] - 59s 146ms/step - loss: 5.5223 - sparse_categorical_accuracy: 0.0048 - val_loss: 5.5168 - val_sparse_categorical_accuracy: 0.0050\n",
      "Epoch 2/200\n",
      "392/392 [==============================] - 61s 155ms/step - loss: 5.5050 - sparse_categorical_accuracy: 0.0054 - val_loss: 5.4683 - val_sparse_categorical_accuracy: 0.0058\n",
      "Epoch 3/200\n",
      "392/392 [==============================] - 63s 160ms/step - loss: 5.4086 - sparse_categorical_accuracy: 0.0092 - val_loss: 5.3127 - val_sparse_categorical_accuracy: 0.0089\n",
      "Epoch 4/200\n",
      "392/392 [==============================] - 62s 159ms/step - loss: 5.2911 - sparse_categorical_accuracy: 0.0106 - val_loss: 5.2343 - val_sparse_categorical_accuracy: 0.0094\n",
      "Epoch 5/200\n",
      "392/392 [==============================] - 64s 162ms/step - loss: 5.2268 - sparse_categorical_accuracy: 0.0109 - val_loss: 5.1663 - val_sparse_categorical_accuracy: 0.0109\n",
      "Epoch 6/200\n",
      "392/392 [==============================] - 65s 166ms/step - loss: 5.1862 - sparse_categorical_accuracy: 0.0109 - val_loss: 5.1097 - val_sparse_categorical_accuracy: 0.0120\n",
      "Epoch 7/200\n",
      "392/392 [==============================] - 64s 163ms/step - loss: 5.1445 - sparse_categorical_accuracy: 0.0126 - val_loss: 5.1002 - val_sparse_categorical_accuracy: 0.0136\n",
      "Epoch 8/200\n",
      "392/392 [==============================] - 66s 169ms/step - loss: 5.1061 - sparse_categorical_accuracy: 0.0138 - val_loss: 5.1169 - val_sparse_categorical_accuracy: 0.0163\n",
      "Epoch 9/200\n",
      "392/392 [==============================] - 67s 170ms/step - loss: 5.0720 - sparse_categorical_accuracy: 0.0165 - val_loss: 5.0185 - val_sparse_categorical_accuracy: 0.0190\n",
      "Epoch 10/200\n",
      "392/392 [==============================] - 67s 171ms/step - loss: 5.0378 - sparse_categorical_accuracy: 0.0179 - val_loss: 4.9586 - val_sparse_categorical_accuracy: 0.0209\n",
      "Epoch 11/200\n",
      "392/392 [==============================] - 68s 173ms/step - loss: 5.0074 - sparse_categorical_accuracy: 0.0200 - val_loss: 4.9425 - val_sparse_categorical_accuracy: 0.0221\n",
      "Epoch 12/200\n",
      "392/392 [==============================] - 72s 183ms/step - loss: 4.9781 - sparse_categorical_accuracy: 0.0220 - val_loss: 4.9459 - val_sparse_categorical_accuracy: 0.0230\n",
      "Epoch 13/200\n",
      "392/392 [==============================] - 74s 188ms/step - loss: 4.9593 - sparse_categorical_accuracy: 0.0216 - val_loss: 4.8976 - val_sparse_categorical_accuracy: 0.0260\n",
      "Epoch 14/200\n",
      "392/392 [==============================] - 70s 179ms/step - loss: 4.9307 - sparse_categorical_accuracy: 0.0231 - val_loss: 4.8624 - val_sparse_categorical_accuracy: 0.0265\n",
      "Epoch 15/200\n",
      "392/392 [==============================] - 67s 171ms/step - loss: 4.9045 - sparse_categorical_accuracy: 0.0239 - val_loss: 4.8545 - val_sparse_categorical_accuracy: 0.0281\n",
      "Epoch 16/200\n",
      "392/392 [==============================] - 65s 166ms/step - loss: 4.8827 - sparse_categorical_accuracy: 0.0266 - val_loss: 4.8462 - val_sparse_categorical_accuracy: 0.0299\n",
      "Epoch 17/200\n",
      "392/392 [==============================] - 67s 172ms/step - loss: 4.8710 - sparse_categorical_accuracy: 0.0266 - val_loss: 4.9300 - val_sparse_categorical_accuracy: 0.0259\n",
      "Epoch 18/200\n",
      "392/392 [==============================] - 68s 173ms/step - loss: 4.8560 - sparse_categorical_accuracy: 0.0276 - val_loss: 4.8053 - val_sparse_categorical_accuracy: 0.0315\n",
      "Epoch 19/200\n",
      "392/392 [==============================] - 68s 174ms/step - loss: 4.8305 - sparse_categorical_accuracy: 0.0292 - val_loss: 4.7880 - val_sparse_categorical_accuracy: 0.0320\n",
      "Epoch 20/200\n",
      "392/392 [==============================] - 68s 174ms/step - loss: 4.8111 - sparse_categorical_accuracy: 0.0296 - val_loss: 4.7762 - val_sparse_categorical_accuracy: 0.0345\n",
      "Epoch 21/200\n",
      "392/392 [==============================] - 66s 170ms/step - loss: 4.7989 - sparse_categorical_accuracy: 0.0297 - val_loss: 4.8051 - val_sparse_categorical_accuracy: 0.0304\n",
      "Epoch 22/200\n",
      "392/392 [==============================] - 68s 173ms/step - loss: 4.7743 - sparse_categorical_accuracy: 0.0320 - val_loss: 4.7552 - val_sparse_categorical_accuracy: 0.0350\n",
      "Epoch 23/200\n",
      "392/392 [==============================] - 65s 165ms/step - loss: 4.7649 - sparse_categorical_accuracy: 0.0309 - val_loss: 4.7451 - val_sparse_categorical_accuracy: 0.0369\n",
      "Epoch 24/200\n",
      "392/392 [==============================] - 65s 167ms/step - loss: 4.7527 - sparse_categorical_accuracy: 0.0333 - val_loss: 4.7083 - val_sparse_categorical_accuracy: 0.0371\n",
      "Epoch 25/200\n",
      "392/392 [==============================] - 65s 165ms/step - loss: 4.7324 - sparse_categorical_accuracy: 0.0350 - val_loss: 4.7169 - val_sparse_categorical_accuracy: 0.0353\n",
      "Epoch 26/200\n",
      "392/392 [==============================] - 65s 165ms/step - loss: 4.7176 - sparse_categorical_accuracy: 0.0351 - val_loss: 4.6856 - val_sparse_categorical_accuracy: 0.0391\n",
      "Epoch 27/200\n",
      "392/392 [==============================] - 65s 166ms/step - loss: 4.6980 - sparse_categorical_accuracy: 0.0367 - val_loss: 4.6815 - val_sparse_categorical_accuracy: 0.0393\n",
      "Epoch 28/200\n",
      "392/392 [==============================] - 65s 167ms/step - loss: 4.6841 - sparse_categorical_accuracy: 0.0381 - val_loss: 4.7645 - val_sparse_categorical_accuracy: 0.0358\n",
      "Epoch 29/200\n",
      "392/392 [==============================] - 65s 165ms/step - loss: 4.6690 - sparse_categorical_accuracy: 0.0378 - val_loss: 4.6726 - val_sparse_categorical_accuracy: 0.0403\n",
      "Epoch 30/200\n",
      "392/392 [==============================] - 66s 169ms/step - loss: 4.6518 - sparse_categorical_accuracy: 0.0411 - val_loss: 4.7288 - val_sparse_categorical_accuracy: 0.0391\n",
      "Epoch 31/200\n",
      "392/392 [==============================] - 67s 170ms/step - loss: 4.6222 - sparse_categorical_accuracy: 0.0432 - val_loss: 4.6659 - val_sparse_categorical_accuracy: 0.0438\n",
      "Epoch 32/200\n",
      "392/392 [==============================] - 66s 168ms/step - loss: 4.6147 - sparse_categorical_accuracy: 0.0439 - val_loss: 4.6138 - val_sparse_categorical_accuracy: 0.0479\n",
      "Epoch 33/200\n",
      "392/392 [==============================] - 64s 163ms/step - loss: 4.5908 - sparse_categorical_accuracy: 0.0443 - val_loss: 4.5809 - val_sparse_categorical_accuracy: 0.0513\n",
      "Epoch 34/200\n",
      "392/392 [==============================] - 64s 164ms/step - loss: 4.5710 - sparse_categorical_accuracy: 0.0474 - val_loss: 4.5248 - val_sparse_categorical_accuracy: 0.0518\n",
      "Epoch 35/200\n",
      "392/392 [==============================] - 64s 165ms/step - loss: 4.5486 - sparse_categorical_accuracy: 0.0500 - val_loss: 4.5212 - val_sparse_categorical_accuracy: 0.0564\n",
      "Epoch 36/200\n",
      "392/392 [==============================] - 65s 165ms/step - loss: 4.5267 - sparse_categorical_accuracy: 0.0477 - val_loss: 4.5797 - val_sparse_categorical_accuracy: 0.0513\n",
      "Epoch 37/200\n",
      "392/392 [==============================] - 65s 166ms/step - loss: 4.5153 - sparse_categorical_accuracy: 0.0509 - val_loss: 4.4842 - val_sparse_categorical_accuracy: 0.0598\n",
      "Epoch 38/200\n",
      "392/392 [==============================] - 64s 164ms/step - loss: 4.4856 - sparse_categorical_accuracy: 0.0544 - val_loss: 4.4514 - val_sparse_categorical_accuracy: 0.0607\n",
      "Epoch 39/200\n",
      "392/392 [==============================] - 65s 166ms/step - loss: 4.4666 - sparse_categorical_accuracy: 0.0541 - val_loss: 4.4757 - val_sparse_categorical_accuracy: 0.0574\n",
      "Epoch 40/200\n",
      "392/392 [==============================] - 65s 166ms/step - loss: 4.4511 - sparse_categorical_accuracy: 0.0552 - val_loss: 4.4310 - val_sparse_categorical_accuracy: 0.0650\n",
      "Epoch 41/200\n",
      "392/392 [==============================] - 67s 170ms/step - loss: 4.4317 - sparse_categorical_accuracy: 0.0611 - val_loss: 4.4280 - val_sparse_categorical_accuracy: 0.0631\n",
      "Epoch 42/200\n",
      "392/392 [==============================] - 64s 164ms/step - loss: 4.4110 - sparse_categorical_accuracy: 0.0602 - val_loss: 4.3714 - val_sparse_categorical_accuracy: 0.0697\n",
      "Epoch 43/200\n",
      "392/392 [==============================] - 67s 172ms/step - loss: 4.3955 - sparse_categorical_accuracy: 0.0622 - val_loss: 4.3656 - val_sparse_categorical_accuracy: 0.0692\n",
      "Epoch 44/200\n",
      "392/392 [==============================] - 64s 164ms/step - loss: 4.3798 - sparse_categorical_accuracy: 0.0628 - val_loss: 4.3918 - val_sparse_categorical_accuracy: 0.0670\n",
      "Epoch 45/200\n",
      "392/392 [==============================] - 64s 163ms/step - loss: 4.3658 - sparse_categorical_accuracy: 0.0653 - val_loss: 4.3375 - val_sparse_categorical_accuracy: 0.0725\n",
      "Epoch 46/200\n",
      "392/392 [==============================] - 66s 167ms/step - loss: 4.3473 - sparse_categorical_accuracy: 0.0688 - val_loss: 4.3409 - val_sparse_categorical_accuracy: 0.0658\n",
      "Epoch 47/200\n",
      "392/392 [==============================] - 71s 180ms/step - loss: 4.3336 - sparse_categorical_accuracy: 0.0675 - val_loss: 4.3025 - val_sparse_categorical_accuracy: 0.0761\n",
      "Epoch 48/200\n",
      "392/392 [==============================] - 65s 166ms/step - loss: 4.3261 - sparse_categorical_accuracy: 0.0672 - val_loss: 4.3874 - val_sparse_categorical_accuracy: 0.0684\n",
      "Epoch 49/200\n",
      "392/392 [==============================] - 65s 166ms/step - loss: 4.3089 - sparse_categorical_accuracy: 0.0708 - val_loss: 4.2884 - val_sparse_categorical_accuracy: 0.0753\n",
      "Epoch 50/200\n",
      "392/392 [==============================] - 65s 167ms/step - loss: 4.2871 - sparse_categorical_accuracy: 0.0717 - val_loss: 4.3183 - val_sparse_categorical_accuracy: 0.0737\n",
      "Epoch 51/200\n",
      "392/392 [==============================] - 65s 165ms/step - loss: 4.2696 - sparse_categorical_accuracy: 0.0742 - val_loss: 4.2674 - val_sparse_categorical_accuracy: 0.0777\n",
      "Epoch 52/200\n",
      "392/392 [==============================] - 66s 169ms/step - loss: 4.2549 - sparse_categorical_accuracy: 0.0764 - val_loss: 4.4023 - val_sparse_categorical_accuracy: 0.0652\n",
      "Epoch 53/200\n",
      "392/392 [==============================] - 66s 168ms/step - loss: 4.2497 - sparse_categorical_accuracy: 0.0783 - val_loss: 4.2696 - val_sparse_categorical_accuracy: 0.0812\n",
      "Epoch 54/200\n",
      "392/392 [==============================] - 67s 171ms/step - loss: 4.2272 - sparse_categorical_accuracy: 0.0784 - val_loss: 4.2772 - val_sparse_categorical_accuracy: 0.0783\n",
      "Epoch 55/200\n",
      "392/392 [==============================] - 66s 169ms/step - loss: 4.2209 - sparse_categorical_accuracy: 0.0764 - val_loss: 4.2248 - val_sparse_categorical_accuracy: 0.0805\n",
      "Epoch 56/200\n",
      "392/392 [==============================] - 65s 167ms/step - loss: 4.2038 - sparse_categorical_accuracy: 0.0794 - val_loss: 4.2658 - val_sparse_categorical_accuracy: 0.0807\n",
      "Epoch 57/200\n",
      "392/392 [==============================] - 65s 166ms/step - loss: 4.2073 - sparse_categorical_accuracy: 0.0827 - val_loss: 4.1727 - val_sparse_categorical_accuracy: 0.0922\n",
      "Epoch 58/200\n",
      "392/392 [==============================] - 69s 176ms/step - loss: 4.1832 - sparse_categorical_accuracy: 0.0822 - val_loss: 4.2101 - val_sparse_categorical_accuracy: 0.0871\n",
      "Epoch 59/200\n",
      "392/392 [==============================] - 67s 171ms/step - loss: 4.1728 - sparse_categorical_accuracy: 0.0845 - val_loss: 4.1617 - val_sparse_categorical_accuracy: 0.0906\n",
      "Epoch 60/200\n",
      "392/392 [==============================] - 65s 165ms/step - loss: 4.1688 - sparse_categorical_accuracy: 0.0857 - val_loss: 4.1622 - val_sparse_categorical_accuracy: 0.0903\n",
      "Epoch 61/200\n",
      "392/392 [==============================] - 65s 166ms/step - loss: 4.1478 - sparse_categorical_accuracy: 0.0864 - val_loss: 4.1722 - val_sparse_categorical_accuracy: 0.0924\n",
      "Epoch 62/200\n",
      "392/392 [==============================] - 65s 165ms/step - loss: 4.1443 - sparse_categorical_accuracy: 0.0895 - val_loss: 4.2966 - val_sparse_categorical_accuracy: 0.0813\n",
      "Epoch 63/200\n",
      "392/392 [==============================] - 64s 162ms/step - loss: 4.1331 - sparse_categorical_accuracy: 0.0890 - val_loss: 4.1199 - val_sparse_categorical_accuracy: 0.0983\n",
      "Epoch 64/200\n",
      "392/392 [==============================] - 65s 165ms/step - loss: 4.1142 - sparse_categorical_accuracy: 0.0927 - val_loss: 4.1880 - val_sparse_categorical_accuracy: 0.0876\n",
      "Epoch 65/200\n",
      "392/392 [==============================] - 65s 166ms/step - loss: 4.1031 - sparse_categorical_accuracy: 0.0917 - val_loss: 4.1645 - val_sparse_categorical_accuracy: 0.0924\n",
      "Epoch 66/200\n",
      "392/392 [==============================] - 65s 165ms/step - loss: 4.0930 - sparse_categorical_accuracy: 0.0931 - val_loss: 4.1635 - val_sparse_categorical_accuracy: 0.0940\n",
      "Epoch 67/200\n",
      "392/392 [==============================] - 65s 165ms/step - loss: 4.0847 - sparse_categorical_accuracy: 0.0942 - val_loss: 4.1044 - val_sparse_categorical_accuracy: 0.1010\n",
      "Epoch 68/200\n",
      "392/392 [==============================] - 65s 167ms/step - loss: 4.0805 - sparse_categorical_accuracy: 0.0970 - val_loss: 4.0897 - val_sparse_categorical_accuracy: 0.0999\n",
      "Epoch 69/200\n",
      "392/392 [==============================] - 63s 161ms/step - loss: 4.0691 - sparse_categorical_accuracy: 0.0984 - val_loss: 4.0763 - val_sparse_categorical_accuracy: 0.1063\n",
      "Epoch 70/200\n",
      "392/392 [==============================] - 64s 163ms/step - loss: 4.0606 - sparse_categorical_accuracy: 0.0987 - val_loss: 4.0730 - val_sparse_categorical_accuracy: 0.1027\n",
      "Epoch 71/200\n",
      "392/392 [==============================] - 64s 163ms/step - loss: 4.0470 - sparse_categorical_accuracy: 0.1019 - val_loss: 4.0610 - val_sparse_categorical_accuracy: 0.0989\n",
      "Epoch 72/200\n",
      "392/392 [==============================] - 63s 161ms/step - loss: 4.0418 - sparse_categorical_accuracy: 0.1018 - val_loss: 4.0704 - val_sparse_categorical_accuracy: 0.0973\n",
      "Epoch 73/200\n",
      "392/392 [==============================] - 795s 2s/step - loss: 4.0369 - sparse_categorical_accuracy: 0.1008 - val_loss: 4.1129 - val_sparse_categorical_accuracy: 0.0994\n",
      "Epoch 74/200\n",
      "392/392 [==============================] - 56s 142ms/step - loss: 4.0270 - sparse_categorical_accuracy: 0.1003 - val_loss: 4.0512 - val_sparse_categorical_accuracy: 0.1034\n",
      "Epoch 75/200\n",
      "392/392 [==============================] - 57s 145ms/step - loss: 4.0138 - sparse_categorical_accuracy: 0.1058 - val_loss: 4.0343 - val_sparse_categorical_accuracy: 0.1074\n",
      "Epoch 76/200\n",
      "392/392 [==============================] - 59s 150ms/step - loss: 4.0127 - sparse_categorical_accuracy: 0.1068 - val_loss: 4.0565 - val_sparse_categorical_accuracy: 0.1012\n",
      "Epoch 77/200\n",
      "392/392 [==============================] - 60s 153ms/step - loss: 3.9942 - sparse_categorical_accuracy: 0.1074 - val_loss: 4.0375 - val_sparse_categorical_accuracy: 0.1083\n",
      "Epoch 78/200\n",
      "392/392 [==============================] - 59s 151ms/step - loss: 4.0038 - sparse_categorical_accuracy: 0.1058 - val_loss: 3.9947 - val_sparse_categorical_accuracy: 0.1154\n",
      "Epoch 79/200\n",
      "392/392 [==============================] - 60s 152ms/step - loss: 3.9853 - sparse_categorical_accuracy: 0.1059 - val_loss: 4.0832 - val_sparse_categorical_accuracy: 0.1039\n",
      "Epoch 80/200\n",
      "392/392 [==============================] - 61s 155ms/step - loss: 3.9742 - sparse_categorical_accuracy: 0.1106 - val_loss: 4.0221 - val_sparse_categorical_accuracy: 0.1058\n",
      "Epoch 81/200\n",
      "392/392 [==============================] - 62s 157ms/step - loss: 3.9728 - sparse_categorical_accuracy: 0.1096 - val_loss: 3.9991 - val_sparse_categorical_accuracy: 0.1135\n",
      "Epoch 82/200\n",
      "392/392 [==============================] - 61s 155ms/step - loss: 3.9613 - sparse_categorical_accuracy: 0.1112 - val_loss: 4.0507 - val_sparse_categorical_accuracy: 0.1023\n",
      "Epoch 83/200\n",
      "392/392 [==============================] - 60s 153ms/step - loss: 3.9582 - sparse_categorical_accuracy: 0.1130 - val_loss: 3.9697 - val_sparse_categorical_accuracy: 0.1198\n",
      "Epoch 84/200\n",
      "392/392 [==============================] - 61s 155ms/step - loss: 3.9492 - sparse_categorical_accuracy: 0.1151 - val_loss: 4.0166 - val_sparse_categorical_accuracy: 0.1077\n",
      "Epoch 85/200\n",
      "392/392 [==============================] - 61s 155ms/step - loss: 3.9314 - sparse_categorical_accuracy: 0.1183 - val_loss: 3.9712 - val_sparse_categorical_accuracy: 0.1187\n",
      "Epoch 86/200\n",
      "392/392 [==============================] - 61s 156ms/step - loss: 3.9247 - sparse_categorical_accuracy: 0.1198 - val_loss: 4.0409 - val_sparse_categorical_accuracy: 0.1104\n",
      "Epoch 87/200\n",
      "392/392 [==============================] - 61s 157ms/step - loss: 3.9248 - sparse_categorical_accuracy: 0.1141 - val_loss: 4.0113 - val_sparse_categorical_accuracy: 0.1114\n",
      "Epoch 88/200\n",
      "392/392 [==============================] - 61s 155ms/step - loss: 3.9213 - sparse_categorical_accuracy: 0.1160 - val_loss: 3.9826 - val_sparse_categorical_accuracy: 0.1165\n",
      "Epoch 89/200\n",
      "392/392 [==============================] - 61s 156ms/step - loss: 3.9018 - sparse_categorical_accuracy: 0.1207 - val_loss: 3.9875 - val_sparse_categorical_accuracy: 0.1182\n",
      "Epoch 90/200\n",
      "392/392 [==============================] - 61s 156ms/step - loss: 3.9075 - sparse_categorical_accuracy: 0.1165 - val_loss: 3.9664 - val_sparse_categorical_accuracy: 0.1264\n",
      "Epoch 91/200\n",
      "392/392 [==============================] - 61s 157ms/step - loss: 3.8952 - sparse_categorical_accuracy: 0.1253 - val_loss: 4.0471 - val_sparse_categorical_accuracy: 0.1163\n",
      "Epoch 92/200\n",
      "392/392 [==============================] - 61s 155ms/step - loss: 3.8887 - sparse_categorical_accuracy: 0.1209 - val_loss: 4.0510 - val_sparse_categorical_accuracy: 0.1128\n",
      "Epoch 93/200\n",
      "392/392 [==============================] - 61s 155ms/step - loss: 3.8862 - sparse_categorical_accuracy: 0.1218 - val_loss: 3.9865 - val_sparse_categorical_accuracy: 0.1155\n",
      "Epoch 94/200\n",
      "392/392 [==============================] - 60s 154ms/step - loss: 3.8904 - sparse_categorical_accuracy: 0.1230 - val_loss: 3.9443 - val_sparse_categorical_accuracy: 0.1202\n",
      "Epoch 95/200\n",
      "392/392 [==============================] - 62s 158ms/step - loss: 3.8758 - sparse_categorical_accuracy: 0.1245 - val_loss: 3.9352 - val_sparse_categorical_accuracy: 0.1274\n",
      "Epoch 96/200\n",
      "392/392 [==============================] - 62s 159ms/step - loss: 3.8663 - sparse_categorical_accuracy: 0.1257 - val_loss: 3.9196 - val_sparse_categorical_accuracy: 0.1262\n",
      "Epoch 97/200\n",
      "392/392 [==============================] - 62s 159ms/step - loss: 3.8484 - sparse_categorical_accuracy: 0.1293 - val_loss: 3.9226 - val_sparse_categorical_accuracy: 0.1234\n",
      "Epoch 98/200\n",
      "392/392 [==============================] - 61s 157ms/step - loss: 3.8676 - sparse_categorical_accuracy: 0.1256 - val_loss: 3.9412 - val_sparse_categorical_accuracy: 0.1264\n",
      "Epoch 99/200\n",
      "392/392 [==============================] - 65s 166ms/step - loss: 3.8552 - sparse_categorical_accuracy: 0.1268 - val_loss: 3.9424 - val_sparse_categorical_accuracy: 0.1232\n",
      "Epoch 100/200\n",
      "392/392 [==============================] - 66s 168ms/step - loss: 3.8441 - sparse_categorical_accuracy: 0.1298 - val_loss: 3.8670 - val_sparse_categorical_accuracy: 0.1301\n",
      "Epoch 101/200\n",
      "392/392 [==============================] - 63s 161ms/step - loss: 3.8332 - sparse_categorical_accuracy: 0.1325 - val_loss: 3.9232 - val_sparse_categorical_accuracy: 0.1299\n",
      "Epoch 102/200\n",
      "392/392 [==============================] - 61s 156ms/step - loss: 3.8302 - sparse_categorical_accuracy: 0.1324 - val_loss: 3.9230 - val_sparse_categorical_accuracy: 0.1245\n",
      "Epoch 103/200\n",
      "392/392 [==============================] - 61s 154ms/step - loss: 3.8306 - sparse_categorical_accuracy: 0.1315 - val_loss: 3.9812 - val_sparse_categorical_accuracy: 0.1194\n",
      "Epoch 104/200\n",
      "392/392 [==============================] - 60s 154ms/step - loss: 3.8174 - sparse_categorical_accuracy: 0.1358 - val_loss: 3.9751 - val_sparse_categorical_accuracy: 0.1194\n",
      "Epoch 105/200\n",
      "392/392 [==============================] - 61s 156ms/step - loss: 3.8164 - sparse_categorical_accuracy: 0.1327 - val_loss: 3.9004 - val_sparse_categorical_accuracy: 0.1304\n",
      "Epoch 106/200\n",
      "392/392 [==============================] - 62s 157ms/step - loss: 3.8059 - sparse_categorical_accuracy: 0.1345 - val_loss: 3.9888 - val_sparse_categorical_accuracy: 0.1261\n",
      "Epoch 107/200\n",
      "392/392 [==============================] - 62s 157ms/step - loss: 3.8039 - sparse_categorical_accuracy: 0.1342 - val_loss: 3.8499 - val_sparse_categorical_accuracy: 0.1398\n",
      "Epoch 108/200\n",
      "392/392 [==============================] - 61s 156ms/step - loss: 3.8033 - sparse_categorical_accuracy: 0.1340 - val_loss: 3.8521 - val_sparse_categorical_accuracy: 0.1377\n",
      "Epoch 109/200\n",
      "392/392 [==============================] - 60s 154ms/step - loss: 3.8010 - sparse_categorical_accuracy: 0.1337 - val_loss: 3.8669 - val_sparse_categorical_accuracy: 0.1389\n",
      "Epoch 110/200\n",
      "392/392 [==============================] - 61s 155ms/step - loss: 3.7924 - sparse_categorical_accuracy: 0.1381 - val_loss: 3.8776 - val_sparse_categorical_accuracy: 0.1371\n",
      "Epoch 111/200\n",
      "392/392 [==============================] - 61s 154ms/step - loss: 3.7869 - sparse_categorical_accuracy: 0.1400 - val_loss: 3.8424 - val_sparse_categorical_accuracy: 0.1403\n",
      "Epoch 112/200\n",
      "392/392 [==============================] - 61s 155ms/step - loss: 3.7776 - sparse_categorical_accuracy: 0.1396 - val_loss: 3.8838 - val_sparse_categorical_accuracy: 0.1350\n",
      "Epoch 113/200\n",
      "392/392 [==============================] - 61s 155ms/step - loss: 3.7679 - sparse_categorical_accuracy: 0.1376 - val_loss: 3.9302 - val_sparse_categorical_accuracy: 0.1326\n",
      "Epoch 114/200\n",
      "392/392 [==============================] - 62s 159ms/step - loss: 3.7690 - sparse_categorical_accuracy: 0.1403 - val_loss: 3.8430 - val_sparse_categorical_accuracy: 0.1462\n",
      "Epoch 115/200\n",
      "392/392 [==============================] - 62s 157ms/step - loss: 3.7610 - sparse_categorical_accuracy: 0.1416 - val_loss: 3.8334 - val_sparse_categorical_accuracy: 0.1389\n",
      "Epoch 116/200\n",
      "392/392 [==============================] - 62s 158ms/step - loss: 3.7544 - sparse_categorical_accuracy: 0.1455 - val_loss: 3.8551 - val_sparse_categorical_accuracy: 0.1413\n",
      "Epoch 117/200\n",
      "392/392 [==============================] - 67s 171ms/step - loss: 3.7522 - sparse_categorical_accuracy: 0.1427 - val_loss: 3.8381 - val_sparse_categorical_accuracy: 0.1400\n",
      "Epoch 118/200\n",
      "392/392 [==============================] - 62s 158ms/step - loss: 3.7532 - sparse_categorical_accuracy: 0.1408 - val_loss: 3.8158 - val_sparse_categorical_accuracy: 0.1421\n",
      "Epoch 119/200\n",
      "392/392 [==============================] - 64s 162ms/step - loss: 3.7398 - sparse_categorical_accuracy: 0.1456 - val_loss: 3.8880 - val_sparse_categorical_accuracy: 0.1390\n",
      "Epoch 120/200\n",
      "392/392 [==============================] - 62s 159ms/step - loss: 3.7370 - sparse_categorical_accuracy: 0.1430 - val_loss: 3.8172 - val_sparse_categorical_accuracy: 0.1484\n",
      "Epoch 121/200\n",
      "392/392 [==============================] - 61s 156ms/step - loss: 3.7388 - sparse_categorical_accuracy: 0.1443 - val_loss: 3.8243 - val_sparse_categorical_accuracy: 0.1441\n",
      "Epoch 122/200\n",
      "392/392 [==============================] - 62s 157ms/step - loss: 3.7339 - sparse_categorical_accuracy: 0.1449 - val_loss: 3.7969 - val_sparse_categorical_accuracy: 0.1489\n",
      "Epoch 123/200\n",
      "392/392 [==============================] - 62s 159ms/step - loss: 3.7344 - sparse_categorical_accuracy: 0.1459 - val_loss: 3.8303 - val_sparse_categorical_accuracy: 0.1449\n",
      "Epoch 124/200\n",
      "392/392 [==============================] - 62s 157ms/step - loss: 3.7152 - sparse_categorical_accuracy: 0.1467 - val_loss: 3.9245 - val_sparse_categorical_accuracy: 0.1333\n",
      "Epoch 125/200\n",
      "392/392 [==============================] - 62s 158ms/step - loss: 3.7238 - sparse_categorical_accuracy: 0.1460 - val_loss: 3.8740 - val_sparse_categorical_accuracy: 0.1411\n",
      "Epoch 126/200\n",
      "392/392 [==============================] - 67s 171ms/step - loss: 3.7234 - sparse_categorical_accuracy: 0.1468 - val_loss: 3.7964 - val_sparse_categorical_accuracy: 0.1477\n",
      "Epoch 127/200\n",
      "392/392 [==============================] - 63s 161ms/step - loss: 3.7025 - sparse_categorical_accuracy: 0.1503 - val_loss: 3.8496 - val_sparse_categorical_accuracy: 0.1435\n",
      "Epoch 128/200\n",
      "392/392 [==============================] - 59s 152ms/step - loss: 3.7156 - sparse_categorical_accuracy: 0.1484 - val_loss: 3.7791 - val_sparse_categorical_accuracy: 0.1512\n",
      "Epoch 129/200\n",
      "392/392 [==============================] - 62s 159ms/step - loss: 3.7030 - sparse_categorical_accuracy: 0.1519 - val_loss: 3.8237 - val_sparse_categorical_accuracy: 0.1416\n",
      "Epoch 130/200\n",
      "392/392 [==============================] - 60s 152ms/step - loss: 3.6991 - sparse_categorical_accuracy: 0.1484 - val_loss: 3.7610 - val_sparse_categorical_accuracy: 0.1539\n",
      "Epoch 131/200\n",
      "392/392 [==============================] - 60s 153ms/step - loss: 3.6933 - sparse_categorical_accuracy: 0.1543 - val_loss: 3.7860 - val_sparse_categorical_accuracy: 0.1520\n",
      "Epoch 132/200\n",
      "392/392 [==============================] - 59s 150ms/step - loss: 3.6873 - sparse_categorical_accuracy: 0.1530 - val_loss: 3.9155 - val_sparse_categorical_accuracy: 0.1309\n",
      "Epoch 133/200\n",
      "392/392 [==============================] - 60s 152ms/step - loss: 3.7092 - sparse_categorical_accuracy: 0.1511 - val_loss: 3.8473 - val_sparse_categorical_accuracy: 0.1446\n",
      "Epoch 134/200\n",
      "392/392 [==============================] - 62s 158ms/step - loss: 3.6920 - sparse_categorical_accuracy: 0.1499 - val_loss: 3.8903 - val_sparse_categorical_accuracy: 0.1400\n",
      "Epoch 135/200\n",
      "392/392 [==============================] - 64s 162ms/step - loss: 3.6880 - sparse_categorical_accuracy: 0.1535 - val_loss: 3.8130 - val_sparse_categorical_accuracy: 0.1477\n",
      "Epoch 136/200\n",
      "392/392 [==============================] - 61s 156ms/step - loss: 3.6823 - sparse_categorical_accuracy: 0.1523 - val_loss: 3.7496 - val_sparse_categorical_accuracy: 0.1544\n",
      "Epoch 137/200\n",
      "392/392 [==============================] - 61s 157ms/step - loss: 3.6747 - sparse_categorical_accuracy: 0.1539 - val_loss: 3.7777 - val_sparse_categorical_accuracy: 0.1563\n",
      "Epoch 138/200\n",
      "392/392 [==============================] - 62s 158ms/step - loss: 3.6665 - sparse_categorical_accuracy: 0.1533 - val_loss: 3.7555 - val_sparse_categorical_accuracy: 0.1604\n",
      "Epoch 139/200\n",
      "392/392 [==============================] - 61s 157ms/step - loss: 3.6662 - sparse_categorical_accuracy: 0.1569 - val_loss: 3.8220 - val_sparse_categorical_accuracy: 0.1472\n",
      "Epoch 140/200\n",
      "392/392 [==============================] - 61s 156ms/step - loss: 3.6611 - sparse_categorical_accuracy: 0.1585 - val_loss: 3.7664 - val_sparse_categorical_accuracy: 0.1561\n",
      "Epoch 141/200\n",
      "392/392 [==============================] - 64s 163ms/step - loss: 3.6595 - sparse_categorical_accuracy: 0.1564 - val_loss: 3.7932 - val_sparse_categorical_accuracy: 0.1544\n",
      "Epoch 142/200\n",
      "392/392 [==============================] - 66s 169ms/step - loss: 3.6625 - sparse_categorical_accuracy: 0.1579 - val_loss: 3.7691 - val_sparse_categorical_accuracy: 0.1584\n",
      "Epoch 143/200\n",
      "392/392 [==============================] - 67s 170ms/step - loss: 3.6584 - sparse_categorical_accuracy: 0.1519 - val_loss: 3.7793 - val_sparse_categorical_accuracy: 0.1547\n",
      "Epoch 144/200\n",
      "392/392 [==============================] - 64s 163ms/step - loss: 3.6450 - sparse_categorical_accuracy: 0.1604 - val_loss: 3.7106 - val_sparse_categorical_accuracy: 0.1700\n",
      "Epoch 145/200\n",
      "392/392 [==============================] - 63s 160ms/step - loss: 3.6435 - sparse_categorical_accuracy: 0.1595 - val_loss: 3.7578 - val_sparse_categorical_accuracy: 0.1603\n",
      "Epoch 146/200\n",
      "392/392 [==============================] - 61s 156ms/step - loss: 3.6422 - sparse_categorical_accuracy: 0.1600 - val_loss: 3.7894 - val_sparse_categorical_accuracy: 0.1516\n",
      "Epoch 147/200\n",
      "392/392 [==============================] - 62s 158ms/step - loss: 3.6484 - sparse_categorical_accuracy: 0.1600 - val_loss: 3.8426 - val_sparse_categorical_accuracy: 0.1526\n",
      "Epoch 148/200\n",
      "392/392 [==============================] - 63s 162ms/step - loss: 3.6312 - sparse_categorical_accuracy: 0.1593 - val_loss: 3.7429 - val_sparse_categorical_accuracy: 0.1564\n",
      "Epoch 149/200\n",
      "392/392 [==============================] - 64s 164ms/step - loss: 3.6377 - sparse_categorical_accuracy: 0.1614 - val_loss: 3.7215 - val_sparse_categorical_accuracy: 0.1622\n",
      "Epoch 150/200\n",
      "392/392 [==============================] - 64s 164ms/step - loss: 3.6332 - sparse_categorical_accuracy: 0.1628 - val_loss: 3.7797 - val_sparse_categorical_accuracy: 0.1600\n",
      "Epoch 151/200\n",
      "392/392 [==============================] - 63s 160ms/step - loss: 3.6297 - sparse_categorical_accuracy: 0.1581 - val_loss: 3.7635 - val_sparse_categorical_accuracy: 0.1512\n",
      "Epoch 152/200\n",
      "392/392 [==============================] - 63s 162ms/step - loss: 3.6389 - sparse_categorical_accuracy: 0.1613 - val_loss: 3.7519 - val_sparse_categorical_accuracy: 0.1572\n",
      "Epoch 153/200\n",
      "392/392 [==============================] - 62s 158ms/step - loss: 3.6233 - sparse_categorical_accuracy: 0.1654 - val_loss: 3.7376 - val_sparse_categorical_accuracy: 0.1603\n",
      "Epoch 154/200\n",
      "392/392 [==============================] - 61s 155ms/step - loss: 3.6168 - sparse_categorical_accuracy: 0.1619 - val_loss: 3.6992 - val_sparse_categorical_accuracy: 0.1655\n",
      "Epoch 155/200\n",
      "392/392 [==============================] - 63s 161ms/step - loss: 3.6131 - sparse_categorical_accuracy: 0.1665 - val_loss: 3.7037 - val_sparse_categorical_accuracy: 0.1686\n",
      "Epoch 156/200\n",
      "392/392 [==============================] - 63s 160ms/step - loss: 3.6093 - sparse_categorical_accuracy: 0.1644 - val_loss: 3.7145 - val_sparse_categorical_accuracy: 0.1641\n",
      "Epoch 157/200\n",
      "392/392 [==============================] - 61s 155ms/step - loss: 3.6054 - sparse_categorical_accuracy: 0.1676 - val_loss: 3.8307 - val_sparse_categorical_accuracy: 0.1518\n",
      "Epoch 158/200\n",
      "392/392 [==============================] - 62s 159ms/step - loss: 3.6031 - sparse_categorical_accuracy: 0.1646 - val_loss: 3.6771 - val_sparse_categorical_accuracy: 0.1697\n",
      "Epoch 159/200\n",
      "392/392 [==============================] - 64s 164ms/step - loss: 3.5948 - sparse_categorical_accuracy: 0.1681 - val_loss: 3.7035 - val_sparse_categorical_accuracy: 0.1681\n",
      "Epoch 160/200\n",
      "392/392 [==============================] - 69s 176ms/step - loss: 3.6110 - sparse_categorical_accuracy: 0.1627 - val_loss: 3.7092 - val_sparse_categorical_accuracy: 0.1609\n",
      "Epoch 161/200\n",
      "392/392 [==============================] - 67s 171ms/step - loss: 3.5999 - sparse_categorical_accuracy: 0.1689 - val_loss: 3.7011 - val_sparse_categorical_accuracy: 0.1691\n",
      "Epoch 162/200\n",
      "392/392 [==============================] - 64s 164ms/step - loss: 3.5932 - sparse_categorical_accuracy: 0.1662 - val_loss: 3.7100 - val_sparse_categorical_accuracy: 0.1644\n",
      "Epoch 163/200\n",
      "392/392 [==============================] - 66s 168ms/step - loss: 3.5983 - sparse_categorical_accuracy: 0.1680 - val_loss: 3.7435 - val_sparse_categorical_accuracy: 0.1660\n",
      "Epoch 164/200\n",
      "392/392 [==============================] - 67s 171ms/step - loss: 3.5894 - sparse_categorical_accuracy: 0.1679 - val_loss: 3.7398 - val_sparse_categorical_accuracy: 0.1584\n",
      "Epoch 165/200\n",
      "392/392 [==============================] - 66s 168ms/step - loss: 3.5901 - sparse_categorical_accuracy: 0.1688 - val_loss: 3.7050 - val_sparse_categorical_accuracy: 0.1689\n",
      "Epoch 166/200\n",
      "392/392 [==============================] - 65s 167ms/step - loss: 3.5850 - sparse_categorical_accuracy: 0.1687 - val_loss: 3.7277 - val_sparse_categorical_accuracy: 0.1662\n",
      "Epoch 167/200\n",
      "392/392 [==============================] - 71s 182ms/step - loss: 3.5832 - sparse_categorical_accuracy: 0.1670 - val_loss: 3.7248 - val_sparse_categorical_accuracy: 0.1652\n",
      "Epoch 168/200\n",
      "392/392 [==============================] - 80s 204ms/step - loss: 3.5767 - sparse_categorical_accuracy: 0.1699 - val_loss: 3.6898 - val_sparse_categorical_accuracy: 0.1673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1767a1130>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=N_EPOCHS,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# 2023-04-30: 170-180 min"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "978/978 [==============================] - 26s 26ms/step - loss: 3.4583 - sparse_categorical_accuracy: 0.2182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.4583210945129395, 0.21821261942386627]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/transformer_epochs_0-168/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/transformer_epochs_0-168/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(OUTDIR+'transformer_epochs_0-168')\n",
    "model.save(OUTDIR+'transformer_epochs_0-168.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
