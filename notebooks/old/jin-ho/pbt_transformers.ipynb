{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE04qAVhTZNE"
      },
      "source": [
        "# Tuning 🤗 Transformers with Population Based Training\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDe8FU6-swMI"
      },
      "source": [
        "In this notebook we show how to fine tune our Huggingface transformers using Population Based Training. The corresponding blog post is [here](https://medium.com/@amog_97444/c4e32c6c989b?source=friends_link&sk=92c2ed36420cd9e26281fd51da7c19b6).\n",
        "\n",
        "For our implementation of the fine tuning, we used [Ray Tune](https://https://docs.ray.io/en/master/tune/index.html), an open source library for scalable hyperparameter tuning. It is built on top of the [Ray](https://https://ray.io/) framework, which makes it perfect for parallel hyperparameter tuning on multiple GPUs. Make sure to set you runtime to use GPUs when going through this notebook. Since Colab provides us with limited memory and a single GPU, we use a much smaller transformer (tiny-distilroberta), run only 3 samples, and use a perturbation interval of 2 iterations in this notebook. The results in the blog post were obtained with a standard BERT model, 8 samples, perturbation after every iteration, and was run on a AWS p3.16xlarge instance. The exact code used for the blog post is [here](https://https://docs.ray.io/en/master/tune/examples/pbt_transformers.html)\n",
        "\n",
        "Let’s take a look at how we can implement parallel Population Based Training for our transformers using this library!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfFpbg76vVPy"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoS-HKJxtAZY"
      },
      "source": [
        "The first step is to import our main libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l45TvyLw5si5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.0.2\n",
            "  Using cached transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "Requirement already satisfied: numpy in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from transformers==3.0.2) (1.23.5)\n",
            "Collecting tokenizers==0.8.1.rc1 (from transformers==3.0.2)\n",
            "  Using cached tokenizers-0.8.1rc1.tar.gz (97 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: packaging in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from transformers==3.0.2) (23.1)\n",
            "Requirement already satisfied: filelock in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from transformers==3.0.2) (3.12.0)\n",
            "Requirement already satisfied: requests in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from transformers==3.0.2) (2.28.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from transformers==3.0.2) (4.65.0)\n",
            "Collecting regex!=2019.12.17 (from transformers==3.0.2)\n",
            "  Using cached regex-2023.3.23-cp39-cp39-macosx_11_0_arm64.whl (288 kB)\n",
            "Collecting sentencepiece!=0.1.92 (from transformers==3.0.2)\n",
            "  Using cached sentencepiece-0.1.98-cp39-cp39-macosx_11_0_arm64.whl (1.2 MB)\n",
            "Collecting sacremoses (from transformers==3.0.2)\n",
            "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests->transformers==3.0.2) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests->transformers==3.0.2) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests->transformers==3.0.2) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests->transformers==3.0.2) (2022.12.7)\n",
            "Requirement already satisfied: six in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from sacremoses->transformers==3.0.2) (1.16.0)\n",
            "Requirement already satisfied: click in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from sacremoses->transformers==3.0.2) (8.1.3)\n",
            "Requirement already satisfied: joblib in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from sacremoses->transformers==3.0.2) (1.2.0)\n",
            "Building wheels for collected packages: tokenizers\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[51 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/tg/gv0w34yj4575fd8wlc9nv1k40000gn/T/pip-build-env-vy625gbt/overlay/lib/python3.9/site-packages/setuptools/dist.py:519: InformationOnly: Normalizing '0.8.1.rc1' to '0.8.1rc1'\n",
            "  \u001b[31m   \u001b[0m   self.metadata.version = self._normalize_version(\n",
            "  \u001b[31m   \u001b[0m running bdist_wheel\n",
            "  \u001b[31m   \u001b[0m running build\n",
            "  \u001b[31m   \u001b[0m running build_py\n",
            "  \u001b[31m   \u001b[0m creating build\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-12.6-arm64-cpython-39\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-12.6-arm64-cpython-39/tokenizers\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/__init__.py -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-12.6-arm64-cpython-39/tokenizers/models\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/models/__init__.py -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers/models\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-12.6-arm64-cpython-39/tokenizers/decoders\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/decoders/__init__.py -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers/decoders\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-12.6-arm64-cpython-39/tokenizers/normalizers\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/normalizers/__init__.py -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers/normalizers\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-12.6-arm64-cpython-39/tokenizers/pre_tokenizers\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/pre_tokenizers/__init__.py -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers/pre_tokenizers\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-12.6-arm64-cpython-39/tokenizers/processors\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/processors/__init__.py -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers/processors\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-12.6-arm64-cpython-39/tokenizers/trainers\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/trainers/__init__.py -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers/trainers\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-12.6-arm64-cpython-39/tokenizers/implementations\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/implementations/byte_level_bpe.py -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers/implementations\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/implementations/sentencepiece_bpe.py -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers/implementations\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/implementations/base_tokenizer.py -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers/implementations\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/implementations/__init__.py -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers/implementations\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/implementations/char_level_bpe.py -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers/implementations\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/implementations/bert_wordpiece.py -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers/implementations\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/__init__.pyi -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/models/__init__.pyi -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers/models\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/decoders/__init__.pyi -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers/decoders\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/normalizers/__init__.pyi -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers/normalizers\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/pre_tokenizers/__init__.pyi -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers/pre_tokenizers\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/processors/__init__.pyi -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers/processors\n",
            "  \u001b[31m   \u001b[0m copying tokenizers/trainers/__init__.pyi -> build/lib.macosx-12.6-arm64-cpython-39/tokenizers/trainers\n",
            "  \u001b[31m   \u001b[0m running build_ext\n",
            "  \u001b[31m   \u001b[0m running build_rust\n",
            "  \u001b[31m   \u001b[0m info: syncing channel updates for 'nightly-2020-05-14-aarch64-apple-darwin'\n",
            "  \u001b[31m   \u001b[0m info: latest update on 2020-05-14, rust version 1.45.0-nightly (75e1463c5 2020-05-13)\n",
            "  \u001b[31m   \u001b[0m error: target 'aarch64-apple-darwin' not found in channel.  Perhaps check https://doc.rust-lang.org/nightly/rustc/platform-support.html for available targets\n",
            "  \u001b[31m   \u001b[0m error: can't find Rust compiler\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m To update pip, run:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m     pip install --upgrade pip\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m and then retry package installation.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build tokenizers\n",
            "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Collecting ray\n",
            "  Downloading ray-2.4.0-cp39-cp39-macosx_11_0_arm64.whl (57.6 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m497.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:04\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from ray) (23.1.0)\n",
            "Requirement already satisfied: click>=7.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from ray) (8.1.3)\n",
            "Requirement already satisfied: filelock in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from ray) (3.12.0)\n",
            "Requirement already satisfied: jsonschema in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from ray) (4.17.3)\n",
            "Collecting msgpack<2.0.0,>=1.0.0 (from ray)\n",
            "  Downloading msgpack-1.0.5-cp39-cp39-macosx_11_0_arm64.whl (70 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.5/70.5 kB\u001b[0m \u001b[31m554.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m469.4 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from ray) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from ray) (6.0)\n",
            "Requirement already satisfied: aiosignal in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from ray) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from ray) (1.3.3)\n",
            "Requirement already satisfied: requests in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from ray) (2.28.2)\n",
            "Collecting virtualenv<20.21.1,>=20.0.24 (from ray)\n",
            "  Downloading virtualenv-20.21.0-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m372.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting grpcio<=1.49.1,>=1.32.0 (from ray)\n",
            "  Downloading grpcio-1.49.1.tar.gz (22.1 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.1/22.1 MB\u001b[0m \u001b[31m288.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.3 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from ray) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5.2 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from grpcio<=1.49.1,>=1.32.0->ray) (1.16.0)\n",
            "Collecting distlib<1,>=0.3.6 (from virtualenv<20.21.1,>=20.0.24->ray)\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m751.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m730.8 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<4,>=2.4 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from virtualenv<20.21.1,>=20.0.24->ray) (3.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from jsonschema->ray) (0.19.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests->ray) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests->ray) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests->ray) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests->ray) (2022.12.7)\n",
            "Building wheels for collected packages: grpcio\n",
            "  Building wheel for grpcio (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for grpcio: filename=grpcio-1.49.1-cp39-cp39-macosx_12_0_arm64.whl size=3953496 sha256=cf3201569e9aa2987d06b9d56d650876868e8aa8199e2d758abc67aac1e74e1e\n",
            "  Stored in directory: /Users/jin-holee/Library/Caches/pip/wheels/35/a0/8c/de46f52c6cde99252a495c2f83232f7ce94f847c22eced1837\n",
            "Successfully built grpcio\n",
            "Installing collected packages: msgpack, distlib, virtualenv, grpcio, ray\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.54.0\n",
            "    Uninstalling grpcio-1.54.0:\n",
            "      Successfully uninstalled grpcio-1.54.0\n",
            "Successfully installed distlib-0.3.6 grpcio-1.49.1 msgpack-1.0.5 ray-2.4.0 virtualenv-20.21.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "zsh:1: no matches found: ray[tune]\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers\n",
        "%pip install ray\n",
        "%pip install ray[tune]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZA5KV9Ct1--"
      },
      "source": [
        "Depending on your current setup, there might be other libraries you have to install like torch. Also if you’re wondering how I made the beautiful plots in the blog post, it’s with a library called [Weights & Biases](https://https://www.wandb.com/). If you'd like, we’ll go through how we can easily integrate W&B with our code as well so you can visualize your training runs, though using W&B is optional. First, create an account with them, and then we can install it and login:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jqhhzz6YtRqD"
      },
      "outputs": [],
      "source": [
        "%pip install wandb\n",
        "import os\n",
        "os.environ[\"WANDB_API_KEY\"] = \"567cfcfcfb79b870512bc37972a2c7d1a3d158f8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT2cmzH5ueS8"
      },
      "source": [
        "Now we can get started with our code! The first step is to start up ray. If you’re running this on a cluster, make sure to specify an address to ray. For this notebook example, we don't have to worry about this. Also make sure to set log_to_driver to False, otherwise we get hit with a bunch of unnecessary tqdm training bars!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiAD-EtCvdtB"
      },
      "outputs": [],
      "source": [
        "import ray\n",
        "\n",
        "# If running on a cluster uncomment use the line below instead \n",
        "# ray.init(address=\"auto\", log_to_driver=False)\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init(log_to_driver=True, ignore_reinit_error=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT0mRl-dvsOA"
      },
      "source": [
        "Then, we can load and cache our transformer model, tokenizer, and the RTE dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcfbQ8FmuIM9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "# Util import\n",
        "from ray.tune.examples.pbt_transformers import utils\n",
        "\n",
        "\n",
        "\n",
        "# Set this to whatever you like\n",
        "data_dir_name = \"./data\"\n",
        "data_dir = os.path.abspath(os.path.join(os.getcwd(), data_dir_name))\n",
        "if not os.path.exists(data_dir):\n",
        "    os.mkdir(data_dir, 0o755)\n",
        "\n",
        "# Change these as needed.\n",
        "model_name = \"sshleifer/tiny-distilroberta-base\"\n",
        "task_name = \"rte\"\n",
        "\n",
        "task_data_dir = os.path.join(data_dir, task_name.upper())\n",
        "\n",
        "# Download and cache tokenizer, model, and features\n",
        "print(\"Downloading and caching Tokenizer\")\n",
        "\n",
        "# Triggers tokenizer download to cache\n",
        "AutoTokenizer.from_pretrained(model_name)\n",
        "print(\"Downloading and caching pre-trained model\")\n",
        "\n",
        "# Triggers model download to cache\n",
        "AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Download data.\n",
        "utils.download_data(task_name, data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB5BLmKev_6-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3ICOgXTwEIh"
      },
      "source": [
        "With everything now downloaded and cached, we can now set up our training function. Our training function defines the training execution for a single hyperparameter configuration. For now we pull these hyperparameters from a config argument, but we’ll see later how this is passed in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9GTvUzawE5b"
      },
      "source": [
        "First we get our datasets- we only use the first half of the dev dataset for validation, and leave the rest of testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uikm4YpavIOh"
      },
      "outputs": [],
      "source": [
        "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
        "from transformers import GlueDataset\n",
        "\n",
        "def get_datasets(config):\n",
        "  data_args = DataTrainingArguments(\n",
        "        task_name=config[\"task_name\"], data_dir=config[\"data_dir\"])\n",
        "  tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n",
        "  train_dataset = GlueDataset(\n",
        "      data_args,\n",
        "      tokenizer=tokenizer,\n",
        "      mode=\"train\",\n",
        "      cache_dir=config[\"data_dir\"])\n",
        "  eval_dataset = GlueDataset(\n",
        "      data_args,\n",
        "      tokenizer=tokenizer,\n",
        "      mode=\"dev\",\n",
        "      cache_dir=config[\"data_dir\"])\n",
        "  # Only use the first half for validation\n",
        "  eval_dataset = eval_dataset[:len(eval_dataset) // 2]\n",
        "  return train_dataset, eval_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd7nAyxZ16W_"
      },
      "source": [
        "### Checkpointing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw14Ot7n1HQ-"
      },
      "source": [
        "We also need to add extra functionality for *checkpointing*. After every epoch of training, we need to save our training state. This is crucial for Population Based Training since it allows us to continue training from where we left off even when hyperparameters are perturbed. The Huggingface Trainer provides functionality to save and load from a checkpoint, but we do have to make some modifications to integrate this with Ray Tune checkpointing and to checkpoint after every epoch. The first step is to subclass the Trainer from the transformers library. Ray Tune provides this [TuneTransformerTrainer](https://github.com/ray-project/ray/blob/master/python/ray/tune/examples/pbt_transformers/trainer.py) subclass which we utilize. Take a look at the class- we see that it handles reporting evaluation metrics to Tune, checkpointing everytime evaluate is called, and even a way to pass in custom W&B arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoXMZicduw3Q"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "from typing import Dict, Optional, Tuple\n",
        "\n",
        "from ray import tune\n",
        "\n",
        "import transformers\n",
        "from transformers.file_utils import is_torch_tpu_available\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR, is_wandb_available\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "if is_wandb_available():\n",
        "  import wandb\n",
        "\n",
        "class TuneTransformerTrainer(transformers.Trainer):\n",
        "    def get_optimizers(\n",
        "            self, num_training_steps\n",
        "    ):\n",
        "        self.current_optimizer, self.current_scheduler = super(\n",
        "        ).get_optimizers(num_training_steps)\n",
        "        return (self.current_optimizer, self.current_scheduler)\n",
        "\n",
        "    def evaluate(self,\n",
        "                 eval_dataset= None):\n",
        "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
        "        output = self._prediction_loop(\n",
        "            eval_dataloader, description=\"Evaluation\")\n",
        "        self._log(output.metrics)\n",
        "\n",
        "        self.save_state()\n",
        "\n",
        "        tune.report(**output.metrics)\n",
        "\n",
        "        return output.metrics\n",
        "\n",
        "    def save_state(self):\n",
        "        with tune.checkpoint_dir(step=self.global_step) as checkpoint_dir:\n",
        "            self.args.output_dir = checkpoint_dir\n",
        "            # This is the directory name that Huggingface requires.\n",
        "            output_dir = os.path.join(\n",
        "                self.args.output_dir,\n",
        "                f\"{PREFIX_CHECKPOINT_DIR}-{self.global_step}\")\n",
        "            self.save_model(output_dir)\n",
        "            if self.is_world_master():\n",
        "                torch.save(self.current_optimizer.state_dict(),\n",
        "                           os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                torch.save(self.current_scheduler.state_dict(),\n",
        "                           os.path.join(output_dir, \"scheduler.pt\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHG0h9qUFOv3"
      },
      "source": [
        "The only addition we have to make is to add a function to recover the checkpoint file from Tune's checkpoint directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wekJMP34FUK1"
      },
      "outputs": [],
      "source": [
        "def recover_checkpoint(tune_checkpoint_dir, model_name=None):\n",
        "    if tune_checkpoint_dir is None or len(tune_checkpoint_dir) == 0:\n",
        "        return model_name\n",
        "    # Get subdirectory used for Huggingface.\n",
        "    subdirs = [\n",
        "        os.path.join(tune_checkpoint_dir, name)\n",
        "        for name in os.listdir(tune_checkpoint_dir)\n",
        "        if os.path.isdir(os.path.join(tune_checkpoint_dir, name))\n",
        "    ]\n",
        "    # There should only be 1 subdir.\n",
        "    assert len(subdirs) == 1, subdirs\n",
        "    return subdirs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuILNQN6xnxw"
      },
      "source": [
        "Finally, we put all of these together as well as create our training arguments, model, and Huggingface Trainer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOdga53vxVtx"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, TrainingArguments, glue_tasks_num_labels\n",
        "from ray.tune.integration.wandb import wandb_mixin\n",
        "\n",
        "@wandb_mixin\n",
        "def train_transformer(config, checkpoint_dir=None):\n",
        "  train_dataset, eval_dataset = get_datasets(config)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "        output_dir=tune.get_trial_dir(),\n",
        "        learning_rate=config[\"learning_rate\"],\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        evaluate_during_training=True,\n",
        "        # Run eval after every epoch.\n",
        "        eval_steps=(len(train_dataset) // config[\"per_gpu_train_batch_size\"]) +\n",
        "        1,\n",
        "        # We explicitly set save to 0, and do checkpointing in evaluate instead\n",
        "        save_steps=0,\n",
        "        num_train_epochs=config[\"num_epochs\"],\n",
        "        max_steps=config[\"max_steps\"],\n",
        "        per_device_train_batch_size=config[\"per_gpu_train_batch_size\"],\n",
        "        per_device_eval_batch_size=config[\"per_gpu_val_batch_size\"],\n",
        "        warmup_steps=0,\n",
        "        weight_decay=config[\"weight_decay\"],\n",
        "        logging_dir=\"./logs\",\n",
        "    )\n",
        "\n",
        "  model_name_or_path = recover_checkpoint(checkpoint_dir, config[\"model_name\"])\n",
        "  num_labels = glue_tasks_num_labels[config[\"task_name\"]]\n",
        "\n",
        "  config = AutoConfig.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        num_labels=num_labels,\n",
        "        finetuning_task=task_name,\n",
        "    )\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        config=config,\n",
        "    )\n",
        "   \n",
        "  # Use our modified TuneTransformerTrainer\n",
        "  tune_trainer = TuneTransformerTrainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=eval_dataset,\n",
        "      compute_metrics=utils.build_compute_metrics_fn(task_name),\n",
        "  )\n",
        "  tune_trainer.train(model_name_or_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EhcYLiZHLzB"
      },
      "source": [
        "Our training function takes in 2 parameters: config which contains all of our hyperparameters, and checkpoint_dir which is a directory containing the previous state of our trial. As we'll see below, these 2 arguments are passed in to our training function by Tune\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11xEMV8mHiJ4"
      },
      "source": [
        "## Hyperparameter Tuning with Ray Tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYxmkIpuHlSx"
      },
      "source": [
        "Now that we have our training function setup, we run our hyperparameter search with Ray Tune. We first create an initial hyperparameter configuration which specifies the hyperparameters each trial will use initially. For some of our hyperparameters, we want to try different configurations, so we sample those from a distribution.\n",
        "\n",
        "We also pass in our W&B arguments here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqNbMF1CHf3H"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "        # These 3 configs below were defined earlier\n",
        "        \"model_name\": model_name,\n",
        "        \"task_name\": task_name,\n",
        "        \"data_dir\": task_data_dir,\n",
        "        \"per_gpu_val_batch_size\": 32,\n",
        "        \"per_gpu_train_batch_size\": tune.choice([16, 32, 64]),\n",
        "        \"learning_rate\": tune.uniform(1e-5, 5e-5),\n",
        "        \"weight_decay\": tune.uniform(0.0, 0.3),\n",
        "        \"num_epochs\": tune.choice([2, 3, 4, 5]),\n",
        "        \"max_steps\": -1,  # We use num_epochs instead.\n",
        "        \"wandb\": {\n",
        "            \"project\": \"pbt_transformers\",\n",
        "            \"reinit\": True,\n",
        "            \"allow_val_change\": True\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGIax71xIDqy"
      },
      "source": [
        "Now we can set up our Population Based Training scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip6shHkNICTs"
      },
      "outputs": [],
      "source": [
        "from ray.tune.schedulers import PopulationBasedTraining\n",
        "\n",
        "scheduler = PopulationBasedTraining(\n",
        "        time_attr=\"training_iteration\",\n",
        "        metric=\"eval_acc\",\n",
        "        mode=\"max\",\n",
        "        perturbation_interval=2,\n",
        "        hyperparam_mutations={\n",
        "            \"weight_decay\": lambda: tune.uniform(0.0, 0.3).func(None),\n",
        "            \"learning_rate\": lambda: tune.uniform(1e-5, 5e-5).func(None),\n",
        "            \"per_gpu_train_batch_size\": [16, 32, 64],\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvmerUBvIMei"
      },
      "source": [
        "We also create a CLI reporter to view our results from the command line. We specify the hyperparameters we want to see from the command line, as well as what metrics we want to see. The metrics are the inputs to the tune.report we call we make in TuneTransformerTrainer.evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMi3NOkPIJS4"
      },
      "outputs": [],
      "source": [
        "from ray.tune import CLIReporter\n",
        "\n",
        "reporter = CLIReporter(\n",
        "        parameter_columns={\n",
        "            \"weight_decay\": \"w_decay\",\n",
        "            \"learning_rate\": \"lr\",\n",
        "            \"per_gpu_train_batch_size\": \"train_bs/gpu\",\n",
        "            \"num_epochs\": \"num_epochs\"\n",
        "        },\n",
        "        metric_columns=[\n",
        "            \"eval_acc\", \"eval_loss\", \"epoch\", \"training_iteration\"\n",
        "        ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnWPeLyWIb76"
      },
      "source": [
        "Finally, we pass in our training function, config, PBT scheduler, and reporter to tune:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cy5X41IzIVd3"
      },
      "outputs": [],
      "source": [
        "analysis = tune.run(\n",
        "        train_transformer,\n",
        "        resources_per_trial={\n",
        "            \"cpu\": 1,\n",
        "            \"gpu\": 1\n",
        "        },\n",
        "        config=config,\n",
        "        num_samples=3,\n",
        "        scheduler=scheduler,\n",
        "        keep_checkpoints_num=3,\n",
        "        checkpoint_score_attr=\"training_iteration\",\n",
        "        progress_reporter=reporter,\n",
        "        local_dir=\"./ray_results/\",\n",
        "        name=\"tune_transformer_pbt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW2qMopVnMQY"
      },
      "source": [
        "Let’s dive deeper into what’s going on here. Initially, tune creates 3 (from num_samples) trials, or instantiations of our training function. Each trial has a hyperparameter configuration provided by config. So we have 3 different executions of transformer fine-tuning, each with different hyperparameters, all running in parallel. However, we also pass in a PBT scheduler, with time_attr set to training_iteration and perturbation_interval set to 2. So, after 2 training iterations, we see PBT come into effect. The bottom 25% of trials according to eval_acc exploit from the top 25% of trials by copying over their model weights and hyperparameters. Then after copying over, we do exploration on these trials, by mutating certain hyperparameters specified by hyperparam_mutations. This is where checkpointing becomes crucial- this process results in a creation of a new trial, so we need checkpointing to continue training where we left off, except with the new hyperparameters. This process continues after each training iteration, and instead of randomly searching across our entire hyperparameter space, we can focus on the best performing trials and do a more fine-grained search in that smaller area."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSLqqV_CTzBj"
      },
      "source": [
        "## Testing the Best Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9yIl8LxT8a_"
      },
      "source": [
        "Once our hyperparameter tuning experiment is complete, we can get the best performin model and try it out on our test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxKyvQ6WNlvG"
      },
      "outputs": [],
      "source": [
        "data_args = DataTrainingArguments(task_name=config[\"task_name\"], data_dir=config[\"data_dir\"])\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n",
        "\n",
        "best_config = analysis.get_best_config(metric=\"eval_acc\", mode=\"max\")\n",
        "print(best_config)\n",
        "best_checkpoint = recover_checkpoint(\n",
        "    analysis.get_best_trial(metric=\"eval_acc\",\n",
        "                            mode=\"max\").checkpoint.value)\n",
        "print(best_checkpoint)\n",
        "best_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    best_checkpoint).to(\"cuda\")\n",
        "\n",
        "test_args = TrainingArguments(output_dir=\"./best_model_results\", )\n",
        "test_dataset = GlueDataset(\n",
        "    data_args, tokenizer=tokenizer, mode=\"dev\", cache_dir=data_dir)\n",
        "test_dataset = test_dataset[len(test_dataset) // 2:]\n",
        "\n",
        "test_trainer = transformers.Trainer(\n",
        "    best_model,\n",
        "    test_args,\n",
        "    compute_metrics=utils.build_compute_metrics_fn(task_name))\n",
        "\n",
        "metrics = test_trainer.evaluate(test_dataset)\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-v_hzwTiVVt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
