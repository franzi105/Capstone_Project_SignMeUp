{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook is adapted and modified from the following Youtube tutorial: https://www.youtube.com/watch?v=doDUihpj6ro "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this notebook, real-time predictions can be made on signs you perform in front of your running webcam. \n",
    "\n",
    "### <span style=\"color:green\">Quick User Guide: \n",
    "\n",
    "<span style=\"color:green\">1. Load your trained TF Model.</span>\n",
    "\n",
    "<span style=\"color:green\">2. Copy + paste used Configurations from Franziska's pre-processing notebook</span>\n",
    "\n",
    "<span style=\"color:green\">Optional: 3. Copy + paste used Pre-processing Layer from Franziska's pre-processing notebook</span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import Dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-macos in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (2.12.0)\n",
      "Requirement already satisfied: opencv-python in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (4.7.0.72)\n",
      "Requirement already satisfied: mediapipe-silicon in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (0.9.1)\n",
      "Requirement already satisfied: sklearn in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (0.0.post1)\n",
      "Requirement already satisfied: matplotlib in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (3.4.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (23.3.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (3.8.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (0.4.8)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (16.0.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (1.23.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (58.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (1.53.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (2.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorflow-macos) (2.12.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from mediapipe-silicon) (22.2.0)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from mediapipe-silicon) (4.7.0.72)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow-macos) (0.40.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from jax>=0.3.15->tensorflow-macos) (0.0.4)\n",
      "Requirement already satisfied: scipy>=1.7 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from jax>=0.3.15->tensorflow-macos) (1.10.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (2.17.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (0.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-macos) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-macos) (6.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-macos) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-macos) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-macos) (3.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow-macos opencv-python mediapipe-silicon sklearn matplotlib\n",
    "#!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib # original code line from tutorial (he had a windows system)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os # easier file path handling\n",
    "\n",
    "# for camera feed\n",
    "import cv2 # opencv\n",
    "from matplotlib import pyplot as plt # imshow for easy visualization\n",
    "import time # to insert \"sleep\" in between frames\n",
    "import mediapipe as mp # for accessing and reading from webcam\n",
    "\n",
    "# for model re-building\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# for loading .json dictionary\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Load Files</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Load Saved Model</span>\n",
    "\n",
    "#### <span style=\"color:green\">Here, you can load your trained TensorFlow model, e.g. LSTM.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = tf.keras.models.load_model('../models/LSTM_model_20signsz_8.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sign_to_prediction_index_map.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label map dictionary \n",
    "#LABEL_MAP = json.load(open(\"../data/asl-signs/sign_to_prediction_index_map.json\", \"r\")) \n",
    "LABEL_MAP = {'brown': 0,  'callonphone': 1,  'cow': 2,  'cry': 3,  'dad': 4,  'fireman': 5,  'frog': 6,  'gum': 7,  'icecream': 8,  'minemy': 9,  'nose': 10,  'owl': 11,  'please': 12,  'radio': 13,  'shhh': 14,  'shirt': 15,  'tomorrow': 16,  'uncle': 17,  'water': 18,  'who': 19}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Mechanics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTDOWN = 0\n",
    "#SELECTED_SIGNS = ['hello', 'mom', 'dad', 'hungry', 'thirsty']\n",
    "#SELECTED_SIGNS = ['apple', 'bath', 'owl', 'clown', 'better']\n",
    "SELECTED_SIGNS = list(LABEL_MAP.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Franziskas's Configuration :) </span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">For now, just copy + paste the \"Configuration\" cell from Franziska's notebook \"TF_Load-PreprocessData.ipynb\" into the following cell :D </span>\n",
    "\n",
    "#### <span style=\"color:green\">These configuration must be the same ones, which you used to preprocess the data to train the TensorFlow model, which you loaded above! </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of used landmarks: 43\n",
      "(22, 129)\n"
     ]
    }
   ],
   "source": [
    "#limit dataset for quick test\n",
    "QUICK_TEST = True\n",
    "QUICK_LIMIT = 500\n",
    "\n",
    "#Define length of sequences for padding or cutting; 22 is the median length of all sequences\n",
    "LENGTH = 22\n",
    "\n",
    "#define min or max length of sequences; sequences too long/too short will be dropped\n",
    "#max value of 92 was defined by calculating the interquartile range\n",
    "MIN_LENGTH = 10\n",
    "MAX_LENGTH = 92\n",
    "\n",
    "#final data will be flattened, if false data will be 3 dimensional\n",
    "FLATTEN = False\n",
    "\n",
    "#define initialization of numpy array \n",
    "ARRAY = False #(True=Zeros, False=empty values)\n",
    "\n",
    "#Define padding mode \n",
    "#1 = padding at start&end; 2 = padding at end; 3 = no padding, 4 = copy first/lastframe, 5 = copy last frame)\n",
    "#Note: Mode 3 will give you an error due to different lengths, working on that\n",
    "PADDING = 2\n",
    "CONSTANT_VALUE = 0 #only required for mode 1 and 2; enter tf.constant(float('nan')) for NaN\n",
    "\n",
    "#define if z coordinate will be dropped\n",
    "DROP_Z = False\n",
    "\n",
    "#mirror, flips x coordinate for data augmentation\n",
    "MIRROR = True\n",
    "\n",
    "#define if csv file should be filtered\n",
    "CSV_FILTER  = False\n",
    "#define how many participants for test set\n",
    "TEST_COUNT = 5 #5 participants account for ca 23% of dataset\n",
    "#generate test or train dataset (True = Train dataset; False = Test dataset)\n",
    "#TRAIN = True #only works if CSV_FILTER is activated\n",
    "TRAIN = True\n",
    "\n",
    "#filter for specific signs\n",
    "SIGN_FILTER = True\n",
    "sign_list = [0,1,5,8]\n",
    "\n",
    "#define filenames for x and y:\n",
    "feature_data = 'X' #x data\n",
    "feature_labels = 'y' #y data\n",
    "\n",
    "#use for test dataset\n",
    "#feature_data = 'X_test_h6' #x data\n",
    "#feature_labels = 'y_test_h6' #y data\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "#Defining Landmarks\n",
    "#index ranges for each landmark type\n",
    "#dont change these landmarks\n",
    "FACE = list(range(0, 468))\n",
    "LEFT_HAND = list(range(468, 489))\n",
    "POSE = list(range(489, 522))\n",
    "POSE_UPPER = list(range(489, 510))\n",
    "RIGHT_HAND = list(range(522, 543))\n",
    "LIPS = [61, 185, 40, 39, 37,  0, 267, 269, 270, 409,\n",
    "                 291,146, 91,181, 84, 17, 314, 405, 321, 375, \n",
    "                 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, \n",
    "                 95, 88, 178, 87, 14,317, 402, 318, 324, 308]\n",
    "lipsUpperOuter= [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291]\n",
    "lipsLowerOuter= [146, 91, 181, 84, 17, 314, 405, 321, 375, 291]\n",
    "lipsUpperInner= [78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308]\n",
    "lipsLowerInner= [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308]\n",
    "#defining landmarks that will be merged\n",
    "averaging_sets = []\n",
    "\n",
    "#generating list with all landmarks selected for preprocessing\n",
    "#change landmarks you want to use here:\n",
    "point_landmarks_right = RIGHT_HAND + lipsUpperInner + lipsLowerInner\n",
    "point_landmarks_left = LEFT_HAND + lipsUpperInner + lipsLowerInner\n",
    "\n",
    "#calculating sum of total landmarks used\n",
    "LANDMARKS = len(point_landmarks_right) + len(averaging_sets)\n",
    "print(f'Total count of used landmarks: {LANDMARKS}')\n",
    "\n",
    "#defining input shape for model\n",
    "if DROP_Z:\n",
    "    INPUT_SHAPE = (LENGTH,LANDMARKS*2)\n",
    "else:\n",
    "    INPUT_SHAPE = (LENGTH,LANDMARKS*3)\n",
    "print(INPUT_SHAPE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSITION_FRAMES = 2\n",
    "SEQ_LENGTH = 30"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objects and Functions for MP Holistic Keypoints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize MP Holistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing utilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Functions (later they all go into a python module for multiple use)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mediapipe_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to detect MP Holistic landmarks from an image, e.g. frames of your camera feed\n",
    "def mediapipe_detection(image, model): \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # color conversion BGR to RGB\n",
    "    image.flags.writeable = False                   # image no longer writeable\n",
    "    results = model.process(image)                  # make prediction\n",
    "    image.flags.writeable = True                    # image is writeable again\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # color conversion back to original\n",
    "    return image, results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### draw_styled_landmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to draw landmarks points and connecting lines on top of an image, e.g. on top of your camera feed\n",
    "def draw_styled_landmarks(image, results): \n",
    "    # draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                              mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                              mp_drawing.DrawingSpec(color=(224,208,64), thickness=1, circle_radius=1))\n",
    "    # draw pose connections\n",
    "    #mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "    #                          mp_drawing.DrawingSpec(color=(224,208,64), thickness=2, circle_radius=2)) \n",
    "    # draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                              mp_drawing.DrawingSpec(color=(224,208,64), thickness=2, circle_radius=4), \n",
    "                              mp_drawing.DrawingSpec(color=(235,206,135), thickness=2, circle_radius=2)) \n",
    "    # draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                              mp_drawing.DrawingSpec(color=(224,208,64), thickness=2, circle_radius=4), \n",
    "                              mp_drawing.DrawingSpec(color=(128,128,240), thickness=2, circle_radius=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### extract_keypoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract coordinates (+visibility) of all landmarks --> keypoints\n",
    "# and concatenates everything into a flattened list \n",
    "def extract_keypoints(results): \n",
    "    face = np.array([[r.x, r.y, r.z] for r in results.face_landmarks.landmark]) if results.face_landmarks else np.zeros([468, 3])\n",
    "    left_hand = np.array([[r.x, r.y, r.z] for r in results.left_hand_landmarks.landmark]) if results.left_hand_landmarks else np.zeros([21, 3])\n",
    "    pose = np.array([[r.x, r.y, r.z] for r in results.pose_landmarks.landmark]) if results.pose_landmarks else np.zeros([33, 3]) # x, y, z and extra value visibility\n",
    "    right_hand = np.array([[r.x, r.y, r.z] for r in results.right_hand_landmarks.landmark]) if results.right_hand_landmarks else np.zeros([21, 3])\n",
    "    return np.concatenate([face, left_hand, pose, right_hand]) # original code\n",
    "    # a flattened list with list of all pose, face, left_hand, right_hand landmark x, y, z, (+visibility) coordinates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### prob_viz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to visualize predicted word probabilities with a dynamic real-time bar chart\n",
    "def prob_viz(pred, image, SELECTED_SIGNS): \n",
    "    output_frame = image.copy() \n",
    "    bar_zero = 15\n",
    "    \n",
    "    for num, prob in enumerate(pred): \n",
    "        cv2.rectangle(output_frame, \n",
    "                      pt1=(bar_zero, 75+num*30), \n",
    "                      pt2=(bar_zero+int(prob*100), 95+num*30), \n",
    "                      color=(200, 200, 200), thickness=-1)\n",
    "        # cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "        cv2.putText(img=output_frame, \n",
    "                    text=SELECTED_SIGNS[num], \n",
    "                    org=(bar_zero, 95+num*30), \n",
    "                    fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.75, \n",
    "                    color=(50, 50, 50), \n",
    "                    thickness=1, lineType=cv2.LINE_AA)\n",
    "        # cv2.putText(image, 'OpenCV', org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "    return output_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Pre-processing Layer</span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_nan_mean(x, axis=0):\n",
    "    #calculates the mean of a TensorFlow tensor x along a specified axis while ignoring any NaN values in the tensor.\n",
    "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_hand_percentage(x):\n",
    "    #calculates percentage of right hand usage\n",
    "    right = tf.gather(x, RIGHT_HAND, axis=1)\n",
    "    left = tf.gather(x, LEFT_HAND, axis=1)\n",
    "    right_count = tf.reduce_sum(tf.where(tf.math.is_nan(right), tf.zeros_like(right), tf.ones_like(right)))\n",
    "    left_count = tf.reduce_sum(tf.where(tf.math.is_nan(left), tf.zeros_like(left), tf.ones_like(left)))\n",
    "    return right_count / (left_count+right_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">In the following cell copy + paste the pre-processing layer from Franziska's notebook (,if you added / changed something)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating preprocessing layer that will be added to final model\n",
    "class FeatureGen(tf.keras.layers.Layer):\n",
    "    #defines custom tensorflow layer \n",
    "    def __init__(self):\n",
    "        #initializes layer\n",
    "        super(FeatureGen, self).__init__()\n",
    "    \n",
    "    def call(self, x_in, MIRROR=False):\n",
    "        #drop z coordinates if required\n",
    "        if DROP_Z:\n",
    "            x_in = x_in[:, :, 0:2]\n",
    "        if MIRROR:\n",
    "            #flipping x coordinates\n",
    "            x_in = np.array(x_in)\n",
    "            x_in[:, :, 0] = (x_in[:, :, 0]-1)*(-1)\n",
    "            x_in = tf.convert_to_tensor(x_in)\n",
    "\n",
    "        #generates list with mean values for landmarks that will be merged\n",
    "        x_list = [tf.expand_dims(tf_nan_mean(x_in[:, av_set[0]:av_set[0]+av_set[1], :], axis=1), axis=1) for av_set in averaging_sets]\n",
    "        \n",
    "        #extracts specific columns from input x_in defined by landmarks\n",
    "        handedness = right_hand_percentage(x_in)\n",
    "        if handedness > 0.5:\n",
    "            x_list.append(tf.gather(x_in, point_landmarks_right, axis=1))\n",
    "        else: \n",
    "            x_list.append(tf.gather(x_in, point_landmarks_left, axis=1))\n",
    "\n",
    "        #concatenates the two tensors from above along axis 1/columns\n",
    "        x = tf.concat(x_list, 1)\n",
    "\n",
    "        #padding to desired length of sequence (defined by LENGTH)\n",
    "        #get current number of rows\n",
    "        x_padded = x\n",
    "        current_rows = tf.shape(x_padded)[0]\n",
    "        #if current number of rows is greater than desired number of rows, truncate excess rows\n",
    "        if current_rows > LENGTH:\n",
    "            x_padded = x_padded[-LENGTH:, :, :]\n",
    "        #if current number of rows is less than desired number of rows, add padding\n",
    "        elif current_rows < LENGTH:\n",
    "            #calculate amount of padding needed\n",
    "            pad_rows = LENGTH - current_rows\n",
    "\n",
    "            if PADDING ==4: #copy first/last frame\n",
    "                if pad_rows %2 == 0: #if pad_rows is even\n",
    "                    padding_front = tf.repeat(x_padded[0:1, :], pad_rows//2, axis=0)\n",
    "                    padding_back = tf.repeat(x_padded[-1:, :], pad_rows//2, axis=0)\n",
    "                else: #if pad_rows is odd\n",
    "                    padding_front = tf.repeat(x_padded[0:1, :], (pad_rows//2)+1, axis=0)\n",
    "                    padding_back = tf.repeat(x_padded[-1:, :], pad_rows//2, axis=0)\n",
    "                x_padded = tf.concat([padding_front, x_padded, padding_back], axis=0)\n",
    "            elif PADDING == 5: #copy last frame\n",
    "                padding_back = tf.repeat(x_padded[-1:, :], pad_rows, axis=0)\n",
    "                x_padded = tf.concat([x_padded, padding_back], axis=0)\n",
    "            else:\n",
    "                if PADDING ==1: #padding at start and end\n",
    "                    if pad_rows %2 == 0: #if pad_rows is even\n",
    "                        paddings = [[pad_rows//2, pad_rows//2], [0, 0], [0, 0]]\n",
    "                    else: #if pad_rows is odd\n",
    "                        paddings = [[pad_rows//2+1, pad_rows//2], [0, 0], [0, 0]]\n",
    "                elif PADDING ==2: #padding only at the end of sequence\n",
    "                    paddings = [[0, pad_rows], [0, 0], [0, 0]]\n",
    "                elif PADDING ==3: #no padding\n",
    "                    paddings = [[0, 0], [0, 0], [0, 0]]\n",
    "                x_padded = tf.pad(x_padded, paddings, mode='CONSTANT', constant_values=CONSTANT_VALUE)\n",
    "\n",
    "        x = x_padded\n",
    "        current_rows = tf.shape(x)[0]\n",
    "\n",
    "        #interpolate single missing values\n",
    "        x = pd.DataFrame(np.array(x).flatten()).interpolate(method='linear', limit=2, limit_direction='both')\n",
    "        #fill missing values with zeros\n",
    "        x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
    "        \n",
    "        #reshape data to 2D or 3D array\n",
    "        if FLATTEN:\n",
    "            x = tf.reshape(x, (1, current_rows*INPUT_SHAPE[1]))\n",
    "        else:\n",
    "            x = tf.reshape(x, (1, current_rows, INPUT_SHAPE[1]))\n",
    "\n",
    "        return x\n",
    "\n",
    "#define converter using generated layer\n",
    "feature_converter = FeatureGen()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Time Prediction / Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Press \"Q\" to interrupt the camera feed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. New detection variables \n",
    "sequence = [] # to collect all 22 frames for prediction\n",
    "sentence = [] # history of all predictions (predicted words)\n",
    "predictions = []\n",
    "threshold = 0.5 # confidence metrics (only render prediction results, if confidence is above threshold)\n",
    "\n",
    "cap = cv2.VideoCapture(0) # grabbing webcam\n",
    "MODE = 0 #changing modes to show all selected signs\n",
    "# set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic: \n",
    "    while cap.isOpened(): # loop through all frames \n",
    "        key = cv2.waitKey(10) & 0xFF\n",
    "        # read feed\n",
    "        ret, frame = cap.read()\n",
    "        # make detections \n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        #draw_landmarks(image, results)\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results) # extract keypoints x, y, z for face, left_hand, pose, right_hand from mediapipe holistic predictions, keypoints.shape e.g. (543, 3)\n",
    "\n",
    "        if MODE == 0 or MODE == 1:\n",
    "            if results.left_hand_landmarks or results.right_hand_landmarks: #check if hand landmarks are present: \n",
    "                sequence.append(keypoints) # keep appending keypoints (frames) to a sequence, np.array(sequence).shape e.g. (22, 543, 3)\n",
    "                # pre-processing\n",
    "                model_input = feature_converter(np.array(sequence))\n",
    "\n",
    "                # prediction\n",
    "                pred = model.predict(model_input)[0] # model.fit() expects something in shape (num_sequences, 30, 1662), e.g. (1, 30, 1662) for a single sequence\n",
    "                selected_labels = [LABEL_MAP[x] for x in SELECTED_SIGNS]\n",
    "                #pred_all = pred \n",
    "                pred = pred[selected_labels]\n",
    "                if MODE == 1:\n",
    "                    image = prob_viz(pred, image, SELECTED_SIGNS)\n",
    "                sentence = ['...']\n",
    "            else: #no hands visible\n",
    "                if len(sequence) > 2 : #run predictions if sequence was created\n",
    "                    # pre-processing\n",
    "                    model_input = feature_converter(np.array(sequence))\n",
    "\n",
    "                    # prediction\n",
    "                    pred = model.predict(model_input)[0] # model.fit() expects something in shape (num_sequences, 30, 1662), e.g. (1, 30, 1662) for a single sequence\n",
    "                    selected_labels = [LABEL_MAP[x] for x in SELECTED_SIGNS]\n",
    "                    #pred_all = pred \n",
    "                    pred = pred[selected_labels]\n",
    "                    #predictions.append(np.argmax(pred))\n",
    "\n",
    "                    # 3. Visualization logic\n",
    "                    # makes sure the last x frames had the same prediction (more stable transition from one sign to another) \n",
    "                    #if np.unique(predictions[-TRANSITION_FRAMES:])[0]==np.argmax(pred): \n",
    "                        # if the confidence of the most confident prediction is above threshold\n",
    "                    if pred[np.argmax(pred)] > threshold: \n",
    "                        sentence = [SELECTED_SIGNS[np.argmax(pred)]]\n",
    "                    #if threshold is not reached\n",
    "                    else:\n",
    "                        sentence = ['...']\n",
    "                    # limit the history to the last 5 predictions\n",
    "                    #if len(sentence) > 5: \n",
    "                    #    sentence = sentence[-5:]\n",
    "                    #reseting sequence for next iteration\n",
    "                    sequence = []\n",
    "                else:\n",
    "                    pred = 20 *[0]\n",
    "                    #sentence = ['...']\n",
    "            \n",
    "            #viz probabilities\n",
    "            if MODE == 1:\n",
    "                image = prob_viz(pred, image, SELECTED_SIGNS)\n",
    "            # some rendering\n",
    "            cv2.rectangle(image, (0, 0), (1280, 60), (200, 200, 200), -1)\n",
    "            cv2.putText(image, ' '.join(sentence), (600, 45), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (52, 75, 102), 1, cv2.LINE_AA)\n",
    "            # cv2.putText(image, 'OpenCV', org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "        # show to screen\n",
    "        cv2.imshow(\"OpenCV Feed\", image)\n",
    "\n",
    "        #change modes\n",
    "        if key == ord('1'): \n",
    "            MODE = 1\n",
    "        elif key == ord('0'): \n",
    "            MODE = 0\n",
    "        elif key == ord('g'): \n",
    "            MODE = 2\n",
    "        elif key == ord('q'): \n",
    "            break\n",
    "        \n",
    "# release camera and close feed window \n",
    "cap.release()\n",
    "cv2.destroyAllWindows() \n",
    "cv2.waitKey(1) # some workaround to fix the bug, that window doesn't close\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
