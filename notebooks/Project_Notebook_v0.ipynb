{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Setup__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SERVER = 'kaggle'\n",
    "SERVER = \"local\"\n",
    "\n",
    "MODE = \"interactive\"  # uses only 0.01 % of the whole dataset\n",
    "# MODE = \"full\"  # uses the whole dataset\n",
    "\n",
    "LOAD_PARQUET = 1  # 0: disables, 1: enables loading of parquet files\n",
    "\n",
    "# random seed for the notebook\n",
    "RSEED = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __0. Kaggle Competition__: Google - Isolated Sign Language Recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/competitions/asl-signs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Overview__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Description__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Goal of the Competition__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this competition is to classify isolated American Sign Language (ASL) signs. You will create a [TensorFlow Lite](https://www.tensorflow.org/lite) model trained on labeled landmark data extracted using the [MediaPipe Holistic Solution](https://google.github.io/mediapipe/solutions/holistic.html).\n",
    "\n",
    "Your work may improve the ability of PopSign* to help relatives of deaf children learn basic signs and communicate better with their loved ones.^"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/jcyWo_1Q_jY/0.jpg)](https://www.youtube.com/watch?v=jcyWo_1Q_jY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Context__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Every day, 33 babies are born with permanent hearing loss in the U.S.__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 90% of which are born to hearing parents many of which may not know American Sign Language. (kdhe.ks.gov, deafchildren.org) Without sign language, deaf babies are at risk of Language Deprivation Syndrome. This syndrome is characterized by a lack of access to naturally occurring language acquisition during their critical language-learning years. It can cause serious impacts on different aspects of their lives, such as relationships, education, and employment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Learning sign language is challenging.__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning American Sign Language is as difficult for English speakers as learning Japanese. (jstor.org) It takes time and resources, which many parents don't have. They want to learn sign language, but it's hard when they are working long hours just to make ends meet. And even if they find the time and money for classes, the classes are often far away."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Games can help.__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PopSign is a smartphone game app that makes learning American Sign Language fun, interactive, and accessible. Players match videos of ASL signs with bubbles containing written English words to pop them.\n",
    "PopSign is designed to help parents with deaf children learn ASL, but it's open to anyone who wants to learn sign language vocabulary. By adding a sign language recognizer from this competition, PopSign players will be able to sign the type of bubble they want to shoot, providing the player with the opportunity to practice the sign themselves instead of just watching videos of other people signing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __You can help connect deaf children and their parents.__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By training a sign language recognizer for PopSign, you can help make the game more interactive and improve the learning and confidence of players who want to learn sign language to communicate with their loved ones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Why TensorFlow Lite__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow the ML model to run on device in an attempt to limit latency inside the game, PopSign doesn’t send user videos to the cloud. Therefore, all inference must be done on the phone itself. PopSign is building its recognition pipeline on top of TensorFlow Lite, which runs on both Android and iOS. In order for the competition models to integrate seamlessly with PopSign, we are asking our competitors to submit their entries in the form of TensorFlow Lite models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Special thanks to our partners__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’d like to thank the Georgia Institute of Technology, the National Technical Institute for the Deaf at Rochester Institute of Technology, and Deaf Professional Arts Network for their work to create the dataset, the PopSign game, and overall competition preparation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| This is a Code Competition. Refer to [Code Requirements](https://kaggle.com/competitions/asl-signs/overview/code-requirements) for details |\n",
    "|--------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "\n",
    "**PopSign is an app developed by the Georgia Institute of Technology and the National Technical Institute for the Deaf at Rochester Institute of Technology. The app is available in beta on Android and iOS.*\n",
    "\n",
    "^*We cannot guarantee the competition will benefit the competitors or the disabled community directly.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Evaluation__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metric for this contest is simple classification __accuracy__."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Submission Process__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this competition you will be submitting a TensorFlow Lite model file. The model must take one or more landmark frames as an input and return a float vector (the predicted probabilities of each sign class) as the output. Your model must be packaged into a `submission.zip` file and compatible with the [TensorFlow Lite Runtime v2.9.1](https://www.tensorflow.org/lite/guide/python#run_an_inference_using_tflite_runtime). You are welcome to train your model using the framework of your choice, as long as you convert the model checkpoint into the tflite format prior to submission.\n",
    "\n",
    "Your model must also perform inference with less than 100 milliseconds of latency per video on average and use less than 40 MB of storage space. Expect to see approximately 40,000 videos in the test set. We allow an additional 10 minute buffer for loading the data and miscellaneous overhead.\n",
    "\n",
    "Each video is loaded with the following function:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference is performed (roughly) as follows, ignoring details like how we manage multiple videos:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import tflite_runtime.interpreter as tflite\n",
    "interpreter = tflite.Interpreter(model_path)\n",
    "\n",
    "found_signatures = list(interpreter.get_signature_list().keys())\n",
    "\n",
    "if REQUIRED_SIGNATURE not in found_signatures:\n",
    "    raise KernelEvalException('Required input signature not found.')\n",
    "\n",
    "prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "output = prediction_fn(inputs=frames)\n",
    "sign = np.argmax(output[\"outputs\"])\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Timeline__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __February 23, 2023__ - Start Date.\n",
    "\n",
    "- __April 25, 2023__ - Entry Deadline. You must accept the competition rules before this date in order to compete.\n",
    "\n",
    "- __April 25, 2023__ - Team Merger Deadline. This is the last day participants may join or merge teams.\n",
    "\n",
    "- __May 2, 2023__ - Final Submission Deadline.\n",
    "\n",
    "All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Prizes__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1st Place - $50,000\n",
    "* 2nd Place - $20,000\n",
    "* 3rd Place - $10,000\n",
    "* 4th Place - $10,000\n",
    "* 5th Place - $10,000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Code Requirements__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __This is a Code Competition__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submissions to this competition must be made through Notebooks. In order for the \"Submit\" button to be active after a commit, the following conditions must be met:\n",
    "\n",
    "* CPU Notebook <= 9 hours run-time\n",
    "* GPU Notebook <= 9 hours run-time\n",
    "* Internet access disabled\n",
    "* Freely & publicly available external data is allowed, including pre-trained models\n",
    "* Submission file must be named submission.zip.\n",
    "\n",
    "Please see the Code [Competition FAQ](https://www.kaggle.com/docs/competitions#notebooks-only-FAQ) for more information on how to submit. And review the [code debugging doc](https://www.kaggle.com/code-competition-debugging) if you are encountering submission errors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Acknowledgements__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provided by Deaf Professional Arts Network and the Georgia Institute of Technology is licensed under [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/). Kaggle and Google do not own and have not validated the dataset in any way."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Data Card__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Dataset Card for the Isolated Sign Language Recognition Corpus__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Dataset Summary__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Isolated Sign Language Recognition corpus (version 1.0) is a collection of hand and facial landmarks generated by Mediapipe version 0.9.0.1 on ~100k videos of isolated signs performed by 21 Deaf signers from a 250-sign vocabulary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Supported Tasks and Leaderboards__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/competitions/asl-signs/leaderboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Languages__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "American Sign Language"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Dataset Structure__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Data Instances__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'frame': 27, 'row_id': '27-face-0', 'type': 'face', 'landmark_index': 0, 'x': 0.4764270484447479, 'y': 0.3772650957107544, 'z': -0.05066078156232834}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Data Fields__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://www.kaggle.com/competitions/asl-signs/data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Data Splits__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not applicable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Dataset Creation__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Curation Rationale__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signs in the dataset represent 250 of the first concepts taught to infants in any language. The goal is to create an isolated sign recognizer to incorporate into educational games for helping hearing parents of Deaf children learn American Sign Language (ASL). Around 90% of deaf infants are born to hearing parents, many of whom may not know American Sign Language. (kdhe.ks.gov, deafchildren.org). Surrounding Deaf children with sign helps avoid Language Deprivation Syndrome. This syndrome is characterized by a lack of access to naturally occurring language acquisition during the critical language-learning years. It can cause serious impacts on different aspects of their lives, such as relationships, education, and employment.\n",
    "\n",
    "Learning American Sign Language (ASL) is as difficult for English speakers as learning Japanese (jstor.org). It takes time and resources that many parents don't have. They want to learn sign language, but it's hard when they are working long hours just to make ends meet. And even if they find the time and money for classes, the classes are often far away. \n",
    "PopSign is a smartphone game app that makes learning American Sign Language fun, interactive, and accessible. Currently, players match videos of ASL signs with bubbles containing written English words to pop the bubbles and advance game play.\n",
    "By adding isolated sign language recognition to Popsign, parents will play the game by making the signs instead of watching videos of signing. This sort of expressive practice improves confidence for communicating with Deaf children and the Deaf community."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Source Data__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### __Initial Data Collection and Normalization__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signers who communicate using American Sign Language as their primary language were recruited from across the United States. They were shipped a Pixel 4a smartphone with an installed collection app. The app prompted the signer with the concept in English to sign, randomly selected from the 250-sign vocabulary. Signers pressed and held an on-screen button on the phone to record video while signing each concept, releasing the button after each sign. The video of the sign is extracted with a buffer 0.5 seconds before the press of the button and 0.5 seconds after the release of the button. This method of video collection matches the game interface where players touch the screen to aim a bubble and release the touch after they have finished signing.\n",
    "\n",
    "While the app provided a video example of the sign desired, signers routinely made variants of the sign based on their background and region. More rarely, signers might fingerspell a sign, miss it completely, or produce the wrong sign. Extraneous movements, such as scratching an itch, or the ending movement from the previous sign or the onset of the next sign, are sometimes included. Conversely, some signers pressed the button late or released the button early, causing cropping in some sign examples. Some signers sign with their left hand; others sign with their right. Some signers switch their signing hand. All of these situations must be handled by the game’s recognition system.\n",
    "\n",
    "While the game includes 250 signs, it only needs to distinguish between five signs at a time due to the game design. Since accuracy increases as vocabulary decreases, even a recognition system with 60% accuracy on the 250-sign task should perform well when distinguishing between five signs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### __Who are the source language producers?__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21 signers recruited by the Deaf Professional Arts Network provided the sign. They are from many regions across the United States and all use American Sign Language as their primary form of communication. They represent a mix of skin tones and genders."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Annotations__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### __Annotation process__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each video was annotated at creation time by the smartphone app. Videos were coarsely reviewed to attempt to remove poor recordings, but little judgment was made on the correctness or quality of the sign itself."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### __Who are the annotators?__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researchers at the Georgia Institute of Technology coarsely reviewed the individual videos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Personal and Sensitive Information__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The landmark data has been de-identified. Landmark data should not be used to identify or re-identify an individual. Landmark data is not intended to enable any form of identity recognition or store any unique biometric identification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Considerations for Using the Data__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Social Impact of Dataset__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Isolated Sign Language Recognition corpus (version 1.0), which contains Mediapipe landmarks only, will be used to create sign language recognition systems for Popsign, an educational game that encourages hearing parents of deaf infants to practice their ASL signing. The same dataset can be used to add signing to other games. For example, one proposed use is to create a game that allows Deaf children to practice their written English skills. The video set upon which the corpus is based is being used to examine variations in signing and provide examples of those variations for the wider Deaf community."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Discussion of Biases__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While ASL is the most common sign language used in the United States, there are many sign languages, including British Sign Language, Native American Sign Languages, Hawaiian Sign Language, French Sign Language, and Signed Exact English. In addition, there are many regional and cultural accents associated with sign in the United States, including Black Sign Language. This dataset focuses on American Sign Language, but it does not capture a representative sample of all the sign variations that would be commonly understood in conversation. ASL has a grammar that is very different from English, and isolated signs do not capture the variation that occurs when a concept is signed in context. A larger number of signers is necessary to better represent skin tones, hand features, and different levels of signing dexterity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Other Known Limitations__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isolated sign dataset is intended to help create educational games for teaching ASL, and is not appropriate for other purposes such as ASL-to-English translation or natural language interfaces for computers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Additional Information__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Dataset Curators__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deaf Professional Arts Network (DPAN), is a 501(c)(3) non-profit founded in 2006 to make music, entertainment, and media accessible. The Georgia Institute of Technology is a top-10 public research university committed to improving the human condition through advanced science and technology. The National Technical Institute for the Deaf is one of the nine colleges of the Rochester Institute of Technology and is home to the world’s first and largest technological college for deaf and hard-of-hearing students."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Licensing Information__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provided by Deaf Professional Arts Network and the Georgia Institute of Technology is licensed under CC-BY."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Citation Information__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the bottom of the Kaggle competition overview page for citation information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Contributions__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the staff at DPAN and the students and faculty at Georgia Tech and NTID who make Popsign and this dataset possible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "—-"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Data__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Dataset Description__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deaf children are often born to hearing parents who do not know sign language. Your challenge in this competition is to help identify signs made in processed videos, which will support the development of mobile apps to help teach parents sign language so they can communicate with their Deaf children.\n",
    "\n",
    "This competition requires submissions to be made in the form of [TensorFlow Lite models](https://www.tensorflow.org/lite). You are welcome to train your model using the framework of your choice as long as you convert the model checkpoint into the tflite format prior to submission. Please see [the evaluation page](https://www.kaggle.com/competitions/asl-signs/overview/evaluation) for details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __train_landmark_files/[participant_id]/[sequence_id].parquet__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The landmark data. The landmarks were extracted from raw videos with the [MediaPipe holistic model](https://google.github.io/mediapipe/solutions/holistic.html). Not all of the frames necessarily had visible hands or hands that could be detected by the model. \n",
    "\n",
    "*Landmark data should not be used to identify or re-identify an individual. Landmark data is not intended to enable any form of identity recognition or store any unique biometric identification.*\n",
    "\n",
    "- `frame` - The frame number in the raw video.\n",
    "- `row_id` - A unique identifier for the row.\n",
    "- `type` - The type of landmark. One of ['face', 'left_hand', 'pose', 'right_hand'].\n",
    "- `landmark_index` - The landmark index number. Details of the hand landmark locations can be found here.\n",
    "- `[x/y/z]` - The normalized spatial coordinates of the landmark. These are the only columns that will be provided to your submitted model for inference. The MediaPipe model is not fully trained to predict depth so you may wish to ignore the z values.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [__train.csv__](../data/asl-signs/train.csv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `path` - The path to the landmark file.\n",
    "- `participant_id` - A unique identifier for the data contributor.\n",
    "- `sequence_id` - A unique identifier for the landmark sequence.\n",
    "- `sign` - The label for the landmark sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [__sign_to_prediction_index_map.json__ (3.35 kB)](../data/asl-signs/sign_to_prediction_index_map.json)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "kaggle competitions download -c asl-signs\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __1. Domain Knowledge__: Sign Language"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Basics of (American) Sign Language__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Preservation of Sign Language by George W. Veditz (former president of National Association of the Deaf of the United States): "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/XITbj3NTLUQ/0.jpg)](https://www.youtube.com/watch?v=XITbj3NTLUQ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### American Sign Language"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one out of many other sign languages in the world\n",
    "- visual-manual modality\n",
    "- fully-fledged language with own grammar and lexicon\n",
    "- distinct from English, e.g. British and American sign languages differ\n",
    "- regional accents, dialect, slang, etc. \n",
    "- fingerspelling: only for names or to indicate an English word\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fingerspelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Palm usually faces the viewer! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ASL finger spelling](https://www.researchgate.net/profile/Sergio-Benini/publication/259921409/figure/fig1/AS:614251592949760@1523460400036/ASL-finger-spelling-alphabet-reproduced-from-3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Signs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Basic ASL](https://www.dummies.com/wp-content/uploads/Sign-language-essential-expressions.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASL grammar basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Spatial grammar and simultaneity__\n",
    "\n",
    "- main difference: spoken language is linear vs. visual signs can be simultaneous!  \n",
    "- classifiers (handshapes) to visually indicate\n",
    "    - objects\n",
    "    - people\n",
    "    - size and shape\n",
    "    - location\n",
    "    - movement\n",
    "        \n",
    "__Iconicity__\n",
    "- similarity between “form” and “meaning” \n",
    "\n",
    "__Non-manual elements__ \n",
    "- movements of body, head, eyebrows, eyes, cheeks, mouth\n",
    "    - e.g. facial expressions accompany verbs of emotion, as in the sign for angry\n",
    "    - e.g. “not yet”: requires that the tongue touch the lower lip and that the head rotate from side to side, in addition to - the manual part of the sign\n",
    "    - e.g. asking a question: \n",
    "        - for “yes” or “no” answer: raise eyebrows\n",
    "        - for more detailed answer: lower eybrows\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many signs?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 10,000 different ASL signs exist that corresponds to English, which has about 200,000 words…\n",
    "\n",
    "--> conciseness, efficiency, simultaneity\n",
    "\n",
    "--> language restricted vocabulary\n",
    "\t\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.nidcd.nih.gov/health/american-sign-language\n",
    "\n",
    "https://en.wikipedia.org/wiki/Sign_language#Relationships_with_spoken_languages\n",
    "\n",
    "https://www.youtube.com/watch?v=XS2c07HCdyo\n",
    "\n",
    "https://link.springer.com/article/10.1007/s00371-014-0921-x\n",
    "\n",
    "https://www.youtube.com/watch?v=BzGkYu47wuk\n",
    "\n",
    "https://www.babiesandlanguage.com/what-is-iconicity-and-arbitrariness/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Geographical and demographical statistics__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many sign languages are there? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- different sign languages used around the world\n",
    "- sign languages developed naturally through different groups of people interacting with each other\n",
    "- between 138 and 300 different types of sign language \n",
    "- countries that share the same spoken language do not necessarily have the same sign language \n",
    "\n",
    "Home Sign:\n",
    "- deaf children of hearing parents create their own signs\n",
    "- invented spontaneously by a deaf child who lacks accessible linguistic input    \n",
    "\n",
    "International Sign: \n",
    "- international communication system \n",
    "- highly variable type of signed communication used between two signers who lack a common sign language\n",
    "- no fixed grammar or lexicon; relies heavily on gestures\n",
    "- mostly used at international meetings or conferences of the deaf community\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution around the globe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Global Map of Sign Languages](https://upload.wikimedia.org/wikipedia/commons/e/ed/Sign_language_families.svg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![#5d6a8f](https://placeholder.com/15/5d6a8f/000000?text) French Sign Language family\n",
    "\n",
    "![#1e2f5d](https://placeholder.com/15/1e2f5d/000000?text) American Sign Language (ASL) cluster, derived from FSL\n",
    "\n",
    "![#93a1c7](https://placeholder.com/15/93a1c7/000000?text) Russian Sign Language cluster, derived from FSL\n",
    "\n",
    "![#a8d3db](https://placeholder.com/15/a8d3db/000000?text) Czech Sign Language cluster, derived from FSL\n",
    "\n",
    "![#9c6cd0](https://placeholder.com/15/9c6cd0/000000?text) Danish Sign Language family, probably related to either FSL or SSL\n",
    "\n",
    "![#a596d8](https://placeholder.com/15/a596d8/000000?text) Swedish Sign Language family, probably related to DSL\n",
    "\n",
    "![#b08660](https://placeholder.com/15/b08660/000000?text) German Sign Language family\n",
    "\n",
    "![#660066](https://placeholder.com/15/660066/000000?text) Vietnamese sign languages, also some Thai and Lao SLs\n",
    "\n",
    "![#383834](https://placeholder.com/15/383834/000000?text) Arab sign-language family\n",
    "\n",
    "![#99ff00](https://placeholder.com/15/99ff00/000000?text) Indian Sign Language\n",
    "\n",
    "![#ff9691](https://placeholder.com/15/ff9691/000000?text) Chinese Sign Language (unrelated to Taiwanese Sign Language)\n",
    "\n",
    "![#bd4b31](https://placeholder.com/15/bd4b31/000000?text) Japanese Sign Language family (including Taiwanese Sign Language)\n",
    "\n",
    "![#4b874b](https://placeholder.com/15/4b874b/000000?text) BANZSL family (British, Australian and New Zealand Sign Language)\n",
    "\n",
    "![#1c5e31](https://placeholder.com/15/1c5e31/000000?text) South African Sign Language, derived from BANZSL\n",
    "\n",
    "![#fff200](https://placeholder.com/15/fff200/000000?text) Isolated languages\n",
    "\n",
    "![#e6e6e6](https://placeholder.com/15/e6e6e6/000000?text) No data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phylogeny of Sign Languages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Phylogenetic Tree of Sign Languages](https://www.shh.mpg.de/1607317/original-1579622096.webp?t=eyJ3aWR0aCI6NjgyLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjE2MDczMTd9--b6a129694f8115ce0a2fe6f503dea46a3f8e36b9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognition of Sign Languages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Global Map of Sign Language Recognition](https://1.bp.blogspot.com/-2Z5k0754f6o/YKaMHgBM9cI/AAAAAAAA6eY/zHUQRGHCMH8u7NYgLuZvtLvN5QGiSP_NACLcBGAsYHQ/s2580/SL%2BInfographic%2Brecogniton.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many people use Sign Language?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 70 million hearing-impaired and deaf people all over the world use sign language (466 million deaf/hearing-impaired people in the world)\n",
    "- Estimates for sign language use are very crude, and definitions of what counts as proficiency are difficult\n",
    "- For most sign languages, there are no concrete estimates\n",
    "\n",
    "__ASL__:\n",
    "- In the United States, 500,000 hearing-impaired and deaf people use American sign language (ASL) \n",
    "- Only 1% of the hearing-impaired and deaf population in the United States\n",
    "- Not everyone with a hearing impairment has the resources to learn it\n",
    "- ASL is considered the fourth most-spoken language in the United States\n",
    "\n",
    "\n",
    "United Kingdom: over 80,000 British sign language speakers\n",
    "\n",
    "Indonesia: about 900,000 users of Indonesian sign language\n",
    "\n",
    "Germany: at least 200,000 people, of whom about 80,000 are deaf\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasons for usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- deaf or hearing impaired people use sign languages to communicate\n",
    "- allows them to learn, work, access services, and be included in their communities\n",
    "\n",
    "- it’s also used by hearing people to communicate with deaf family member or friends\n",
    "- some people learn ASL as a second language to be helpful, and to be an interpreter  \n",
    "\n",
    "__Benefits from learning sign language:__\n",
    "\n",
    "- enhanced spatial reasoning - how we view visual information in our surroundings and consider three dimensional objects\n",
    "- activates how you interpret body language, improves non-verbal communication and brings charisma\n",
    "- helps with peripheral vision and reaction time\n",
    "- helps with long-term cognition\n",
    "- you can be helpful for others\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Info graphic deaf people world-wide and proportion coming from hearing families](https://1.bp.blogspot.com/-a0NWU7nmNh0/XYKUKIpt35I/AAAAAAAAg34/mw7nQMEQ5RIUCvyDrdet2PLuG9SCTFOjQCLcBGAsYHQ/s1600/SL%2BInfographic%2BPopulation.jpeg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "https://earthweb.com/sign-language-users/https://www.ai-media.tv/ai-media-blog/sign-language-alphabets-from-around-the-world/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Sign_language\n",
    "\n",
    "https://www.unusualverse.com/2019/09/infographic-sign-language-rights-for-all.html\n",
    "\n",
    "https://www.deutschland.de/en/topic/life/germany-language-german-sign-language\n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_sign_languages_by_number_of_native_signers\n",
    "\n",
    "https://asianabsolute.co.uk/blog/2020/05/19/different-types-of-sign-language-used-around-the-world/\n",
    "\n",
    "https://www.shh.mpg.de/1607280/evolution-of-signed-languages "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Learning sign language__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Formal classes__: (schools, colleges, or community centers). \n",
    "- in-person \n",
    "- online \n",
    "\n",
    "__Self-study__: (through books, videos, or online resources).\n",
    "\n",
    "__Immersion__: this can be through attending deaf events, socializing with deaf individuals, or even living with deaf roommates or family members.\n",
    "\n",
    "__Language exchange__: Finding a deaf or hard-of-hearing individual who is willing to practice sign language. \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why people learn sign language?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Communication with deaf and hard-of-hearing individuals: it help break down barriers between hearing and non-hearing individuals.\n",
    "\n",
    "- Professional opportunities: in fields such as education, social work, and healthcare, where there may be a need to communicate with deaf and hard-of-hearing clients or patients.\n",
    "\n",
    "- Cultural interest\n",
    "\n",
    "- Personal growth\n",
    "\n",
    "- Communication with young children: Parents may learn sign language to communicate with young children who have not yet developed verbal language skills. \n",
    "\n",
    "- Enjoyment: Some people learn sign language simply for the joy and pleasure of learning a new language and skill.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Sign language apps__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Yes, many. Some more popular than others__: \n",
    "\n",
    "ASL Dictionary: Available for iOS (https://apps.apple.com/us/app/asl-dictionary-sign-language/id421837323) and Android (https://play.google.com/store/apps/details?id=com.soardevelopment.asldictionary&hl=en_US&gl=US).\n",
    "\n",
    "SignSchool: Available for iOS (https://apps.apple.com/us/app/signschool-learn-asl/id1448369094) and Android (https://play.google.com/store/apps/details?id=com.signschool).\n",
    "\n",
    "Marlee Signs: Available for iOS (https://apps.apple.com/us/app/marlee-signs/id576760883).\n",
    "\n",
    "ProDeaf Translator: Available for iOS (https://apps.apple.com/us/app/prodeaf-translator/id1047513535) and Android (https://play.google.com/store/apps/details?id=com.prodeaf.interpreter&hl=en_US&gl=US).\n",
    "\n",
    "ASL Coach: Available for iOS (https://apps.apple.com/us/app/asl-coach/id966794847) and Android (https://play.google.com/store/apps/details?id=com.aslcoachingapp&hl=en_US&gl=US).\n",
    "\n",
    "Signily Keyboard: Available for iOS (https://apps.apple.com/us/app/signily-keyboard-asl-stickers/id1243185410).\n",
    "\n",
    "The ASL App: Available for iOS (https://apps.apple.com/us/app/the-asl-app/id1261131354).\n",
    "\n",
    "Hands-On ASL!: Available for iOS (https://apps.apple.com/us/app/hands-on-asl/id1472012624).\n",
    "\n",
    "ASL Pocket Sign: Available for iOS (https://apps.apple.com/us/app/asl-pocket-sign/id1073506174).\n",
    "\n",
    "My Smart Hands Baby Sign Language Dictionary: Available for iOS (https://apps.apple.com/us/app/my-smart-hands-baby-sign-language-dictionary/id367907212)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Impact of Sign Language__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do all deaf children learn ASL?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- children with hearing loss typically receive hearing aids or cochlear implants (CI)\n",
    "    - diagnosis in hospital\n",
    "    - CI implantation within the first year\n",
    "- thus many parents never teach their children ASL or at least do not learn it themselves\n",
    "- Problem: technology often does’t work very well, no way to predict whether it works\n",
    "    - one study: 47%  of deaf children stopped using their implants\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CI + spoken language or ASL?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CI (cochlear implants) + spoken language: \n",
    "- might hinder spoken language acquisition\n",
    "- spoken language with CI is super time sensitive\n",
    "- parents have to learn ASL too\n",
    "- learning ASL is time consuming and expensive \n",
    "\n",
    "(A)SL ((American) Sign Language): \n",
    "- protects from linguistic deprivation\n",
    "- promotes early parent-child communication\n",
    "- when CI fails, need a way of communicating\n",
    "- able to communicate across social circles\n",
    "- lip reading only about 60% accuracy\n",
    "- sign language correlates positively with written & spoken language development\n",
    "- impossible to predict if CI works\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilingual is ideal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- children display better mental flexibility, cognitive control, creative thinking\n",
    "- perform better academically, better communication with family\n",
    "- bad communication correlates to more symptoms of depression\n",
    "- more opportunities to socialize in both languages\n",
    "- alleviates stress concerning CI: you can have a full, rich life even if CI fails"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://crownschool.uchicago.edu/ssa_magazine/sign-language-best-deaf-children.html\n",
    "\n",
    "https://www.researchgate.net/publication/278732836_Should_All_Deaf_Children_Learn_Sign_Language\n",
    "\n",
    "https://deafchildren.org/2019/02/deaf-adult-role-models/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# for baseline model\n",
    "%pip install nb_black --quiet # for autoformatting\n",
    "%load_ext lab_black\n",
    "\n",
    "# for video visualization of parquet files\n",
    "%pip install -q flatbuffers 2> /dev/null\n",
    "%pip install -q mediapipe 2> /dev/null"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[1794]: Class CaptureDelegate is implemented in both /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x17ef625a0) and /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x288b40860). One of the two will be used. Which one is undefined.\n",
      "objc[1794]: Class CVWindow is implemented in both /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x17ef625f0) and /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x17b058a68). One of the two will be used. Which one is undefined.\n",
      "objc[1794]: Class CVView is implemented in both /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x17ef62618) and /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x17b058a90). One of the two will be used. Which one is undefined.\n",
      "objc[1794]: Class CVSlider is implemented in both /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x17ef62640) and /Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x17b058ab8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "# general\n",
    "import os  # easier file path handling\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for loading .json dictionary\n",
    "import json\n",
    "\n",
    "# for visualizing missing values\n",
    "import missingno as msno\n",
    "\n",
    "# for visualization of parquet files\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "import IPython\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ds\n",
    "from IPython.display import HTML\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "# for data pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for baseline model\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# for model evaluation\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later this function can be used to loop through all `.parquet` files (e.g. based on the `train.csv` file). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to load .parquet file into a new dataframe\n",
    "def load_parquet_file_to_df(parquet_path):\n",
    "    \"\"\"loads a .parquet file into a dataframe\n",
    "\n",
    "    Args:\n",
    "        parquet_path (string): path to the .parquet file to load\n",
    "\n",
    "    Returns:\n",
    "        df_parquet: dataframe of .parquet file\n",
    "    \"\"\"\n",
    "\n",
    "    # load parquet file\n",
    "    df_parquet = pd.read_parquet(\"../data/asl-signs/\" + parquet_path)\n",
    "    return df_parquet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __2. Data Mining__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For local development, the whole data from the Kaggle competition was downloaded from https://www.kaggle.com/competitions/asl-signs/data into the `data/asl-signs/` folder of our working repository on GitHub https://github.com/vosmani36/Capstone_Project_SignMeUp (the `data` folder was put on `.gitignore` and is only available locally). \n",
    "\n",
    "A brief description of the data: \n",
    "\n",
    "__Files__: 94479 files\n",
    "\n",
    "__Size__: 56.43 GB\n",
    "\n",
    "__Type__: parquet, csv, json\n",
    "\n",
    "Among those data are a\n",
    "\n",
    "- `sign_to_prediction_index_map.json` file, containing a dictionary of all available signs (key, type: `string`) with their corresponding categorical values (value, type: `numeric`). \n",
    "- `train.csv` file, which is a list of all recorded sequences (single word signing) with some basic information. \n",
    "- `train_landmark_files` folder containing subfolders for each participant, which further contain all the `.parquet` files (one `.parquet` file corresponds to one sequence, which is basically the video record of one single sign). \n",
    "\n",
    "In the following, we will first conduct analysis on the `train.csv` file. Then, based on our findings, we will look at all the parquet files and use them for training a best sign language action detection model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set `INPUT_PATH` and `OUTPUT_PATH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SERVER == \"local\":\n",
    "    INPUT_PATH = \"../data/asl-signs\"\n",
    "    OUTPUT_PATH = \"../data\"\n",
    "if SERVER == \"kaggle\":\n",
    "    INPUT_PATH = \"/kaggle/input/asl-signs\"\n",
    "    OUTPUT_PATH = \"/kaggle/working\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `sign_to_prediction_index_map.json` file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the .json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(INPUT_PATH, \"sign_to_prediction_index_map.json\")) as json_file:\n",
    "    sign_to_prediction_index_map = json.load(json_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `train.csv` file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `path` - The path to the landmark file.\n",
    "- `participant_id` - A unique identifier for the data contributor.\n",
    "- `sequence_id` - A unique identifier for the landmark sequence.\n",
    "- `sign` - The label for the landmark sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the train.csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train.csv file into a dataframe\n",
    "fname = \"train.csv\"\n",
    "df_csv = pd.read_csv(os.path.join(INPUT_PATH, fname))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load subsample of data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsample the data, if set by user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample data, if in \"interactive\" mode\n",
    "p = 0.001  # subsample fraction of whole data\n",
    "if MODE == \"interactive\":\n",
    "    df_csv = df_csv.sample(frac=p, random_state=RSEED).reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the train.csv data, quickly check the first rows and the dimensions of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_landmark_files/28656/3311214787.parquet</td>\n",
       "      <td>28656</td>\n",
       "      <td>3311214787</td>\n",
       "      <td>sticky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_landmark_files/53618/3588192588.parquet</td>\n",
       "      <td>53618</td>\n",
       "      <td>3588192588</td>\n",
       "      <td>before</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_landmark_files/4718/1363575346.parquet</td>\n",
       "      <td>4718</td>\n",
       "      <td>1363575346</td>\n",
       "      <td>pretty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_landmark_files/37779/951199059.parquet</td>\n",
       "      <td>37779</td>\n",
       "      <td>951199059</td>\n",
       "      <td>hen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_landmark_files/36257/283190141.parquet</td>\n",
       "      <td>36257</td>\n",
       "      <td>283190141</td>\n",
       "      <td>tomorrow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            path  participant_id  sequence_id  \\\n",
       "0  train_landmark_files/28656/3311214787.parquet           28656   3311214787   \n",
       "1  train_landmark_files/53618/3588192588.parquet           53618   3588192588   \n",
       "2   train_landmark_files/4718/1363575346.parquet            4718   1363575346   \n",
       "3   train_landmark_files/37779/951199059.parquet           37779    951199059   \n",
       "4   train_landmark_files/36257/283190141.parquet           36257    283190141   \n",
       "\n",
       "       sign  \n",
       "0    sticky  \n",
       "1    before  \n",
       "2    pretty  \n",
       "3       hen  \n",
       "4  tomorrow  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `.parquet` files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The landmark data. The landmarks were extracted from raw videos with the MediaPipe holistic model. Not all of the frames necessarily had visible hands or hands that could be detected by the model.\n",
    "\n",
    "Landmark data should not be used to identify or re-identify an individual. Landmark data is not intended to enable any form of identity recognition or store any unique biometric identification.\n",
    "\n",
    "- `frame` - The frame number in the raw video.\n",
    "- `row_id` - A unique identifier for the row.\n",
    "- `type` - The type of landmark. One of ['face', 'left_hand', 'pose', 'right_hand'].\n",
    "- `landmark_index` - The landmark index number. Details of the hand landmark locations can be found here.\n",
    "- [`x`/`y`/`z`] - The normalized spatial coordinates of the landmark. These are the only columns that will be provided to your submitted model for inference. The MediaPipe model is not fully trained to predict depth so you may wish to ignore the z values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading all `.parquet` files into one dataframe can get very big. So here, we load one example `.parquet` file into a dataframe using the helper function `load_parquet_file_to_df()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load example parquet file from train.csv dataframe into a new dataframe\n",
    "parquet_path = df_csv.path[0]\n",
    "df_parquet = load_parquet_file_to_df(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>row_id</th>\n",
       "      <th>type</th>\n",
       "      <th>landmark_index</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>22-face-0</td>\n",
       "      <td>face</td>\n",
       "      <td>0</td>\n",
       "      <td>0.555303</td>\n",
       "      <td>0.441405</td>\n",
       "      <td>-0.058642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>22-face-1</td>\n",
       "      <td>face</td>\n",
       "      <td>1</td>\n",
       "      <td>0.551702</td>\n",
       "      <td>0.403915</td>\n",
       "      <td>-0.079649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>22-face-2</td>\n",
       "      <td>face</td>\n",
       "      <td>2</td>\n",
       "      <td>0.553267</td>\n",
       "      <td>0.418683</td>\n",
       "      <td>-0.048917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>22-face-3</td>\n",
       "      <td>face</td>\n",
       "      <td>3</td>\n",
       "      <td>0.539900</td>\n",
       "      <td>0.376674</td>\n",
       "      <td>-0.048386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>22-face-4</td>\n",
       "      <td>face</td>\n",
       "      <td>4</td>\n",
       "      <td>0.551267</td>\n",
       "      <td>0.393446</td>\n",
       "      <td>-0.081520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   frame     row_id  type  landmark_index         x         y         z\n",
       "0     22  22-face-0  face               0  0.555303  0.441405 -0.058642\n",
       "1     22  22-face-1  face               1  0.551702  0.403915 -0.079649\n",
       "2     22  22-face-2  face               2  0.553267  0.418683 -0.048917\n",
       "3     22  22-face-3  face               3  0.539900  0.376674 -0.048386\n",
       "4     22  22-face-4  face               4  0.551267  0.393446 -0.081520"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parquet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11403, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parquet.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __3. Data Cleaning__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The train.csv file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for `null` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path              0\n",
       "participant_id    0\n",
       "sequence_id       0\n",
       "sign              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have any `null` values :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for `na` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path              0\n",
       "participant_id    0\n",
       "sequence_id       0\n",
       "sign              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have any `na` values :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv.duplicated().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have any duplicates in our data :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The .parquet files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTINUE HERE 2023-04-15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for `null` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1947214667.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    ---\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Cannot parse: 1:3: ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages/lab_black.py\", line 218, in format_cell\n",
      "    formatted_code = _format_code(cell)\n",
      "  File \"/Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages/lab_black.py\", line 29, in _format_code\n",
      "    return format_str(src_contents=code, mode=FileMode())\n",
      "  File \"/Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages/black/__init__.py\", line 1035, in format_str\n",
      "    src_node = lib2to3_parse(src_contents.lstrip(), mode.target_versions)\n",
      "  File \"/Users/jin-holee/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages/black/parsing.py\", line 96, in lib2to3_parse\n",
      "    raise exc from None\n",
      "black.parsing.InvalidInput: Cannot parse: 1:3: ---\n"
     ]
    }
   ],
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for `na` values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X.shape: (number of sequences, number of frames per sequence, total number of key points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __4. Exploratory Data Analysis__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `sign_to_prediction_index_map.json` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a first look at the .json file. We will later use this file to translate / encode our target feature for model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_to_prediction_index_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the number of key-value pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of key-value pairs\n",
    "len(sign_to_prediction_index_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys\n",
    "sign_to_prediction_index_map.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values\n",
    "sign_to_prediction_index_map.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary consists of \n",
    "- 250 `key`-`value` pairs\n",
    "- `keys` as single words\n",
    "- `values` as integers ranging from 0 to 249"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `train.csv` files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file includes a list of all recorded sequences. Each row denotes one sequence and has information about participant_id and target sign.\n",
    "\n",
    "* We have around **95000 sequences**.\n",
    "* They show **250 signs**. Each sign is shown approximately 300 to 400 times.\n",
    "* The sequences were recorded by **21 participants**. Each participant contributed 3500 to 5000 sequences. \n",
    "\n",
    "We add the label numbers to the signs in a column called **\"target\"**.\n",
    "\n",
    "* There are no duplicate rows. \n",
    "* There are no missing values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 94477 rows and 4 columns of data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names are `path` (object), `participant_id` (int64), `sequence_id` (int64) and `sign` (object). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 94477 unique `path`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.path.unique().shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.path.unique()[0] # example path from first row "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 94477 unique paths (equal to the number of rows) for parquet files, which means that each row has a corresponding parquet file. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 94477 unique `sequence_id`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.sequence_id.unique().shape[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have 94477 unique `sequence_id`s (equal to the number of rows), which means that each row corresponds to a single sequence. \n",
    "\n",
    "Thus, each parquet file in `path` corresponds to a certain sequence in `sequence_id`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21 unique `participant_id`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.participant_id.unique().shape[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 21 unique participant IDs in our dataset, which are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_csv.participant_id.unique(), columns=['participant_id'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 250 unique `sign`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.sign.unique().shape[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 250 unique signs in our dataset, which are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_csv.sign.unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average 378 sequences per sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[['sign', 'sequence_id']].groupby('sign').count().sort_values(by='sequence_id').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[['sign', 'sequence_id']].groupby('sign').count().sort_values(by='sequence_id').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[['sign', 'sequence_id']].groupby('sign').count().sort_values(by='sequence_id').plot(kind='bar', figsize=(15,3), fontsize=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[['sign', 'sequence_id']].groupby('sign').count().sort_values(by='sequence_id').describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have between 300 to 400 recorded total sequences for each sign. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average 4499 sequences per participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[['participant_id', 'sequence_id']].groupby('participant_id').count().sort_values(by='sequence_id').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[['sequence_id', 'participant_id']].groupby('participant_id').count().sort_values(by='sequence_id').plot(kind='bar', figsize=(15,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[['sequence_id', 'participant_id']].groupby('participant_id').count().sort_values(by='sequence_id').describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All participants signed well over 3000 sequences and most participants recorded even over 4000 sequences. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some missing signs for two participants 30680 and 25571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[['participant_id', 'sign']].groupby('participant_id').nunique().sort_values(by='sign').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[['participant_id', 'sign']].groupby('participant_id').nunique().sort_values(by='sign').plot(kind='bar', figsize=(15,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[['participant_id', 'sign']].groupby('participant_id').nunique().sort_values(by='sign').describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset some signs are not available from participants 30680 and 25571. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For participant 30680 following signs are missing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_csv.sign.unique()).difference(set(df_csv[df_csv.participant_id == 30680].sign.unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For participant 25571 following signs are missing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_csv.sign.unique()).difference(set(df_csv[df_csv.participant_id == 25571].sign.unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `.parquet` files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most of the stuff past this point, needs to be validated with the whole dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The parquet files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __3. Data Cleaning__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns, that we don't need. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop `row_id` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop row_id\n",
    "df = df.drop('row_id', axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop `z` column "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The client mentioned \"The MediaPipe model is not fully trained to predict depth so you may wish to ignore the z values.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop z\n",
    "df = df.drop('z', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue in Ronja's notebook at \n",
    "\n",
    "**Make one column per frame**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make one column per frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make one column per frame with all the landmarks \n",
    "\n",
    "df_wide = df\n",
    "df_wide['idx'] = df_wide.groupby('sequence_id').cumcount()+1\n",
    "df_wide = df_wide.pivot_table(index=['sequence_id', 'frame', 'sign'], columns=['landmark_index', 'type'], \n",
    "                    values=['x', 'y'], aggfunc='first')\n",
    "\n",
    "df_wide = df_wide.sort_index(axis=1, level=1)\n",
    "df_wide.columns = [f'{x}_{y}_{z}' for x,y,z in df_wide.columns]\n",
    "df_wide = df_wide.reset_index()\n",
    "\n",
    "df_wide.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for `null` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide[df_wide.y_467_face.isnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a considerable amount of `null` values. So, let's look at them all at once in a heatmap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(df_wide.isnull().transpose(),\n",
    "            cmap=\"YlGnBu\",\n",
    "            cbar_kws={'label': 'Missing Data'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have a larger distribution of missing values in the data columns representing the hand landmarks. This can be explained by the fact, that participants had to hold their smartphones in one hand for recording and had only the other hand available for signing. \n",
    "\n",
    "Despite those, there seems to be some entire rows with missing values, thus might be some error / glitch in the sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for `na` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(df_wide.isna().transpose(),\n",
    "            cmap=\"YlGnBu\",\n",
    "            cbar_kws={'label': 'Missing Data'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically showing the same as already seen for `null` values. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing / Infering missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on above findings, we decided to deal with the missing values as follows: \n",
    "\n",
    "1. `GROUP` data `BY` sequences (interpolation should only be done per sequence)\n",
    "2. `IF` are less than 2 consecutive `NaN`s (limit = 2), interpolate `NaN`s linearly from their neighboring rows (frame before and after) \n",
    "3. Impute all remaining `NaN`s to 0.\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolation of single missing values from neighboring values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use interpolate method for single missing values\n",
    "df_wide = df_wide.groupby('sequence_id').apply(lambda group: group.interpolate(limit = 2, limit_direction = 'both'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check what happened. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check updated dataframe\n",
    "plt.figure()\n",
    "sns.heatmap(df_wide.isna().transpose(),\n",
    "            cmap=\"YlGnBu\",\n",
    "            cbar_kws={'label': 'Missing Data'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we already got rid of all the single rows with missing values by imputing their values from the neighboring rows. Now, we can impute the remaining missing values with 0. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing large patches of missing values to `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the rest of the NaN with 0\n",
    "df_wide.fillna(0, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check what happened. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check updated dataframe\n",
    "plt.figure()\n",
    "sns.heatmap(df_wide.isna().transpose(),\n",
    "            cmap=\"YlGnBu\",\n",
    "            cbar_kws={'label': 'Missing Data'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of all na and null values\n",
    "(df_wide.isnull().sum() + df_wide.isna().sum()).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __4. Exploratory Data Analysis__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `frame` column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of frames per sequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some descriptive statistics on the `frame` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide.groupby(\"sequence_id\").frame.describe().round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `count` (count of frames for each sequence) column shows that \n",
    "* some sequences are too short. We will remove sequences shorter than 5 frames. \n",
    "* some sequences are too long. We will remove sequences longer than 130 frames. \n",
    "* sequences differ a lot in their lengths. We will bring all sequences to the same length by cropping or padding. \n",
    "\n",
    "The `min` column shows us, that most sequences don't start at frame 0 (maybe due to some pre-processing by the client). We are going to reassign all frame indices so that each sequence starts with frame 0. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of frames per sign"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get the frame counts for each sequence by reading in each parquet file one by one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_max = df_csv.shape[0]\n",
    "n_frames_all = [] # container for all frame counts\n",
    "\n",
    "# loop over all rows in train.csv dataframe to get the number of frames\n",
    "for i in range(i_max): \n",
    "    # print every 100 i\n",
    "    if i == 0:\n",
    "        print(i)\n",
    "    if i%100 == 0: \n",
    "        print(i)\n",
    "    if i == i_max: \n",
    "        print(i)\n",
    "    df_parquet = pd.read_parquet('../data/asl-signs/' + df_csv.path[i])\n",
    "    n_frames = df_parquet.frame.nunique()\n",
    "    n_frames_all.append(n_frames)\n",
    "\n",
    "n_frames_all = pd.Series(n_frames_all, name='n_frames') # convert to pd.Series\n",
    "df_frames = pd.concat([n_frames_all, df_csv.sign, df_csv.participant_id], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's loot at the boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order by decreasing mean\n",
    "my_order = df_frames.groupby(by=[\"sign\"])[\"n_frames\"].mean().sort_values(ascending=False).index\n",
    "# boxplot\n",
    "sns.boxplot(data=df_frames, x=\"sign\", y=\"n_frames\", order=my_order)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the barplot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order by decreasing mean\n",
    "my_order = df_frames.groupby(by=[\"sign\"])[\"n_frames\"].mean().sort_values(ascending=False).index\n",
    "# barplot\n",
    "sns.barplot(data=df_frames, x=\"sign\", y=\"n_frames\", order=my_order)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of frames per participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order by decreasing mean\n",
    "my_order = df_frames.groupby(by=[\"participant_id\"])[\"n_frames\"].mean().sort_values(ascending=False).index\n",
    "# boxplot\n",
    "sns.boxplot(data=df_frames, x=\"participant_id\", y=\"n_frames\", order=my_order)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order by decreasing mean\n",
    "my_order = df_frames.groupby(by=[\"participant_id\"])[\"n_frames\"].mean().sort_values(ascending=False).index\n",
    "# barplot\n",
    "sns.barplot(data=df_frames, x=\"participant_id\", y=\"n_frames\", order=my_order)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some participants (49445) needed up to almost 70 frames per sequence, and others (37779) down to only 10 frames per sequence. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check: Look at the distribution of signs per participant"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To exclude, that the participants with lowest/highest number of frames are enriched in certain words (e.g. short/long signs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frames = pd.read_csv('../data/n_frames.csv')\n",
    "df_frames.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Participant with highest number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_frames[df_frames.participant_id == 49445].sign.value_counts().reset_index() # participant with highest number of frames\n",
    "my_order = df_frames.groupby(by=[\"sign\"])[\"n_frames\"].mean().sort_values(ascending=False).index # order by increasing mean number of frames\n",
    "\n",
    "sns.barplot(data=data, x=\"index\", y=\"sign\", order=my_order)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Participant with lowest number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_frames[df_frames.participant_id == 37779].sign.value_counts().reset_index() # participant with lowest number of frames\n",
    "my_order = df_frames.groupby(by=[\"sign\"])[\"n_frames\"].mean().sort_values(ascending=False).index # order by increasing mean number of frames\n",
    "\n",
    "sns.barplot(data=data, x=\"index\", y=\"sign\", order=my_order)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of frames per word and per participant"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the number of frames for each single word for each single participant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting parameters\n",
    "participants = df_frames.groupby(by=[\"participant_id\"])[\"n_frames\"].mean().sort_values(ascending=False).index # list of participants sorted by ascending number of frames\n",
    "ncols = 1 # define plot matrix columns always as 3 \n",
    "nrows = math.ceil(len(participants+1)/ncols) # define rows depending on the number of participants / categories + one extra row for the total counts\n",
    "my_order = df_frames.groupby(by=[\"sign\"])[\"n_frames\"].mean().sort_values(ascending=False).index # order by increasing mean number of frames\n",
    "\n",
    "# formatting plotting areas\n",
    "f, axes = plt.subplots(nrows, ncols, figsize=(30, 30)) # define plot area layout based on pre-defined number of columns and rows\n",
    "axes = axes.ravel() # flatten axes object to later access it as axes[i] with i indicating the heatmap number\n",
    "\n",
    "# loop over all available participants to plot each sign distribution\n",
    "for i in range(len(participants)): \n",
    "    #print(i)\n",
    "    participant = participants[i]\n",
    "\n",
    "    # sign count data for current participant \n",
    "    sign_counts = df_frames[df_frames.participant_id == participant].sign.value_counts().reset_index() \n",
    "\n",
    "    # plot\n",
    "    g = sns.barplot(ax=axes[i], data=sign_counts, x=\"index\", y=\"sign\", order=my_order)\n",
    "\n",
    "    # set y axis labels\n",
    "    g.set_ylabel(ylabel=f'Participant {participant}', rotation=0, horizontalalignment='right')\n",
    "\n",
    "    # show xticks labels only for the last row\n",
    "    if i != len(participants)-1: \n",
    "        g.set(xticklabels=[]) \n",
    "        g.set(xlabel=None)\n",
    "        g.tick_params(bottom=False)  # remove the ticks\n",
    "    else: \n",
    "        g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
    "\n",
    "# total sign count data\n",
    "sign_counts = df_frames.sign.value_counts().reset_index() \n",
    "\n",
    "# plot total counts\n",
    "g = sns.barplot(ax=axes[i], data=sign_counts, x=\"index\", y=\"sign\", order=my_order)\n",
    "\n",
    "# set y axis label for total\n",
    "g.set_ylabel(ylabel=f'Total', rotation=0, horizontalalignment='right')\n",
    "\n",
    "# final plotting\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video visualisation of parquet files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you find two approaches to visualize the parquet files of sign recordings. Here, we want to further find out more about too long/too short sequences. \n",
    "\n",
    "Code for visualization was adapted from https://www.kaggle.com/code/danielpeshkov/animated-data-visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load train.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv file with train targets\n",
    "dir = '/data/asl-signs'\n",
    "train_df = pd.read_csv('../data/asl-signs/train.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example sequences with frame length more than 100 frames:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sequences were extracted from the EDA_DataCleaning Notebook\n",
    "\n",
    "sequence_id - Frame length\n",
    "* 203327050     106\n",
    "* 269920128     129\n",
    "* 621161958     139\n",
    "* 690456887     132\n",
    "* 994309330     134\n",
    "* 1862716737    135\n",
    "* 2085945934    133\n",
    "* 2292927765    114\n",
    "* 2779257484    123\n",
    "* 2889201858    129\n",
    "* 3116562701    118\n",
    "* 3380869090    116\n",
    "* 3797899944    123\n",
    "* 4183154128    125"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining sequence id and extracting sequence information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to check specific signs or specific participants you can get example sequences \n",
    "# enter participant or sign you want to check, enter 0 if all should be queried\n",
    "participant = 55372 \n",
    "label_sign = 'listen'\n",
    "\n",
    "# getting samples for desired sign and/or participant\n",
    "if participant == 0:\n",
    "    # get sequences for specific sign\n",
    "    display(train_df.query(f'sign == \"{label_sign}\"').sample(5))\n",
    "else:\n",
    "    if label_sign == 0:\n",
    "        # get sequences for specific participant\n",
    "        display(train_df.query(f'participant_id == {participant}').sample(5))\n",
    "    else:\n",
    "        display(train_df.query(f'sign == \"{label_sign}\" and participant_id == {participant}').sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy a sequence id from the query above or\n",
    "# enter a specific sequence you want to check\n",
    "sequence = 3116562701  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the corresponding parquet file\n",
    "path_to_sign = train_df.query(f'sequence_id == {sequence}').iloc[0, 0]\n",
    "sign = pd.read_parquet(f'..{dir}/{path_to_sign}')\n",
    "sign.y = sign.y * -1\n",
    "\n",
    "#print information about the selected sequence\n",
    "print(f'The sign shown in the sequence is: {train_df.query(f\"sequence_id == {sequence}\").iloc[0,3]}.')\n",
    "print(f'The sign was recorded by participant: {train_df.query(f\"sequence_id == {sequence}\").iloc[0,1]}.')\n",
    "print(f'Shape of the selected recording is: {sign.shape}')\n",
    "print(f'The frame length is: {sign.frame.max() - sign.frame.min()} frames.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First approach of visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two functions return the coordinate values with connecting lines for hands and pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hand_points(hand):\n",
    "    x = [[hand.iloc[0].x, hand.iloc[1].x, hand.iloc[2].x, hand.iloc[3].x, hand.iloc[4].x], # Thumb\n",
    "         [hand.iloc[5].x, hand.iloc[6].x, hand.iloc[7].x, hand.iloc[8].x], # Index\n",
    "         [hand.iloc[9].x, hand.iloc[10].x, hand.iloc[11].x, hand.iloc[12].x], \n",
    "         [hand.iloc[13].x, hand.iloc[14].x, hand.iloc[15].x, hand.iloc[16].x], \n",
    "         [hand.iloc[17].x, hand.iloc[18].x, hand.iloc[19].x, hand.iloc[20].x], \n",
    "         [hand.iloc[0].x, hand.iloc[5].x, hand.iloc[9].x, hand.iloc[13].x, hand.iloc[17].x, hand.iloc[0].x]]\n",
    "\n",
    "    y = [[hand.iloc[0].y, hand.iloc[1].y, hand.iloc[2].y, hand.iloc[3].y, hand.iloc[4].y],  #Thumb\n",
    "         [hand.iloc[5].y, hand.iloc[6].y, hand.iloc[7].y, hand.iloc[8].y], # Index\n",
    "         [hand.iloc[9].y, hand.iloc[10].y, hand.iloc[11].y, hand.iloc[12].y], \n",
    "         [hand.iloc[13].y, hand.iloc[14].y, hand.iloc[15].y, hand.iloc[16].y], \n",
    "         [hand.iloc[17].y, hand.iloc[18].y, hand.iloc[19].y, hand.iloc[20].y], \n",
    "         [hand.iloc[0].y, hand.iloc[5].y, hand.iloc[9].y, hand.iloc[13].y, hand.iloc[17].y, hand.iloc[0].y]] \n",
    "    return x, y\n",
    "\n",
    "def get_pose_points(pose):\n",
    "    x = [[pose.iloc[8].x, pose.iloc[6].x, pose.iloc[5].x, pose.iloc[4].x, pose.iloc[0].x, pose.iloc[1].x, pose.iloc[2].x, pose.iloc[3].x, pose.iloc[7].x], \n",
    "         [pose.iloc[10].x, pose.iloc[9].x], \n",
    "         [pose.iloc[22].x, pose.iloc[16].x, pose.iloc[20].x, pose.iloc[18].x, pose.iloc[16].x, pose.iloc[14].x, pose.iloc[12].x, \n",
    "          pose.iloc[11].x, pose.iloc[13].x, pose.iloc[15].x, pose.iloc[17].x, pose.iloc[19].x, pose.iloc[15].x, pose.iloc[21].x], \n",
    "         [pose.iloc[12].x, pose.iloc[24].x, pose.iloc[26].x, pose.iloc[28].x, pose.iloc[30].x, pose.iloc[32].x, pose.iloc[28].x], \n",
    "         [pose.iloc[11].x, pose.iloc[23].x, pose.iloc[25].x, pose.iloc[27].x, pose.iloc[29].x, pose.iloc[31].x, pose.iloc[27].x], \n",
    "         [pose.iloc[24].x, pose.iloc[23].x]\n",
    "        ]\n",
    "\n",
    "    y = [[pose.iloc[8].y, pose.iloc[6].y, pose.iloc[5].y, pose.iloc[4].y, pose.iloc[0].y, pose.iloc[1].y, pose.iloc[2].y, pose.iloc[3].y, pose.iloc[7].y], \n",
    "         [pose.iloc[10].y, pose.iloc[9].y], \n",
    "         [pose.iloc[22].y, pose.iloc[16].y, pose.iloc[20].y, pose.iloc[18].y, pose.iloc[16].y, pose.iloc[14].y, pose.iloc[12].y, \n",
    "          pose.iloc[11].y, pose.iloc[13].y, pose.iloc[15].y, pose.iloc[17].y, pose.iloc[19].y, pose.iloc[15].y, pose.iloc[21].y], \n",
    "         [pose.iloc[12].y, pose.iloc[24].y, pose.iloc[26].y, pose.iloc[28].y, pose.iloc[30].y, pose.iloc[32].y, pose.iloc[28].y], \n",
    "         [pose.iloc[11].y, pose.iloc[23].y, pose.iloc[25].y, pose.iloc[27].y, pose.iloc[29].y, pose.iloc[31].y, pose.iloc[27].y], \n",
    "         [pose.iloc[24].y, pose.iloc[23].y]\n",
    "        ]\n",
    "    return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function plots the animation frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run function to plot selected sequence\n",
    "def animation_frame(f):\n",
    "    frame = sign[sign.frame==f]\n",
    "    left = frame[frame.type=='left_hand']\n",
    "    right = frame[frame.type=='right_hand']\n",
    "    pose = frame[frame.type=='pose']\n",
    "    face = frame[frame.type=='face'][['x', 'y']].values\n",
    "    lx, ly = get_hand_points(left)\n",
    "    rx, ry = get_hand_points(right)\n",
    "    px, py = get_pose_points(pose)\n",
    "    ax.clear()\n",
    "    ax.plot(face[:,0], face[:,1], '.')\n",
    "    for i in range(len(lx)):\n",
    "        ax.plot(lx[i], ly[i])\n",
    "    for i in range(len(rx)):\n",
    "        ax.plot(rx[i], ry[i])\n",
    "    for i in range(len(px)):\n",
    "        ax.plot(px[i], py[i])\n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.ylim(ymin, ymax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These values set the limits on the graph to stabilize the video\n",
    "xmin = sign.x.min() - 0.2\n",
    "xmax = sign.x.max() + 0.2\n",
    "ymin = sign.y.min() - 0.2\n",
    "ymax = sign.y.max() + 0.2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "l, = ax.plot([], [])\n",
    "animation = FuncAnimation(fig, func=animation_frame, frames=sign.frame.unique())\n",
    "\n",
    "HTML(animation.to_html5_video())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "#### Second approach for visualisation - Animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for animation was adapted from: https://www.kaggle.com/code/ted0071/gislr-visualization/notebook "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['axes.spines.left'] = False\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.bottom'] = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cfg:\n",
    "    RANDOM_STATE = 2023\n",
    "    INPUT_ROOT = Path('../data/asl-signs/')\n",
    "    OUTPUT_ROOT = Path('../images/working')\n",
    "    INDEX_MAP_FILE = INPUT_ROOT / 'sign_to_prediction_index_map.json'\n",
    "    TRAN_FILE = INPUT_ROOT / 'train.csv'\n",
    "    INDEX = 'sequence_id'\n",
    "    ROW_ID = 'row_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_landmark_data_by_path(file_path, input_root=Cfg.INPUT_ROOT):\n",
    "    \"\"\"Reads landmak data by the given file path.\"\"\"\n",
    "    data = pd.read_parquet(input_root / file_path)\n",
    "    return data.set_index(Cfg.ROW_ID)\n",
    "\n",
    "def read_landmark_data_by_id(sequence_id, train_data):\n",
    "    \"\"\"Reads the landmark data by the given sequence id.\"\"\"\n",
    "    file_path = train_df.query(f'sequence_id == {sequence}').iloc[0, 0] # changed, check\n",
    "    return read_landmark_data_by_path(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 1600\n",
    "width = 1200\n",
    "\n",
    "data = sign\n",
    "frame_id = data['frame'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blank_image(height, width):\n",
    "    return np.zeros((height, width, 3), np.uint8)\n",
    "\n",
    "def draw_landmarks(\n",
    "    data, \n",
    "    image, \n",
    "    frame_id, \n",
    "    landmark_type, \n",
    "    connection_type, \n",
    "    landmark_color=(255, 0, 0), \n",
    "    connection_color=(0, 20, 255), \n",
    "    thickness=1, \n",
    "    circle_radius=1\n",
    "):\n",
    "    \"\"\"Draws landmarks\"\"\"\n",
    "    df = data.groupby(['frame', 'type']).get_group((frame_id, landmark_type))\n",
    "    landmarks = [landmark_pb2.NormalizedLandmark(x=lm.x, y=lm.y, z=lm.z) for idx, lm in df.iterrows()]\n",
    "    landmark_list = landmark_pb2.NormalizedLandmarkList(landmark = landmarks)\n",
    "\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=image,\n",
    "        landmark_list=landmark_list, \n",
    "        connections=connection_type,\n",
    "        landmark_drawing_spec=mp_drawing.DrawingSpec(\n",
    "            color=landmark_color, \n",
    "            thickness=thickness, \n",
    "            circle_radius=circle_radius),\n",
    "        connection_drawing_spec=mp_drawing.DrawingSpec(\n",
    "            color=connection_color, \n",
    "            thickness=thickness, \n",
    "            circle_radius=circle_radius))\n",
    "    return image\n",
    "\n",
    "def draw_left_hand(data, image, frame_id):\n",
    "    return draw_landmarks(\n",
    "        data, \n",
    "        image, \n",
    "        frame_id, \n",
    "        landmark_type='left_hand', \n",
    "        connection_type=mp_hands.HAND_CONNECTIONS,\n",
    "        landmark_color=(255, 0, 0),\n",
    "        connection_color=(0, 20, 255), \n",
    "        thickness=3, \n",
    "        circle_radius=3)\n",
    "\n",
    "def draw_right_hand(data, image, frame_id):\n",
    "    return draw_landmarks(\n",
    "        data, \n",
    "        image, \n",
    "        frame_id, \n",
    "        landmark_type='right_hand', \n",
    "        connection_type=mp_hands.HAND_CONNECTIONS,\n",
    "        landmark_color=(255, 0, 0),\n",
    "        connection_color=(0, 20, 255),\n",
    "        thickness=3, \n",
    "        circle_radius=3)\n",
    "\n",
    "def draw_face(data, image, frame_id):\n",
    "    return draw_landmarks(\n",
    "        data, \n",
    "        image, \n",
    "        frame_id, \n",
    "        landmark_type='face', \n",
    "        connection_type=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "        landmark_color=(255, 255, 255),\n",
    "        connection_color=(0, 255, 0))      \n",
    "    \n",
    "def draw_pose(data, image, frame_id):\n",
    "    return draw_landmarks(\n",
    "        data, \n",
    "        image, \n",
    "        frame_id, \n",
    "        landmark_type='pose', \n",
    "        connection_type=mp_pose.POSE_CONNECTIONS,\n",
    "        landmark_color=(255, 255, 255),\n",
    "        connection_color=(255, 0, 0),\n",
    "        thickness=2, \n",
    "        circle_radius=2)\n",
    "\n",
    "def create_frame(data, frame_id, height=1000, width=1000):\n",
    "    image = create_blank_image(height, width)    \n",
    "\n",
    "    draw_pose(data, image, frame_id) \n",
    "    draw_left_hand(data, image, frame_id)    \n",
    "    draw_right_hand(data, image, frame_id)  \n",
    "    draw_face(data, image, frame_id)\n",
    "     \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frames(sequence_id, train_data, height=800, width=800):\n",
    "    data = read_landmark_data_by_id(sequence_id, train_data)\n",
    "    frame_ids = data['frame'].unique()\n",
    "    images = [create_frame(data, frame_id=fid, height=height, width=width) for fid in frame_ids]\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_animation(images, fig, ax):\n",
    "    ax.axis('off')\n",
    "    \n",
    "    ims = []\n",
    "    for img in images:\n",
    "        im = ax.imshow(img, animated=True)\n",
    "        ims.append([im])\n",
    "    \n",
    "    func_animation = animation.ArtistAnimation(\n",
    "        fig, \n",
    "        ims, \n",
    "        interval=100, \n",
    "        blit=True,\n",
    "        repeat_delay=1000)\n",
    "\n",
    "    return func_animation\n",
    "\n",
    "def play_animation(sequence_id, train_data, height, width, figsize=(4, 4)):\n",
    "    frames = create_frames(sequence_id, train_data, height=height, width=width)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    anim = create_animation(frames, fig, ax)    \n",
    "    video = anim.to_html5_video()\n",
    "    \n",
    "    html = ds.HTML(video)\n",
    "    ds.display(html)\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_animation(sequence, train_df, height=height, width=width)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __5. Feature Engineering__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `frame` column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the EDA section, we are going to perform some adjustement to our data based on the `frame` column. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing sequences that are too short (less than 5 frames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's look at the barplot showing the sequences lengths (frame counts per sequence). Horizontal lines indicate the frame count values 5 (too short) and 130 (too long). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot number of frames per sequence\n",
    "def plot_frames_per_sequence(df_wide): \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    \n",
    "    # drawing the plot\n",
    "    sns.countplot(x = df_wide['sequence_id'],  ax=ax)\n",
    "    plt.axhline(y = 5, color = 'b', label = 'axvline - full height')\n",
    "    plt.axhline(y = 130, color = 'b', label = 'axvline - full height')\n",
    "    plt.show()\n",
    "\n",
    "plot_frames_per_sequence(df_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop sequences with less than 5 frames\n",
    "print(f'{df_wide.sequence_id.nunique()} sequences before filtering.')\n",
    "df_wide = df_wide.groupby('sequence_id').filter(lambda x : len(x)>4)\n",
    "print(f'{df_wide.sequence_id.nunique()} sequences after filtering all with less than 4 frames.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the plot after removing all short sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frames_per_sequence(df_wide)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing sequences that are too long ??? (more than ??? frames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we remove all sequences that we consider too long (more than 130 frames). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide = df_wide.groupby('sequence_id').filter(lambda x : len(x)<131)\n",
    "df_wide.sequence_id.nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again we have a look at the barplot after removing long sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frames_per_sequence(df_wide)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reassign frame indices to start from `0` for each sequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new column `frame_index` with frame indices ranging from 0 to the last frame for each sequence. Then, we drop the old `frame` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all sequences, give frames a new index starting from 0\n",
    "df_wide[\"frame_index\"] = df_wide.groupby(\"sequence_id\").cumcount()  # make new frame indices\n",
    "df_wide = df_wide.drop('frame', axis = 1) # drop the old frame indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly check, if it worked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check if the number of starting frames (= 0) is equal to the number of sequences\n",
    "(df_wide.frame_index == 0).sum() == df_wide.sequence_id.nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems, it worked :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropping / padding sequences to same length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we know, that the lengths of sequences vary a lot and decided to bring all sequences to the same length by either cropping or padding. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define some helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for padding\n",
    "def padding(array, xx, yy):\n",
    "    \"\"\"\n",
    "    :param array: numpy array\n",
    "    :param xx: desired height\n",
    "    :param yy: desirex width\n",
    "    :return: padded array\n",
    "    \"\"\"\n",
    "\n",
    "    h = array.shape[0]\n",
    "    w = array.shape[1]\n",
    "\n",
    "    a = (xx - h) // 2\n",
    "    aa = xx - a - h\n",
    "\n",
    "    b = (yy - w) // 2\n",
    "    bb = yy - b - w\n",
    "\n",
    "    return np.pad(array, pad_width=((a, aa), (b, bb)), mode='constant')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to truncate the middle data points of each sequence\n",
    "\n",
    "def crop_center(img,cropx,cropy):\n",
    "    y,x = img.shape\n",
    "    startx = x//2-(cropx//2)\n",
    "    starty = y//2-(cropy//2)    \n",
    "    return img[starty:starty+cropy,startx:startx+cropx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to make sequences first for this to work\n",
    "\n",
    "# decide which columns are of interest\n",
    "feature_columns = df_wide.columns.to_list()[2:1088]\n",
    "\n",
    "#make sequences\n",
    "sequences = list()\n",
    "\n",
    "for sequence_id, group in df_wide.groupby('sequence_id'):\n",
    "    sequence_features = group[feature_columns]\n",
    "    #label = df_wide[df_wide.sequence_id == sequence_id].iloc[0].sign #this would at the label to the sequence if we want that\n",
    "    values = sequence_features.values \n",
    "    #sequences.append((sequence_features, label))\n",
    "    sequences.append(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_seq = []\n",
    "for one_seq in sequences: #loop through all sequences\n",
    "    new_one_seq = padding(one_seq,130, 1086)\n",
    "    new_seq.append(new_one_seq)\n",
    "\n",
    "\n",
    "final_seq = np.stack(new_seq)\n",
    "final_seq[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_seq = []\n",
    "for one_seq in final_seq: #loop through all sequences\n",
    "    \n",
    "    crop_one_seq = crop_center(one_seq,1086, 30)\n",
    "    crop_seq.append(crop_one_seq)\n",
    "\n",
    "\n",
    "cropped_seq = np.stack(crop_seq)\n",
    "cropped_seq[9]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate `X` and `y` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cropped_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TALK TO RONJA\n",
    "# get corresponding y\n",
    "y = df_wide.groupby('sequence_id')['sign'].agg(pd.Series.mode)\n",
    "pd.DataFrame(y)\n",
    "# get dummies because this is how the model wants the data\n",
    "y = pd.get_dummies(y)\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save `X` and `y` data as `.npy` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as numpy files\n",
    "np.save('../data/y-data.npy', y)\n",
    "np.save('../data/X-data.npy', X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __6. Modeling__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/asl-signs/train.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying dictionary on sign to create target column\n",
    "df[\"target\"] = df.sign.map(sign_to_prediction_index_map)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.target\n",
    "X = df.drop(\"target\", axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(y)\n",
    "y.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Baseline Model: Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Install nb_black for autoformatting\n",
    "!pip install nb_black --quiet\n",
    "%load_ext lab_black\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training and prediction: Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = DummyClassifier(strategy=\"stratified\", random_state=42)\n",
    "dummy_model.fit(X_train, y_train)\n",
    "y_pred = dummy_model.predict(X_test)\n",
    "y_proba = dummy_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_original = y_test.copy()\n",
    "y_pred_original = y_pred.copy()\n",
    "y_test = np.argmax(np.array(y_test), axis=1).tolist()\n",
    "y_pred = np.argmax(y_pred, axis=1).tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multilabel confusion matrix\n",
    "multilabel_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "* I am not 100% sure how well this works for multiclass classification because for y_pred you get several predictions for several classes\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
