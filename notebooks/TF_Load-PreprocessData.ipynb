{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow - Loading data - Adjusting the layer to our needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/roberthatch/gislr-feature-data-on-the-shoulders/notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook generates a tensorflow preprocessing layer that transforms the whole dataset of single parquet files into numpy arrays.\n",
    "\n",
    "The flow is as follow:\n",
    "\n",
    "***preprocessing layer:***\n",
    "1. drop z coordinate if desired (this would reduce number of coordinates to 2; following array shapes show number for 3 coordinates)\n",
    "2. Convert input into x containing avg face, lips, upper pose landmarks, left hand, right hand for every frame, \n",
    "    e.g. [23, 106, 3] = (number of frames, landmarks, coordinates)\n",
    "3. Pad x or cut x such that the length is as defined (e.g. length = 30 --> [30, 106, 3])  = (number of frames, landmarks, coordinates)\n",
    "4. interpolate single missing values, then replace NaN values with zero\n",
    "4. Resize it to either flattened or 3 dimensional array:\n",
    "    * 3D: ( [1, 30, 318] = (1 row, number of frames, landmarks * coordinates) \n",
    "    * or flattened: [1, 9540])  = (1 row, number of frames * landmarks * coordinates)\n",
    "\n",
    "***looping through csv file ***\n",
    "\n",
    "By looping through the csv file each parquet file will be loaded and the required data will be extracted and transformed by running it through the preprocessing layer.\n",
    "Then the data will be added to our final numpy array of all recordings. If the recording is exceeding defined length limits it will be removed from the dataset.\n",
    "\n",
    "* final shape of data: flattened: x (94477, 5796) and y (94477,) or unflattened: (94477, 30, 212) (94477,)\n",
    "* shape also depends on selected landmarks and coordinates and migth vary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/franzi/neuefische/Capstone_Project_SignMeUp/.venv/lib/python3.9/site-packages (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm\n",
    "import os\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file paths for Kaggle\n",
    "# LANDMARK_FILES_DIR = \"/kaggle/input/asl-signs/train_landmark_files\"\n",
    "# TRAIN_FILE = \"/kaggle/input/asl-signs/train.csv\"\n",
    "# OUTPUT = \"\"\n",
    "# label_map = json.load(open(\"/kaggle/input/asl-signs/sign_to_prediction_index_map.json\", \"r\"))\n",
    "\n",
    "#for local notebook, adjust file paths here if required\n",
    "LANDMARK_FILES_DIR = \"../data/asl-signs/\"\n",
    "TRAIN_FILE = \"../data/asl-signs/train.csv\"\n",
    "OUTPUT = \"../data/\" #path to save x and y files\n",
    "label_map = json.load(open(\"../data/asl-signs/sign_to_prediction_index_map.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE:\n",
    "#option for zeropadding, end/beginning\n",
    "#CSV Filtering\n",
    "#Drop too short/too long sequences\n",
    "#numpy array with NaN instead of zero --> only useful if arrays with different length will be added\n",
    "#first/lastframe padding\n",
    "#interpolate single missing values\n",
    "\n",
    "\n",
    "#TODO: options to add for defining dataset\n",
    "#change numpy array to list? or numpy array with different length --> this would slow down the compiling process significantly\n",
    "#define handedness and mirror coordinates to one side\n",
    "\n",
    "\n",
    "#how does rocket handle different lengths?\n",
    "#how does rocket handle NaNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of used landmarks: 104\n"
     ]
    }
   ],
   "source": [
    "#limit dataset for quick test\n",
    "QUICK_TEST = False\n",
    "QUICK_LIMIT = 1000\n",
    "\n",
    "#Define length of sequences for padding or cutting; 22 is the median length of all sequences\n",
    "LENGTH = 22\n",
    "\n",
    "#define min or max length of sequences; sequences too long/too short will be dropped\n",
    "#max value of 92 was defined by calculating the interquartile range\n",
    "MIN_LENGTH = 10\n",
    "MAX_LENGTH = 92\n",
    "\n",
    "#final data will be flattened, if false data will be 3 dimensional\n",
    "FLATTEN = False\n",
    "\n",
    "#define initialization of numpy array \n",
    "ARRAY = False #(True=Zeros, False=empty values)\n",
    "\n",
    "#Define padding mode \n",
    "#1 = padding at start&end; 2 = padding at end; 3 = no padding, 4 = copy first/lastframe, 5 = copy last frame)\n",
    "#Note: Mode 3 will give you an error due to different lengths, working on that\n",
    "PADDING = 2\n",
    "CONSTANT_VALUE = 0 #only required for mode 1 and 2; enter tf.constant(float('nan')) for NaN\n",
    "\n",
    "#define if z coordinate will be dropped\n",
    "DROP_Z = True\n",
    "\n",
    "#define if csv file should be filtered\n",
    "CSV_FILTER  = True\n",
    "#define how many participants for test set\n",
    "TEST_COUNT = 5 #5 participants account for ca 23% of dataset\n",
    "#generate test or train dataset (True = Train dataset; False = Test dataset)\n",
    "TRAIN = True #only works if CSV_FILTER is activated\n",
    "#TRAIN = False\n",
    "\n",
    "#define filenames for x and y:\n",
    "feature_data = 'X_train' #x data\n",
    "feature_labels = 'y_train' #y data\n",
    "\n",
    "#use for test dataset\n",
    "#feature_data = 'X_test' #x data\n",
    "#feature_labels = 'y_test' #y data\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "#Defining Landmarks\n",
    "#index ranges for each landmark type\n",
    "#dont change these landmarks\n",
    "FACE = list(range(0, 468))\n",
    "LEFT_HAND = list(range(468, 489))\n",
    "POSE = list(range(489, 522))\n",
    "POSE_UPPER = list(range(489, 510))\n",
    "RIGHT_HAND = list(range(522, 543))\n",
    "LIPS = [61, 185, 40, 39, 37,  0, 267, 269, 270, 409,\n",
    "                 291,146, 91,181, 84, 17, 314, 405, 321, 375, \n",
    "                 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, \n",
    "                 95, 88, 178, 87, 14,317, 402, 318, 324, 308]\n",
    "#defining landmarks that will be merged\n",
    "averaging_sets = [FACE]\n",
    "\n",
    "#generating list with all landmarks selected for preprocessing\n",
    "#change landmarks you want to use here:\n",
    "point_landmarks = LEFT_HAND + POSE_UPPER + RIGHT_HAND + LIPS\n",
    "\n",
    "\n",
    "#calculating sum of total landmarks used\n",
    "LANDMARKS = len(point_landmarks) + len(averaging_sets)\n",
    "print(f'Total count of used landmarks: {LANDMARKS}')\n",
    "\n",
    "#defining input shape for model\n",
    "if DROP_Z:\n",
    "    INPUT_SHAPE = (LENGTH,LANDMARKS*2)\n",
    "else:\n",
    "    INPUT_SHAPE = (LENGTH,LANDMARKS*3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS_PER_FRAME = 543\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    #defines which columns will be read from the file\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    #calculates the number of frames in the data by dividing the length of the data by the number of rows per frame\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    #reshapes the data into a 3D array with shape (n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_hand_percentage(x):\n",
    "    #calculates percentage of right hand usage\n",
    "    right = tf.gather(x, RIGHT_HAND, axis=1)\n",
    "    left = tf.gather(x, LEFT_HAND, axis=1)\n",
    "    right_count = tf.reduce_sum(tf.where(tf.math.is_nan(right), tf.zeros_like(right), tf.ones_like(right)))\n",
    "    left_count = tf.reduce_sum(tf.where(tf.math.is_nan(left), tf.zeros_like(left), tf.ones_like(left)))\n",
    "    return right_count / (left_count+right_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_nan_mean(x, axis=0):\n",
    "    #calculates the mean of a TensorFlow tensor x along a specified axis while ignoring any NaN values in the tensor.\n",
    "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis)\n",
    "\n",
    "def tf_nan_std(x, axis=0):\n",
    "    #calculates the standard deviation of a tensor x along a specified axis, while ignoring any NaN values in the tensor\n",
    "    d = x - tf_nan_mean(x, axis=axis)\n",
    "    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis))\n",
    "\n",
    "#this function is only required if mean and std will be calculated for specific segments of the data\n",
    "def flatten_means_and_stds(x, axis=0):\n",
    "    #Get means and stds\n",
    "    x_mean = tf_nan_mean(x, axis=0)\n",
    "    x_std  = tf_nan_std(x,  axis=0)\n",
    "    #concats mean and std values for each sequence\n",
    "    x_out = tf.concat([x_mean, x_std], axis=0)\n",
    "    x_out = tf.reshape(x_out, (1, INPUT_SHAPE[1]*2))\n",
    "    #replaces NaN values with zeros\n",
    "    x_out = tf.where(tf.math.is_finite(x_out), x_out, tf.zeros_like(x_out))\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Feature Preprocessing Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating preprocessing layer that will be added to final model\n",
    "class FeatureGen(tf.keras.layers.Layer):\n",
    "    #defines custom tensorflow layer \n",
    "    def __init__(self):\n",
    "        #initializes layer\n",
    "        super(FeatureGen, self).__init__()\n",
    "    \n",
    "    def call(self, x_in):\n",
    "        #drop z coordinates if required\n",
    "        if DROP_Z:\n",
    "            x_in = x_in[:, :, 0:2]\n",
    "        \n",
    "        #generates list with mean values for landmarks that will be merged\n",
    "        x_list = [tf.expand_dims(tf_nan_mean(x_in[:, av_set[0]:av_set[0]+av_set[1], :], axis=1), axis=1) for av_set in averaging_sets]\n",
    "        #extracts specific columns from input x_in defined by landmarks\n",
    "        x_list.append(tf.gather(x_in, point_landmarks, axis=1))\n",
    "        #concatenates the two tensors from above along axis 1/columns\n",
    "        x = tf.concat(x_list, 1)\n",
    "\n",
    "        #padding to desired length of sequence (defined by LENGTH)\n",
    "        #get current number of rows\n",
    "        x_padded = x\n",
    "        current_rows = tf.shape(x_padded)[0]\n",
    "        #if current number of rows is greater than desired number of rows, truncate excess rows\n",
    "        if current_rows > LENGTH:\n",
    "            x_padded = x_padded[:LENGTH, :, :]\n",
    "\n",
    "        #if current number of rows is less than desired number of rows, add padding\n",
    "        elif current_rows < LENGTH:\n",
    "            #calculate amount of padding needed\n",
    "            pad_rows = LENGTH - current_rows\n",
    "\n",
    "            if PADDING ==4: #copy first/last frame\n",
    "                if pad_rows %2 == 0: #if pad_rows is even\n",
    "                    padding_front = tf.repeat(x_padded[0:1, :], pad_rows//2, axis=0)\n",
    "                    padding_back = tf.repeat(x_padded[-1:, :], pad_rows//2, axis=0)\n",
    "                else: #if pad_rows is odd\n",
    "                    padding_front = tf.repeat(x_padded[0:1, :], (pad_rows//2)+1, axis=0)\n",
    "                    padding_back = tf.repeat(x_padded[-1:, :], pad_rows//2, axis=0)\n",
    "                x_padded = tf.concat([padding_front, x_padded, padding_back], axis=0)\n",
    "            elif PADDING == 5: #copy last frame\n",
    "                padding_back = tf.repeat(x_padded[-1:, :], pad_rows, axis=0)\n",
    "                x_padded = tf.concat([x_padded, padding_back], axis=0)\n",
    "            else:\n",
    "                if PADDING ==1: #padding at start and end\n",
    "                    if pad_rows %2 == 0: #if pad_rows is even\n",
    "                        paddings = [[pad_rows//2, pad_rows//2], [0, 0], [0, 0]]\n",
    "                    else: #if pad_rows is odd\n",
    "                        paddings = [[pad_rows//2+1, pad_rows//2], [0, 0], [0, 0]]\n",
    "                elif PADDING ==2: #padding only at the end of sequence\n",
    "                    paddings = [[0, pad_rows], [0, 0], [0, 0]]\n",
    "                elif PADDING ==3: #no padding\n",
    "                    paddings = [[0, 0], [0, 0], [0, 0]]\n",
    "                x_padded = tf.pad(x_padded, paddings, mode='CONSTANT', constant_values=CONSTANT_VALUE)\n",
    "\n",
    "        x = x_padded\n",
    "        current_rows = tf.shape(x)[0]\n",
    "\n",
    "        #interpolate single missing values\n",
    "        x = pd.DataFrame(np.array(x).flatten()).interpolate(method='linear', limit=2, limit_direction='both')\n",
    "        #fill missing values with zeros\n",
    "        x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
    "        \n",
    "        #reshape data to 2D or 3D array\n",
    "        if FLATTEN:\n",
    "            x = tf.reshape(x, (1, current_rows*INPUT_SHAPE[1]))\n",
    "        else:\n",
    "            x = tf.reshape(x, (1, current_rows, INPUT_SHAPE[1]))\n",
    "\n",
    "        return x\n",
    "\n",
    "#define converter using generated layer\n",
    "feature_converter = FeatureGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 22, 208), dtype=float32, numpy=\n",
       "array([[[0.57777625, 0.5093604 , 0.51049846, ..., 0.53057057,\n",
       "         0.6235912 , 0.5270701 ],\n",
       "        [0.57509214, 0.5065524 , 0.5077551 , ..., 0.52969885,\n",
       "         0.6222859 , 0.5262149 ],\n",
       "        [0.5703879 , 0.5069116 , 0.50810647, ..., 0.52882755,\n",
       "         0.62084705, 0.5254454 ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 835,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tests for generated layer\n",
    "#One tests symbolic tensor, the other tests real data.\n",
    "#print(feature_converter(tf.keras.Input((543, 3), dtype=tf.float32, name=\"inputs\")))\n",
    "\n",
    "#tests preprocessing layer with parquet file\n",
    "feature_converter(load_relevant_data_subset(f'{LANDMARK_FILES_DIR}{pd.read_csv(TRAIN_FILE).path[1]}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These participants are used as test set: [62590, 18796, 2044, 28656, 27610].\n",
      "Together they have 21713 recordings, 22.98% of the dataset.\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72764/72764 [05:08<00:00, 236.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6578 sequences were removed because they are too long, which is 9.04% of the whole dataset.\n",
      "95 sequences were removed because they are too short, which is 0.13% of the whole dataset.\n",
      "-------------------------------------\n",
      "Shapes of the final datasets are:\n",
      "(66091, 22, 208) (66091,)\n"
     ]
    }
   ],
   "source": [
    "def convert_row(i, row, long_sequences, short_sequences):\n",
    "\n",
    "    #loads data from parquet file\n",
    "    x = load_relevant_data_subset(f'{LANDMARK_FILES_DIR}{row[1].path}')\n",
    "    \n",
    "    #if sequence is too long or too short its index will be added to list, so we can later remove them from dataset\n",
    "    if x.shape[0] < MIN_LENGTH:\n",
    "        short_sequences.append(i)\n",
    "    elif x.shape[0] > MAX_LENGTH:\n",
    "        long_sequences.append(i)\n",
    "\n",
    "    #applies preprocessing layer to loaded data\n",
    "    x = feature_converter(tf.convert_to_tensor(x)).cpu().numpy()\n",
    "    #returns transformed x data and label of sign\n",
    "    return x, row[1].label\n",
    "\n",
    "def convert_and_save_data():\n",
    "    #reads csv file\n",
    "    df = pd.read_csv(TRAIN_FILE)\n",
    "    #maps label number to sign column\n",
    "    df['label'] = df['sign'].map(label_map)\n",
    "    \n",
    "    #filter dataset for participant train/test split\n",
    "    if CSV_FILTER:      \n",
    "        #set random state\n",
    "        random.seed(RANDOM_STATE)\n",
    "        #get random participants from dataset and add to sample list, number of participants is defined in setup\n",
    "        sample_list = random.sample(df.groupby('participant_id').mean().index.values.tolist(), TEST_COUNT)\n",
    "        print(f'These participants are used as test set: {sample_list}.') \n",
    "        print(f'Together they have {df.query(f\"participant_id in {sample_list}\").shape[0]} recordings, {round(df.query(f\"participant_id in {sample_list}\").shape[0]/df.shape[0]*100,2)}% of the dataset.')\n",
    "        print('-------------------------------------')\n",
    "        if TRAIN:\n",
    "            #limit dataset to participants used for train set\n",
    "            df = df.query(f'participant_id not in {sample_list}')\n",
    "        else:\n",
    "            #limit dataset to participants used for test set\n",
    "            df = df.query(f'participant_id in {sample_list}')\n",
    "\n",
    "\n",
    "    #sets number of total rows\n",
    "    total = df.shape[0]\n",
    "    #limits number of rows if quick_test is activated\n",
    "    if QUICK_TEST:\n",
    "        total = QUICK_LIMIT\n",
    "    \n",
    "    #generates numpy array with zeros in shape (total number of rows, number of expected columns)\n",
    "    if ARRAY: #initialize array with zeros\n",
    "        if FLATTEN:\n",
    "            npdata = np.zeros((total, INPUT_SHAPE[0]*INPUT_SHAPE[1]))\n",
    "        else:\n",
    "            npdata = np.zeros((total, INPUT_SHAPE[0], INPUT_SHAPE[1]))\n",
    "        nplabels = np.zeros(total)\n",
    "    else: #initialize empty array\n",
    "        if FLATTEN:\n",
    "            npdata = np.empty((total, INPUT_SHAPE[0]*INPUT_SHAPE[1]))\n",
    "        else:\n",
    "            npdata = np.empty((total, INPUT_SHAPE[0], INPUT_SHAPE[1]))\n",
    "        nplabels = np.empty(total)\n",
    "\n",
    "    #initializing lists for collecting too long and too short sequences\n",
    "    long_sequences = []\n",
    "    short_sequences = []\n",
    "\n",
    "    #for loop iterates through each row in df dataframe; i is index of the row and row accesses information in the row of df\n",
    "    #tqdm is used for showing progress bar\n",
    "    for i, row in tqdm(enumerate(df.iterrows()), total=total):\n",
    "        #load specific parquet file, run preprocessing layer and save x and y data\n",
    "        (x,y) = convert_row(i, row, long_sequences, short_sequences)\n",
    "        #save x and y to specific row in prepared numpy arrays\n",
    "        npdata[i,:] = x\n",
    "        nplabels[i] = y\n",
    "\n",
    "        #break if quick test is activated\n",
    "        if QUICK_TEST and i == QUICK_LIMIT - 1:\n",
    "            break\n",
    "\n",
    "    #remove rows of sequences that are too long/too short\n",
    "    npdata = np.delete(npdata, obj = (long_sequences + short_sequences), axis=0)\n",
    "    nplabels = np.delete(nplabels, obj = (long_sequences + short_sequences), axis=0)\n",
    "\n",
    "    print (f'{len(long_sequences)} sequences were removed because they are too long, which is {round(len(long_sequences)/df.shape[0]*100,2)}% of the whole dataset.')\n",
    "    print (f'{len(short_sequences)} sequences were removed because they are too short, which is {round(len(short_sequences)/df.shape[0]*100,2)}% of the whole dataset.')\n",
    "    print('-------------------------------------')\n",
    "    print('Shapes of the final datasets are:')\n",
    "    print(npdata.shape, nplabels.shape)\n",
    "\n",
    "    #save as np file\n",
    "    np.save(f\"{OUTPUT}{feature_data}.npy\", npdata)\n",
    "    np.save(f\"{OUTPUT}{feature_labels}.npy\", nplabels)\n",
    "        \n",
    "convert_and_save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66091, 22, 208) (66091,)\n"
     ]
    }
   ],
   "source": [
    "#test of loading data\n",
    "X = np.load(f\"{OUTPUT}{feature_data}.npy\")\n",
    "y = np.load(f\"{OUTPUT}{feature_labels}.npy\")\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
